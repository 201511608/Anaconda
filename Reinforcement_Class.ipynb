{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforcement:\n",
    "    # Defautl Goal_steps , LR, score_requirement and initia_games Defined  \n",
    "    def __init__( self,goal_steps = 500,LR = 1e-3,score_requirement = 100,initial_games = 10000):\n",
    "        print(\"Unsupervised Instance Begin\") \n",
    "        print(\"__init__( self): Always executed when the class is being initiated\")\n",
    "        self.LR = LR\n",
    "        self.goal_steps = goal_steps\n",
    "        self.score_requirement = score_requirement\n",
    "        self.initial_games = initial_games \n",
    "\n",
    "        \n",
    "    def __del__(self):\n",
    "        print(\"Unsupervised Instance Destroyed\")\n",
    "    \n",
    "    def check(self):\n",
    "        print(\"In Unsupervised class\")\n",
    "        \n",
    "     # 1.1\n",
    "    # Pole Balance\n",
    "    # Reinforcement \n",
    "    # Make it Better\n",
    "    def Pole_Balance(self):\n",
    "        import gym\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import tflearn\n",
    "        from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "        from tflearn.layers.estimator import regression\n",
    "        from statistics import median, mean\n",
    "        from collections import Counter\n",
    "\n",
    "        LR = 1e-3\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        env.reset()\n",
    "        goal_steps = 500\n",
    "        score_requirement = 100\n",
    "        initial_games = 10000\n",
    "        return score_requirement\n",
    "\n",
    "    def some_random_games_first(self):\n",
    "        # Each of these is its own game.\n",
    "        for episode in range(5):\n",
    "            env.reset()\n",
    "            # this is each frame, up to 200...but we wont make it that far.\n",
    "            for t in range(200):\n",
    "                # This will display the environment\n",
    "                # Only display if you really want to see it.\n",
    "                # Takes much longer to display it.\n",
    "                env.render()\n",
    "\n",
    "                # This will just create a sample action in any environment.\n",
    "  \n",
    "# In this environment, the action can be 0 or 1, which is left or right\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "                # this executes the environment with an action, \n",
    "                # and returns the observation of the environment, \n",
    "                # the reward, if the env is over, and other info.\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    #some_random_games_first()\n",
    "    def initial_population(self):\n",
    "        import numpy as np\n",
    "        import random\n",
    "        import gym\n",
    "        from statistics import median, mean\n",
    "        from collections import Counter\n",
    "        \n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        env.reset()\n",
    "        \n",
    "        # [OBS, MOVES]\n",
    "        training_data = []\n",
    "        # all scores:\n",
    "        scores = []\n",
    "        # just the scores that met our threshold:\n",
    "        accepted_scores = []\n",
    "        # iterate through however many games we want:\n",
    "        for _ in range(self.initial_games):\n",
    "            score = 0\n",
    "            # moves specifically from this environment:\n",
    "            game_memory = []\n",
    "            # previous observation that we saw\n",
    "            prev_observation = []\n",
    "            # for each frame in 200\n",
    "            for _ in range(self.goal_steps):\n",
    "                # choose random action (0 or 1)\n",
    "                action = random.randrange(0,2)\n",
    "                # do it!\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                # notice that the observation is returned FROM the action\n",
    "                # so we'll store the previous observation here, pairing\n",
    "                # the prev observation to the action we'll take.\n",
    "                if len(prev_observation) > 0 :\n",
    "                    game_memory.append([prev_observation, action])\n",
    "                prev_observation = observation\n",
    "                score+=reward\n",
    "                if done: break\n",
    "\n",
    "            # IF our score is higher than our threshold, we'd like to save\n",
    "            # every move we made\n",
    "            # NOTE the reinforcement methodology here. \n",
    "            # all we're doing is reinforcing the score, we're not trying \n",
    "            # to influence the machine in any way as to HOW that score is \n",
    "            # reached.\n",
    "            if score >= self.score_requirement:\n",
    "                accepted_scores.append(score)\n",
    "                for data in game_memory:\n",
    "                    # convert to one-hot (this is the output layer for our neural network)\n",
    "                    if data[1] == 1:\n",
    "                        output = [0,1]\n",
    "                    elif data[1] == 0:\n",
    "                        output = [1,0]\n",
    "\n",
    "                    # saving our training data\n",
    "                    training_data.append([data[0], output])\n",
    "\n",
    "            # reset env to play again\n",
    "            env.reset()\n",
    "            # save overall scores\n",
    "            scores.append(score)\n",
    "\n",
    "        # just in case you wanted to reference later\n",
    "        training_data_save = np.array(training_data)\n",
    "        np.save('saved1.npy',training_data_save)\n",
    "\n",
    "        # some stats here, to further illustrate the neural network magic!\n",
    "        print('Average accepted score:',mean(accepted_scores))\n",
    "        print('Median score for accepted scores:',median(accepted_scores))\n",
    "        print(Counter(accepted_scores))\n",
    "\n",
    "        return training_data\n",
    "\n",
    "\n",
    "\n",
    "    def neural_network_model(self, input_size):\n",
    "        import tflearn\n",
    "        from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "        from tflearn.layers.estimator import regression\n",
    "\n",
    "        network = input_data(shape=[None, input_size, 1], name='input')\n",
    "\n",
    "        network = fully_connected(network, 128, activation='relu')\n",
    "        network = dropout(network, 0.8)\n",
    "\n",
    "        network = fully_connected(network, 256, activation='relu')\n",
    "        network = dropout(network, 0.8)\n",
    "\n",
    "        network = fully_connected(network, 512, activation='relu')\n",
    "        network = dropout(network, 0.8)\n",
    "\n",
    "        network = fully_connected(network, 512, activation='relu')\n",
    "        network = dropout(network, 0.8)\n",
    "\n",
    "        network = fully_connected(network, 512, activation='relu')\n",
    "        network = dropout(network, 0.8)\n",
    "\n",
    "        network = fully_connected(network, 2, activation='softmax')\n",
    "        network = regression(network, optimizer='adam', learning_rate=self.LR, loss='categorical_crossentropy', name='targets')\n",
    "        model = tflearn.DNN(network, tensorboard_dir='log')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_model(self,training_data, model=False):\n",
    "        import numpy as np\n",
    "\n",
    "        X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "        y = [i[1] for i in training_data]\n",
    "\n",
    "        if not model:\n",
    "            model = Reinforcement.neural_network_model(self,input_size = len(X[0]))\n",
    "\n",
    "        model.fit({'input': X}, {'targets': y}, n_epoch=3, snapshot_step=500, show_metric=True, run_id='openai_learning')\n",
    "        return model\n",
    "\n",
    "    # model = train_model(initial_population())\n",
    "\n",
    "    def Pole_test(self):\n",
    "        # 1.2\n",
    "        import gym\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import tflearn\n",
    "        from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "        from tflearn.layers.estimator import regression\n",
    "        from statistics import median, mean\n",
    "        from collections import Counter\n",
    "\n",
    "        LR = 1e-3\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        env.reset()\n",
    "        goal_steps = 500\n",
    "        score_requirement = 50\n",
    "        initial_games = 10000\n",
    "        training_data =initial_population()\n",
    "        model = train_model(training_data)\n",
    "        scores = []\n",
    "        choices = []\n",
    "\n",
    "        for each_game in range(10):\n",
    "            score = 0\n",
    "            game_memory = []\n",
    "            prev_obs = []\n",
    "            env.reset()\n",
    "            for _ in range(goal_steps):\n",
    "                env.render()\n",
    "                if len(prev_obs)==0:\n",
    "                    action = random.randrange(0,2)\n",
    "                else:\n",
    "                    action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])\n",
    "                choices.append(action)               \n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                prev_obs = new_observation\n",
    "                game_memory.append([new_observation, action])\n",
    "                score+=reward\n",
    "                if done: break\n",
    "            scores.append(score)\n",
    "\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
    "        print(score_requirement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised Instance Begin\n",
      "__init__( self): Always executed when the class is being initiated\n"
     ]
    }
   ],
   "source": [
    "r=Reinforcement()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "r.Pole_Balance()\n",
    "traindata=r.goal_steps\n",
    "print(r.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.69264\u001b[0m\u001b[0m | time: 0.137s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 003 | loss: 0.69264 - acc: 0.5348 -- iter: 256/315\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.69293\u001b[0m\u001b[0m | time: 0.165s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 003 | loss: 0.69293 - acc: 0.5151 -- iter: 315/315\n",
      "--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tflearn.models.dnn.DNN at 0x18abe66eb70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.train_model(r.initial_population())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
