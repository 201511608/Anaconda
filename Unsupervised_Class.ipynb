{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UnSupervised Class\n",
    "\n",
    "# My UnSupervised Learning Functions\n",
    "class Unsupervised:\n",
    "    \n",
    "    def __init__( self):\n",
    "        print(\"Unsupervised Instance Begin\") \n",
    "        \n",
    "    def __del__(self):\n",
    "       print(\"Unsupervised Instance Destroyed\")\n",
    "    \n",
    "    def check(self):\n",
    "        print(\"In Unsupervised class\")\n",
    "    \n",
    "    #plots\n",
    "    def Scatter_Plot(self,x,y,labels=[]):\n",
    "       # labels=[1 for i in range(len(x))]\n",
    "        plt.scatter(x,y,c=labels,alpha =0.75)\n",
    "       # plt.scatter(centroids[:,0],centroids[:,1],color='r',marker='D',s=50)\n",
    "        plt.show()\n",
    "        \n",
    "    def Scatter_Plot_WithCg(self,cgx,cgy,x,y,labels=[]):\n",
    "        # centroids=model.cluster_centers_  # centroids[:,0],centroids[:,1] \n",
    "        #labels=[1 for i in range(len(x))]\n",
    "        plt.scatter(x,y,c=labels,alpha =0.75)\n",
    "        plt.scatter(cgx,cgy,color='r',marker='D',s=50)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def Inertia_Cluster_Graph(self,Train_Data):\n",
    "        sm_data=Train_Data\n",
    "        ks = range(1, 15)\n",
    "        inertias = []\n",
    "        for k in ks:\n",
    "            # Create a KMeans instance with k clusters: model\n",
    "            modle=KMeans(n_clusters=k)    \n",
    "            # Fit model to samples\n",
    "            modle.fit(sm_data)  \n",
    "            # Append the inertia to the list of inertias\n",
    "            inertias.append(modle.inertia_)\n",
    "        plt.plot(ks, inertias, '-o')\n",
    "        plt.xlabel('Number of Clusters, k')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.xticks(ks)\n",
    "        plt.show() \n",
    "        \n",
    "        \n",
    "    def crosstable(predictedData,targetData):\n",
    "        import pandas as pd\n",
    "        ct=pd.crosstab(predictedData,targetData)\n",
    "        print(\"\\nCrossTable\\n\",ct)\n",
    "        \n",
    "    def Normalizer_Kmean(self,Train_Data,Prediction_Data,clusters=5):    \n",
    "        ##import numpy as np\n",
    "        from sklearn.preprocessing import Normalizer\n",
    "        from sklearn.cluster import KMeans\n",
    "\n",
    "        # Market_Data=pd.read_csv('sharemarket_data.csv',header=0,usecols= [x for x in range(1,963)])\n",
    "        ##companiessm_data=np.loadtxt('sharemarket_data.txt') #sharemarket_data\n",
    "        ##companies=pd.read_csv('sharemarket_data.csv',header=0,usecols= [0]).values.reshape(60)\n",
    "\n",
    "        #7.1\n",
    "        normalizer= Normalizer() # Normalizing the Data\n",
    "        kmeans= KMeans(n_clusters=clusters)   # Change Cluster and check the results\n",
    "        pipeline = make_pipeline(normalizer,kmeans)\n",
    "        pipeline.fit(Train_Data)\n",
    "\n",
    "        #7.2\n",
    "        prediction =pipeline.predict(Prediction_Data)\n",
    "        return prediction\n",
    "    \n",
    "    def StandardScaler_Kmean(self,Train_Data,Prediction_Data,clusters=5):                     \n",
    "            from sklearn.pipeline import make_pipeline\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            from sklearn.cluster import KMeans\n",
    "            \n",
    "            # StandardScaler  MaxAbsScalar   Normilizer\n",
    "            scaler = StandardScaler()\n",
    "            # Create KMeans instance: kmeans\n",
    "            kmeans = KMeans(n_clusters=clusters)\n",
    "\n",
    "\n",
    "            # Create pipeline: pipeline\n",
    "            pipeline = make_pipeline(scaler,kmeans)  # Make_pipeline\n",
    "            # Fit the pipeline to samples\n",
    "            pipeline.fit(samples)\n",
    "            # Calculate the cluster labels: labels\n",
    "            labels = pipeline.predict(samples)\n",
    "            prediction=labels\n",
    "            return prediction\n",
    "        \n",
    "        \n",
    "    def Dendgrogram_Plot(self,TrainData,labels,method='complete'):\n",
    "        #label=[1 for i in range(len(TrainData))]\n",
    "        \n",
    "        from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # Calculate the linkage: mergings:: This performs hierarchical clustering\n",
    "        mergings = linkage(TrainData, method=method)\n",
    "\n",
    "        # Plot the dendrogram\n",
    "        dendrogram(mergings,labels=label,leaf_rotation=90,leaf_font_size=6)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def DendgrogramDistance_SubPlot(self,TrainData,labels,distance=5,method='complete'):\n",
    "        from scipy.cluster.hierarchy import linkage\n",
    "        from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "        samples=TrainData\n",
    "        varieties_=labels\n",
    "       # varieties=['Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat']\n",
    "\n",
    "        #Measures distance.\n",
    "        mergings = linkage(samples,method =method)  # Using above samples\n",
    "\n",
    "        dendrogram(mergings,labels=varieties_,leaf_rotation=90,leaf_font_size=6,)\n",
    "        print(\"Full Dendrogram, No distance limits\")\n",
    "        plt.show()\n",
    "\n",
    "        #Extracting Intermediate Clusters!\n",
    "        labels_f = fcluster(mergings,distance,criterion='distance')\n",
    "        print (labels)\n",
    "        print (\"At distance\",distance,\"Division considered Above this all assign as Indivudial Number\")\n",
    "        dendrogram(mergings,labels=labels_f,leaf_rotation=90,leaf_font_size=6,)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def AllPlots(self,samples,labels,distance):\n",
    "        \n",
    "        #Inertia Plot\n",
    "        Unsupervised.Inertia_Cluster_Graph(self,samples)\n",
    "\n",
    "        #Scatter Plot                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              +\n",
    "        Unsupervised.Scatter_Plot(self,samples[:,0],samples[:,1],labels)\n",
    "        Unsupervised.Scatter_Plot(self,samples[:,2],samples[:,3],labels)\n",
    "\n",
    "        #DendroGram Plot\n",
    "        Unsupervised.DendgrogramDistance_SubPlot(self,samples,labels,distance=distance,method='complete')\n",
    "    \n",
    "    def Tsne(self,samples,labels,learning_rate=100):\n",
    "        from sklearn.manifold import TSNE\n",
    "        model = TSNE(learning_rate=learning_rate)\n",
    "        tsne_features = model.fit_transform(samples)\n",
    "        return tsne_features\n",
    "    def TsnePlot(self,samples,labels_numbers,learning_rate=100):\n",
    "        print(\"Sample input Ex :\",samples[0,:])\n",
    "        tsne_features=Unsupervised.Tsne(self,samples,labels,learning_rate=100)\n",
    "        xs = tsne_features[:,0]\n",
    "        ys = tsne_features[:,1]\n",
    "        print(\"Tsne Output Ex:\",tsne_features[0,:])\n",
    "        plt.scatter(xs, ys, c=labels_numbers)\n",
    "        plt.show()\n",
    "    def TsnePlotLabel(self,samples,labels_numbers,labels_name,learning_rate=100):       \n",
    "        tsne_features=Unsupervised.Tsne(self,samples,labels,learning_rate=100)\n",
    "        xs = tsne_features[:,0]\n",
    "        ys = tsne_features[:,1]\n",
    "        print(\"Sample input Ex :\",samples[0,:])\n",
    "        print(\"Tsne Output Ex:\",tsne_features[0,:])\n",
    "        plt.scatter(xs, ys, c=labels_numbers)\n",
    "        for x, y, company in zip(xs, ys, companies):\n",
    "            plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
    "        plt.show()\n",
    "    \n",
    "    def Pca_DimensionReduce(self,TrainData,n_components=2):\n",
    "        #18\n",
    "        #PCA fit and Transforms  ....  Don't have Predict  . It just Reduce the Dimensions!\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        # Create a PCA model with 2 components: pca\n",
    "        pca = PCA(n_components=n_components)   # n_components = Components Require !!!\n",
    "\n",
    "        # Fit the PCA instance to the scaled samples\n",
    "        pca.fit(TrainData)\n",
    "\n",
    "        # Transform the scaled samples: pca_features\n",
    "        pca_features = pca.transform(TrainData)\n",
    "        \n",
    "        \n",
    "        print(\"Just Reduces the Dimension\",TrainData.shape,\"->\",pca_features.shape)\n",
    "\n",
    "        return pca_features\n",
    "    \n",
    "        \n",
    "    def PcaPlot(self,TrainData):\n",
    "        grains=TrainData\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.decomposition import PCA\n",
    "        \n",
    "        model=PCA()\n",
    "        model.fit(grains)\n",
    "        transformed=model.transform(grains)\n",
    "        ## we can use both to gether  FIT AND TRANSFORM\n",
    "        #  model.fit_transform(grains)\n",
    "        #14.1\n",
    "        print(\"Actual Data\")\n",
    "        plt.scatter(grains[:,0], grains[:,1])\n",
    "        #plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "        #14.2\n",
    "        print(\"\\n PCA Transformed \\n Data Shift to Mean=0  and Rotates\")\n",
    "        plt.scatter(transformed[:,0], transformed[:,1])\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "        #print\n",
    "        print(\"\\n\\nActual Data :\",grains[0,:])\n",
    "        print(\"PCA Transformed Data :\",transformed[0,:])\n",
    "        print(\"Mean of NotTransformed Data:\",model.mean_)\n",
    "        \n",
    "        \n",
    "        mean = model.mean_\n",
    "        print(\"\\n\\nmean :\",mean)\n",
    "\n",
    "        # Get the first principal component: first_pc\n",
    "        first_pc = model.components_[0,:]\n",
    "        print(\"first_pc\",first_pc)\n",
    "        print(\"The first principal component of the data is the direction in which the data varies the most\")\n",
    "        # Plot first_pc as an arrow, starting at mean\n",
    "        plt.scatter(grains[:,0], grains[:,1])\n",
    "        plt.arrow(mean[0],mean[1], first_pc[0], first_pc[1], color='red', width=0.051)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def StandardScalarPca_VariencePlot(self,TrainData):\n",
    "        #16  PCA PLOT\n",
    "        # Perform the necessary imports\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fish_Data=TrainData\n",
    "\n",
    "        # Create scaler: scaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Create a PCA instance: pca\n",
    "        pca = PCA()\n",
    "\n",
    "        # Create pipeline: pipeline\n",
    "        pipeline = make_pipeline(scaler,pca)\n",
    "\n",
    "        # Fit the pipeline to 'samples'\n",
    "        pipeline.fit(fish_Data)\n",
    "\n",
    "        # Plot the explained variances\n",
    "        features =range(pca.n_components_)    ## HIGHLIGHTING\n",
    "        print(\"Features is equal to Input :\",features)\n",
    "        plt.bar(features,pca.explained_variance_)\n",
    "        plt.xlabel('PCA feature')\n",
    "        plt.ylabel('variance')\n",
    "        plt.xticks(features)\n",
    "        plt.show()\n",
    "        print(\" Plot Shows to what Level Dimension Can be reduce\\n Higher the Varience value higher the Results dependence \")\n",
    "        print (\" Select PCA(n_components = Base up on above Graph)\")\n",
    "        \n",
    "    def Tfidf_ConverttoCsr(self,TrainData):\n",
    "        #19 \n",
    "        # return csr_mat\n",
    "        # tfidf\n",
    "        # tfidf -> It transforms a list of documents into a word frequency array, which it outputs as a csr_matri\n",
    "        print(\"tfidf -> converts to Csr_mat\")\n",
    "        print(\"csr matrix reduse the space by remembering only non zero entries \\n\")\n",
    "        documents= TrainData\n",
    "        # Import TfidfVectorizer\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        # Create a TfidfVectorizer: tfidf\n",
    "        tfidf = TfidfVectorizer() \n",
    "\n",
    "        # Apply fit_transform to document: csr_mat\n",
    "        csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "        # Print \n",
    "\n",
    "        words = tfidf.get_feature_names()\n",
    "        print()\n",
    "        print(documents)\n",
    "        print()\n",
    "        print(\"\\nLength :\",len(words))\n",
    "        print(words)\n",
    "        print()\n",
    "        print(\"csr_mat.toarray :\\n\",csr_mat.toarray())\n",
    "        print()\n",
    "        print(\"Csr_Matrix :\\n\",csr_mat)\n",
    "\n",
    "        \n",
    "        # return\n",
    "        return csr_mat\n",
    "    \n",
    "    \n",
    "    def TruncatedSvd_Kmean(self,TrainData_csr_matrix,titles,Svd_components=50,n_clusters=6):\n",
    "        #Svd_components -> Dimension Reduction\n",
    "        #n_clusters -> n_clusters Reducion\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "\n",
    "        # Create a TruncatedSVD instance: svd\n",
    "        svd = TruncatedSVD(n_components=Svd_components)\n",
    "\n",
    "        # Create a KMeans instance: kmeans\n",
    "        kmeans =KMeans(n_clusters=n_clusters)   #\n",
    "\n",
    "        # Create a pipeline: pipeline\n",
    "        pipeline = make_pipeline(svd,kmeans)\n",
    "        \n",
    "        \n",
    "        import pandas as pd\n",
    "        \n",
    "        articles = TrainData_csr_matrix         # Dirct csr matrix\n",
    "        titles = titles\n",
    "\n",
    "       \n",
    "        import pandas as pd\n",
    "\n",
    "        # Fit the pipeline to articles\n",
    "        pipeline.fit(articles)\n",
    "\n",
    "        # Calculate the cluster labels: labels\n",
    "        labels = pipeline.predict(articles)\n",
    "        print(len(labels))\n",
    "        print(\"Labels\",labels)\n",
    "\n",
    "        # Create a DataFrame aligning labels and titles: df\n",
    "        df1 = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "\n",
    "        # Print\n",
    "        print(df1.sort_values('label'))\n",
    "\n",
    "    def Nmf(self,TrainData_csr_matrix,titles,n_components=6):\n",
    "        #21\n",
    "        # NMF   :: Non Negative Matrix Factorization (NMF)   #for Non Negative\n",
    "        #  Dimension Reduction technique \n",
    "        from sklearn.decomposition import NMF\n",
    "\n",
    "        #df = pd.read_csv('wikipedia-vectors.csv', index_col=0)\n",
    "        articles =TrainData_csr_matrix #csr_matrix(df.transpose())         # Dirct csr matrix\n",
    "        titles =titles  #list(df.columns)\n",
    "\n",
    "        # Create an NMF instance: model\n",
    "        model = NMF(n_components=n_components)\n",
    "\n",
    "        # Fit the model to articles\n",
    "        model.fit(articles) #articles is CSR_MATRIX\n",
    "\n",
    "        # Transform the articles: nmf_features\n",
    "        nmf_features = model.transform(articles)\n",
    "\n",
    "        # Print the NMF features\n",
    "        print(nmf_features)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Create a pandas DataFrame: df\n",
    "        df = pd.DataFrame(nmf_features,index=titles)\n",
    "\n",
    "        # Print the row for 'Anne Hathaway'\n",
    "        print(df.iloc[0])\n",
    "\n",
    "        # Print the row for 'Denzel Washington'\n",
    "        print(df.iloc[1])\n",
    "        print(\"Model.Components_\\n\",model.components_)\n",
    "        \n",
    "    def Nmf_CompImshow(self,TrainData,n_components=7):\n",
    "       \n",
    "        samples=TrainData\n",
    "        # Import NMF\n",
    "        from sklearn.decomposition import NMF\n",
    "\n",
    "        # Create an NMF model: model\n",
    "        model = NMF(n_components=n_components)\n",
    "\n",
    "        # Apply fit_transform to samples: features\n",
    "        features = model.fit_transform(samples)\n",
    "\n",
    "        # Call show_as_image on each component\n",
    "        for component in model.components_:\n",
    "            Unsupervised.show_as_image(self,component)\n",
    "\n",
    "        # Select the 0th row of features: digit_features\n",
    "        digit_features = features[0,:]\n",
    "\n",
    "        # Print digit_features\n",
    "        print(digit_features)\n",
    "\n",
    "    \n",
    "    def show_as_image(self,sample):\n",
    "        from matplotlib import pyplot as plt\n",
    "        bitmap = sample.reshape((13, 8))\n",
    "        plt.figure()\n",
    "        plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "    def Lcd_Imshow(self,samples,x=13,y=8):\n",
    "        samples = samples #pd.read_csv('lcd-digits.csv',header=None).values  ## IMPORTANT\n",
    "        # Import pyplot\n",
    "        from matplotlib import pyplot as plt\n",
    "\n",
    "        # Select the 0th row: digit\n",
    "        digit = samples[10,:]\n",
    "\n",
    "        # Print digit\n",
    "        print(digit)\n",
    "\n",
    "        # Reshape digit to a 13x8 array: bitmap\n",
    "        bitmap = digit.reshape(x,y)\n",
    "\n",
    "        # Print bitmap\n",
    "        print(bitmap)\n",
    "\n",
    "        # Use plt.imshow to display bitmap\n",
    "        plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "    def NmfNormalizeDot(self,TrainData_Csr,Article_index=0):\n",
    "        #29\n",
    "        # give similar article types\n",
    "        #29.1\n",
    "        articles=TrainData\n",
    "        from sklearn.decomposition import NMF\n",
    "\n",
    "        # Create an NMF instance: model\n",
    "        model = NMF(n_components=6)\n",
    "\n",
    "        # Fit the model to articles\n",
    "        model.fit(articles)\n",
    "\n",
    "        # Transform the articles: nmf_features\n",
    "        nmf_features = model.transform(articles)\n",
    "\n",
    "        #\n",
    "        # Perform the necessary imports\n",
    "        import pandas as pd\n",
    "        from sklearn.preprocessing import normalize\n",
    "\n",
    "        # Normalize the NMF features: norm_features\n",
    "        norm_features =normalize(nmf_features)\n",
    "        # Create a DataFrame: df\n",
    "        df =pd.DataFrame(norm_features,titles)\n",
    "\n",
    "        # Select the row corresponding to 'Cristiano Ronaldo': article\n",
    "        article = df.iloc[Article_index]     # TO  GET SIMILAR TYPE OF ARTICLES !!!\n",
    "\n",
    "        # Compute the dot products: similarities\n",
    "        similarities = df.dot(article)\n",
    "\n",
    "        # Display those with the largest cosine similarity\n",
    "        print(similarities.nlargest())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised Instance Begin\n",
      "Unsupervised Instance Destroyed\n",
      "Unsupervised Instance Begin\n",
      "Unsupervised Instance Destroyed\n",
      "tfidf -> converts to Csr_mat\n",
      "csr matrix reduse the space by remembering only non zero entries \n",
      "\n",
      "\n",
      "['Massive Attack', 'Sublime', 'Beastie Boys', 'Neil Young', 'Dead Kennedys', 'Orbital', 'Miles Davis', 'Leonard Cohen', 'Van Morrison', 'NOFX', 'Rancid', 'Lamb', 'Korn', 'Dropkick Murphys', 'Bob Dylan', 'Eminem', 'Nirvana', 'Van Halen', 'Damien Rice', 'Elvis Costello', 'Everclear', 'Jimi Hendrix', 'PJ Harvey', 'Red Hot Chili Peppers', 'Ryan Adams', 'Soundgarden', 'The White Stripes', 'Madonna', 'Eric Clapton', 'Bob Marley', 'Dr. Dre', 'The Flaming Lips', 'Tom Waits', 'Moby', 'Cypress Hill', 'Garbage', 'Fear Factory', '50 Cent', 'Ani DiFranco', 'Matchbox Twenty', 'The Police', 'Eagles', 'Phish', 'Stone Temple Pilots', 'Black Sabbath', 'Britney Spears', 'Fatboy Slim', 'System of a Down', 'Simon & Garfunkel', 'Snoop Dogg', 'Aimee Mann', 'Less Than Jake', 'Rammstein', 'Reel Big Fish', 'The Prodigy', 'Pantera', 'Foo Fighters', 'The Beatles', 'Incubus', 'Audioslave', 'Bright Eyes', 'Machine Head', 'AC/DC', 'Dire Straits', 'Motörhead', 'Ramones', 'Slipknot', 'Me First and the Gimme Gimmes', 'Bruce Springsteen', 'Queens of the Stone Age', 'The Chemical Brothers', 'Bon Jovi', 'Goo Goo Dolls', 'Alice in Chains', 'Howard Shore', 'Barenaked Ladies', 'Anti-Flag', 'Nick Cave and the Bad Seeds', 'Static-X', 'Misfits', '2Pac', 'Sparta', 'Interpol', 'The Crystal Method', 'The Beach Boys', 'Goldfrapp', 'Bob Marley & the Wailers', 'Kylie Minogue', 'The Blood Brothers', 'Mirah', 'Ludacris', 'Snow Patrol', 'The Mars Volta', 'Yeah Yeah Yeahs', 'Iced Earth', 'Fiona Apple', 'Rilo Kiley', 'Rufus Wainwright', 'Flogging Molly', 'Hot Hot Heat', 'Dredg', 'Switchfoot', 'Tegan and Sara', 'Rage Against the Machine', 'Keane', 'Jet', 'Franz Ferdinand', 'The Postal Service', 'The Dresden Dolls', 'The Killers', 'Death From Above 1979']\n",
      "\n",
      "192\n",
      "['1979', '2pac', '50', 'above', 'ac', 'adams', 'against', 'age', 'aimee', 'alice', 'and', 'ani', 'anti', 'apple', 'attack', 'audioslave', 'bad', 'barenaked', 'beach', 'beastie', 'beatles', 'big', 'black', 'blood', 'bob', 'bon', 'boys', 'bright', 'britney', 'brothers', 'bruce', 'cave', 'cent', 'chains', 'chemical', 'chili', 'clapton', 'cohen', 'costello', 'crystal', 'cypress', 'damien', 'davis', 'dc', 'dead', 'death', 'difranco', 'dire', 'dogg', 'dolls', 'down', 'dr', 'dre', 'dredg', 'dresden', 'dropkick', 'dylan', 'eagles', 'earth', 'elvis', 'eminem', 'eric', 'everclear', 'eyes', 'factory', 'fatboy', 'fear', 'ferdinand', 'fighters', 'fiona', 'first', 'fish', 'flag', 'flaming', 'flogging', 'foo', 'franz', 'from', 'garbage', 'garfunkel', 'gimme', 'gimmes', 'goldfrapp', 'goo', 'halen', 'harvey', 'head', 'heat', 'hendrix', 'hill', 'hot', 'howard', 'iced', 'in', 'incubus', 'interpol', 'jake', 'jet', 'jimi', 'jovi', 'keane', 'kennedys', 'kiley', 'killers', 'korn', 'kylie', 'ladies', 'lamb', 'leonard', 'less', 'lips', 'ludacris', 'machine', 'madonna', 'mann', 'marley', 'mars', 'massive', 'matchbox', 'me', 'method', 'miles', 'minogue', 'mirah', 'misfits', 'moby', 'molly', 'morrison', 'motörhead', 'murphys', 'neil', 'nick', 'nirvana', 'nofx', 'of', 'orbital', 'pantera', 'patrol', 'peppers', 'phish', 'pilots', 'pj', 'police', 'postal', 'prodigy', 'queens', 'rage', 'rammstein', 'ramones', 'rancid', 'red', 'reel', 'rice', 'rilo', 'rufus', 'ryan', 'sabbath', 'sara', 'seeds', 'service', 'shore', 'simon', 'slim', 'slipknot', 'snoop', 'snow', 'soundgarden', 'sparta', 'spears', 'springsteen', 'static', 'stone', 'straits', 'stripes', 'sublime', 'switchfoot', 'system', 'tegan', 'temple', 'than', 'the', 'tom', 'twenty', 'van', 'volta', 'wailers', 'wainwright', 'waits', 'white', 'yeah', 'yeahs', 'young']\n",
      "\n",
      "csr_mat.toarray :\n",
      " [[ 0.   0.   0.  ...,  0.   0.   0. ]\n",
      " [ 0.   0.   0.  ...,  0.   0.   0. ]\n",
      " [ 0.   0.   0.  ...,  0.   0.   0. ]\n",
      " ..., \n",
      " [ 0.   0.   0.  ...,  0.   0.   0. ]\n",
      " [ 0.   0.   0.  ...,  0.   0.   0. ]\n",
      " [ 0.5  0.   0.  ...,  0.   0.   0. ]]\n",
      "\n",
      "Csr_Matrix :\n",
      "   (0, 117)\t0.707106781187\n",
      "  (0, 14)\t0.707106781187\n",
      "  (1, 174)\t1.0\n",
      "  (2, 19)\t0.736181835888\n",
      "  (2, 26)\t0.676783794508\n",
      "  (3, 130)\t0.707106781187\n",
      "  (3, 191)\t0.707106781187\n",
      "  (4, 44)\t0.707106781187\n",
      "  (4, 101)\t0.707106781187\n",
      "  (5, 135)\t1.0\n",
      "  (6, 121)\t0.707106781187\n",
      "  (6, 42)\t0.707106781187\n",
      "  (7, 108)\t0.707106781187\n",
      "  (7, 37)\t0.707106781187\n",
      "  (8, 183)\t0.676783794508\n",
      "  (8, 127)\t0.736181835888\n",
      "  (9, 133)\t1.0\n",
      "  (10, 149)\t1.0\n",
      "  (11, 107)\t1.0\n",
      "  (12, 104)\t1.0\n",
      "  (13, 55)\t0.707106781187\n",
      "  (13, 129)\t0.707106781187\n",
      "  (14, 24)\t0.652940035081\n",
      "  (14, 56)\t0.757409605556\n",
      "  (15, 60)\t1.0\n",
      "  :\t:\n",
      "  (100, 53)\t1.0\n",
      "  (101, 175)\t1.0\n",
      "  (102, 10)\t0.520494998339\n",
      "  (102, 177)\t0.603773532338\n",
      "  (102, 157)\t0.603773532338\n",
      "  (103, 180)\t0.311031026281\n",
      "  (103, 112)\t0.517987397884\n",
      "  (103, 146)\t0.563448647316\n",
      "  (103, 6)\t0.563448647316\n",
      "  (104, 100)\t1.0\n",
      "  (105, 97)\t1.0\n",
      "  (106, 76)\t0.707106781187\n",
      "  (106, 67)\t0.707106781187\n",
      "  (107, 180)\t0.363613864037\n",
      "  (107, 143)\t0.658705153267\n",
      "  (107, 159)\t0.658705153267\n",
      "  (108, 180)\t0.376482014521\n",
      "  (108, 49)\t0.62698870072\n",
      "  (108, 54)\t0.682016467479\n",
      "  (109, 180)\t0.483271263248\n",
      "  (109, 103)\t0.875470665481\n",
      "  (110, 45)\t0.5\n",
      "  (110, 77)\t0.5\n",
      "  (110, 3)\t0.5\n",
      "  (110, 0)\t0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<111x192 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 222 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Unsupervised()\n",
    "# # TruncadedSVD Do take csr Matrix\n",
    "# print(\"tfidf -> converts to Csr_mat\")\n",
    "# print(\"csr matrix reduse the space by remembering only non zero entries \\n\")\n",
    "# documents= ['cats say meow', 'dogs say woof', 'dogs chase cats']\n",
    "# # Import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Create a TfidfVectorizer: tfidf\n",
    "# tfidf = TfidfVectorizer() \n",
    "\n",
    "# # Apply fit_transform to document: csr_mat\n",
    "# csr_mat = tfidf.fit_transform(documents)\n",
    "# print(tfidf.get_feature_names())\n",
    "# list= ['Artcl_1', 'Artcl_2', 'Artcl_3']\n",
    "# #x.TruncatedSvd_Kmean(csr_mat,list,5,2)\n",
    "# x.Nmf(csr_mat,list,n_components=6)\n",
    "# x.Lcd_Imshow(pd.read_csv('lcd-digits.csv',header=None).values,x=13,y=8)\n",
    "\n",
    "\n",
    "# #2\n",
    "# x=Unsupervised()\n",
    "# x.Nmf_CompImshow(pd.read_csv('lcd-digits.csv',header=None).values,7)\n",
    "# df = pd.read_csv('wikipedia-vectors.csv', index_col=0)\n",
    "# articles = csr_matrix(df.transpose())         # Dirct csr matrix\n",
    "# titles = list(df.columns)\n",
    "# x.NmfNormalizeDot(self,articles,Article_index=0)\n",
    "\n",
    "\n",
    "#3\n",
    "\n",
    "artist_names=pd.read_csv('Music_artists.csv',header=None)\n",
    "a=pd.read_csv('Music_scrobbler-small-sample.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
