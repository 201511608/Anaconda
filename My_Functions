# UnSupervised Class

# My UnSupervised Learning Functions
class Unsupervised:
    
    def __init__( self):
        print("Unsupervised Instance Begin") 
        
    def __del__(self):
       print("Unsupervised Instance Destroyed")
    
    def check(self):
        print("In Unsupervised class")
    
    #plots
    def Scatter_Plot(self,x,y,labels=[]):
       # labels=[1 for i in range(len(x))]
        plt.scatter(x,y,c=labels,alpha =0.75)
       # plt.scatter(centroids[:,0],centroids[:,1],color='r',marker='D',s=50)
        plt.show()
        
    def Scatter_Plot_WithCg(self,cgx,cgy,x,y,labels=[]):
        # centroids=model.cluster_centers_  # centroids[:,0],centroids[:,1] 
        #labels=[1 for i in range(len(x))]
        plt.scatter(x,y,c=labels,alpha =0.75)
        plt.scatter(cgx,cgy,color='r',marker='D',s=50)
        plt.show()
        
        
    def Inertia_Cluster_Graph(self,Train_Data):
        sm_data=Train_Data
        ks = range(1, 15)
        inertias = []
        for k in ks:
            # Create a KMeans instance with k clusters: model
            modle=KMeans(n_clusters=k)    
            # Fit model to samples
            modle.fit(sm_data)  
            # Append the inertia to the list of inertias
            inertias.append(modle.inertia_)
        plt.plot(ks, inertias, '-o')
        plt.xlabel('Number of Clusters, k')
        plt.ylabel('Inertia')
        plt.xticks(ks)
        plt.show() 
        
        
    def crosstable(predictedData,targetData):
        import pandas as pd
        ct=pd.crosstab(predictedData,targetData)
        print("\nCrossTable\n",ct)
        
    def Normalizer_Kmean(self,Train_Data,Prediction_Data,clusters=5):    
        ##import numpy as np
        from sklearn.preprocessing import Normalizer
        from sklearn.cluster import KMeans

        # Market_Data=pd.read_csv('sharemarket_data.csv',header=0,usecols= [x for x in range(1,963)])
        ##companiessm_data=np.loadtxt('sharemarket_data.txt') #sharemarket_data
        ##companies=pd.read_csv('sharemarket_data.csv',header=0,usecols= [0]).values.reshape(60)

        #7.1
        normalizer= Normalizer() # Normalizing the Data
        kmeans= KMeans(n_clusters=clusters)   # Change Cluster and check the results
        pipeline = make_pipeline(normalizer,kmeans)
        pipeline.fit(Train_Data)

        #7.2
        prediction =pipeline.predict(Prediction_Data)
        return prediction
    
    def StandardScaler_Kmean(self,Train_Data,Prediction_Data,clusters=5):                     
            from sklearn.pipeline import make_pipeline
            from sklearn.preprocessing import StandardScaler
            from sklearn.cluster import KMeans
            
            # StandardScaler  MaxAbsScalar   Normilizer
            scaler = StandardScaler()
            # Create KMeans instance: kmeans
            kmeans = KMeans(n_clusters=clusters)


            # Create pipeline: pipeline
            pipeline = make_pipeline(scaler,kmeans)  # Make_pipeline
            # Fit the pipeline to samples
            pipeline.fit(samples)
            # Calculate the cluster labels: labels
            labels = pipeline.predict(samples)
            prediction=labels
            return prediction
        
        
    def Dendgrogram_Plot(self,TrainData,labels,method='complete'):
        #label=[1 for i in range(len(TrainData))]
        
        from scipy.cluster.hierarchy import linkage, dendrogram
        import matplotlib.pyplot as plt

        # Calculate the linkage: mergings:: This performs hierarchical clustering
        mergings = linkage(TrainData, method=method)

        # Plot the dendrogram
        dendrogram(mergings,labels=label,leaf_rotation=90,leaf_font_size=6)
        plt.show()




    def DendgrogramDistance_SubPlot(self,TrainData,labels,distance=5,method='complete'):
        from scipy.cluster.hierarchy import linkage
        from scipy.cluster.hierarchy import fcluster

        samples=TrainData
        varieties_=labels
       # varieties=['Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat']

        #Measures distance.
        mergings = linkage(samples,method =method)  # Using above samples

        dendrogram(mergings,labels=varieties_,leaf_rotation=90,leaf_font_size=6,)
        print("Full Dendrogram, No distance limits")
        plt.show()

        #Extracting Intermediate Clusters!
        labels_f = fcluster(mergings,distance,criterion='distance')
        print (labels)
        print ("At distance",distance,"Division considered Above this all assign as Indivudial Number")
        dendrogram(mergings,labels=labels_f,leaf_rotation=90,leaf_font_size=6,)
        plt.show()



    def AllPlots(self,samples,labels,distance):
        
        #Inertia Plot
        Unsupervised.Inertia_Cluster_Graph(self,samples)

        #Scatter Plot                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              +
        Unsupervised.Scatter_Plot(self,samples[:,0],samples[:,1],labels)
        Unsupervised.Scatter_Plot(self,samples[:,2],samples[:,3],labels)

        #DendroGram Plot
        Unsupervised.DendgrogramDistance_SubPlot(self,samples,labels,distance=distance,method='complete')
    
    def Tsne(self,samples,labels,learning_rate=100):
        from sklearn.manifold import TSNE
        model = TSNE(learning_rate=learning_rate)
        tsne_features = model.fit_transform(samples)
        return tsne_features
    def TsnePlot(self,samples,labels_numbers,learning_rate=100):
        print("Sample input Ex :",samples[0,:])
        tsne_features=Unsupervised.Tsne(self,samples,labels,learning_rate=100)
        xs = tsne_features[:,0]
        ys = tsne_features[:,1]
        print("Tsne Output Ex:",tsne_features[0,:])
        plt.scatter(xs, ys, c=labels_numbers)
        plt.show()
    def TsnePlotLabel(self,samples,labels_numbers,labels_name,learning_rate=100):       
        tsne_features=Unsupervised.Tsne(self,samples,labels,learning_rate=100)
        xs = tsne_features[:,0]
        ys = tsne_features[:,1]
        print("Sample input Ex :",samples[0,:])
        print("Tsne Output Ex:",tsne_features[0,:])
        plt.scatter(xs, ys, c=labels_numbers)
        for x, y, company in zip(xs, ys, companies):
            plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
        plt.show()
    
    def Pca_DimensionReduce(self,TrainData,n_components=2):
        #18
        #PCA fit and Transforms  ....  Don't have Predict  . It just Reduce the Dimensions!
        from sklearn.decomposition import PCA

        # Create a PCA model with 2 components: pca
        pca = PCA(n_components=n_components)   # n_components = Components Require !!!

        # Fit the PCA instance to the scaled samples
        pca.fit(TrainData)

        # Transform the scaled samples: pca_features
        pca_features = pca.transform(TrainData)
        
        
        print("Just Reduces the Dimension",TrainData.shape,"->",pca_features.shape)

        return pca_features
    
        
    def PcaPlot(self,TrainData):
        grains=TrainData
        import numpy as np
        import matplotlib.pyplot as plt
        from sklearn.decomposition import PCA
        
        model=PCA()
        model.fit(grains)
        transformed=model.transform(grains)
        ## we can use both to gether  FIT AND TRANSFORM
        #  model.fit_transform(grains)
        #14.1
        print("Actual Data")
        plt.scatter(grains[:,0], grains[:,1])
        #plt.axis('equal')
        plt.show()

        #14.2
        print("\n PCA Transformed \n Data Shift to Mean=0  and Rotates")
        plt.scatter(transformed[:,0], transformed[:,1])
        plt.axis('equal')
        plt.show()

        #print
        print("\n\nActual Data :",grains[0,:])
        print("PCA Transformed Data :",transformed[0,:])
        print("Mean of NotTransformed Data:",model.mean_)
        
        
        mean = model.mean_
        print("\n\nmean :",mean)

        # Get the first principal component: first_pc
        first_pc = model.components_[0,:]
        print("first_pc",first_pc)
        print("The first principal component of the data is the direction in which the data varies the most")
        # Plot first_pc as an arrow, starting at mean
        plt.scatter(grains[:,0], grains[:,1])
        plt.arrow(mean[0],mean[1], first_pc[0], first_pc[1], color='red', width=0.051)
        plt.show()
        
        
    def StandardScalarPca_VariencePlot(self,TrainData):
        #16  PCA PLOT
        # Perform the necessary imports
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler
        from sklearn.pipeline import make_pipeline
        import matplotlib.pyplot as plt

        fish_Data=TrainData

        # Create scaler: scaler
        scaler = StandardScaler()

        # Create a PCA instance: pca
        pca = PCA()

        # Create pipeline: pipeline
        pipeline = make_pipeline(scaler,pca)

        # Fit the pipeline to 'samples'
        pipeline.fit(fish_Data)

        # Plot the explained variances
        features =range(pca.n_components_)    ## HIGHLIGHTING
        print("Features is equal to Input :",features)
        plt.bar(features,pca.explained_variance_)
        plt.xlabel('PCA feature')
        plt.ylabel('variance')
        plt.xticks(features)
        plt.show()
        print(" Plot Shows to what Level Dimension Can be reduce\n Higher the Varience value higher the Results dependence ")
        print (" Select PCA(n_components = Base up on above Graph)")
        
    def Tfidf_ConverttoCsr(self,TrainData):
        #19 
        # return csr_mat
        # tfidf
        # tfidf -> It transforms a list of documents into a word frequency array, which it outputs as a csr_matri
        print("tfidf -> converts to Csr_mat")
        print("csr matrix reduse the space by remembering only non zero entries \n")
        documents= TrainData
        # Import TfidfVectorizer
        from sklearn.feature_extraction.text import TfidfVectorizer

        # Create a TfidfVectorizer: tfidf
        tfidf = TfidfVectorizer() 

        # Apply fit_transform to document: csr_mat
        csr_mat = tfidf.fit_transform(documents)

        # Print 

        words = tfidf.get_feature_names()
        print()
        print(documents)
        print()
        print("\nLength :",len(words))
        print(words)
        print()
        print("csr_mat.toarray :\n",csr_mat.toarray())
        print()
        print("Csr_Matrix :\n",csr_mat)

        
        # return
        return csr_mat
    
    
    def TruncatedSvd_Kmean(self,TrainData_csr_matrix,titles,Svd_components=50,n_clusters=6):
        #Svd_components -> Dimension Reduction
        #n_clusters -> n_clusters Reducion
        from sklearn.decomposition import TruncatedSVD
        from sklearn.cluster import KMeans
        from sklearn.pipeline import make_pipeline

        # Create a TruncatedSVD instance: svd
        svd = TruncatedSVD(n_components=Svd_components)

        # Create a KMeans instance: kmeans
        kmeans =KMeans(n_clusters=n_clusters)   #

        # Create a pipeline: pipeline
        pipeline = make_pipeline(svd,kmeans)
        
        
        import pandas as pd
        
        articles = TrainData_csr_matrix         # Dirct csr matrix
        titles = titles

       
        import pandas as pd

        # Fit the pipeline to articles
        pipeline.fit(articles)

        # Calculate the cluster labels: labels
        labels = pipeline.predict(articles)
        print(len(labels))
        print("Labels",labels)

        # Create a DataFrame aligning labels and titles: df
        df1 = pd.DataFrame({'label': labels, 'article': titles})


        # Print
        print(df1.sort_values('label'))

    def Nmf(self,TrainData_csr_matrix,titles,n_components=6):
        #21
        # NMF   :: Non Negative Matrix Factorization (NMF)   #for Non Negative
        #  Dimension Reduction technique 
        from sklearn.decomposition import NMF

        #df = pd.read_csv('wikipedia-vectors.csv', index_col=0)
        articles =TrainData_csr_matrix #csr_matrix(df.transpose())         # Dirct csr matrix
        titles =titles  #list(df.columns)

        # Create an NMF instance: model
        model = NMF(n_components=n_components)

        # Fit the model to articles
        model.fit(articles) #articles is CSR_MATRIX

        # Transform the articles: nmf_features
        nmf_features = model.transform(articles)

        # Print the NMF features
        print(nmf_features)

        
        
        # Create a pandas DataFrame: df
        df = pd.DataFrame(nmf_features,index=titles)

        # Print the row for 'Anne Hathaway'
        print(df.iloc[0])

        # Print the row for 'Denzel Washington'
        print(df.iloc[1])
        print("Model.Components_\n",model.components_)
        
    def Nmf_CompImshow(self,TrainData,n_components=7):
       
        samples=TrainData
        # Import NMF
        from sklearn.decomposition import NMF

        # Create an NMF model: model
        model = NMF(n_components=n_components)

        # Apply fit_transform to samples: features
        features = model.fit_transform(samples)

        # Call show_as_image on each component
        for component in model.components_:
            Unsupervised.show_as_image(self,component)

        # Select the 0th row of features: digit_features
        digit_features = features[0,:]

        # Print digit_features
        print(digit_features)

    
    def show_as_image(self,sample):
        from matplotlib import pyplot as plt
        bitmap = sample.reshape((13, 8))
        plt.figure()
        plt.imshow(bitmap, cmap='gray', interpolation='nearest')
        plt.colorbar()
        plt.show()
        
    def Lcd_Imshow(self,samples,x=13,y=8):
        samples = samples #pd.read_csv('lcd-digits.csv',header=None).values  ## IMPORTANT
        # Import pyplot
        from matplotlib import pyplot as plt

        # Select the 0th row: digit
        digit = samples[10,:]

        # Print digit
        print(digit)

        # Reshape digit to a 13x8 array: bitmap
        bitmap = digit.reshape(x,y)

        # Print bitmap
        print(bitmap)

        # Use plt.imshow to display bitmap
        plt.imshow(bitmap, cmap='gray', interpolation='nearest')
        plt.colorbar()
        plt.show()
    
    def NmfNormalizeDot(self,TrainData_Csr,Article_index=0):
        #29
        # give similar article types
        #29.1
        articles=TrainData
        from sklearn.decomposition import NMF

        # Create an NMF instance: model
        model = NMF(n_components=6)

        # Fit the model to articles
        model.fit(articles)

        # Transform the articles: nmf_features
        nmf_features = model.transform(articles)

        #
        # Perform the necessary imports
        import pandas as pd
        from sklearn.preprocessing import normalize

        # Normalize the NMF features: norm_features
        norm_features =normalize(nmf_features)
        # Create a DataFrame: df
        df =pd.DataFrame(norm_features,titles)

        # Select the row corresponding to 'Cristiano Ronaldo': article
        article = df.iloc[Article_index]     # TO  GET SIMILAR TYPE OF ARTICLES !!!

        # Compute the dot products: similarities
        similarities = df.dot(article)

        # Display those with the largest cosine similarity
        print(similarities.nlargest())


# Common Functions

#Numpy array to DataFrame
def Array2DataFrame(array):
    import numpy as np
    df = pd.DataFrame(data=array)
    return df

#Numpy array to DataFrame to SaveFile
def Array2SaveCSV(NpArray,filename='Delete'):
    import numpy as np
    df = pd.DataFrame(data=NpArray)
    df.to_csv(filename)