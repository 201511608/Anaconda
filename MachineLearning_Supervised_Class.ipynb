{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: 4 \n",
    "##################################################################################################################\n",
    "#####################################################################################\n",
    "# Supervised Class\n",
    "    # :: 1   \n",
    "    # :: 2   \n",
    "\n",
    "\n",
    "class Supervised:\n",
    "    \n",
    "    def __init__( self):\n",
    "        print(\"Supervised Instance Begin\") \n",
    "        \n",
    "    def __del__(self):\n",
    "        print(\"Supervised Instance Destroyed\")\n",
    "    \n",
    "    def check():\n",
    "        print(\"In Supervised class\")\n",
    "    \n",
    "    # My Supervised Learning Functions\n",
    "    \n",
    "    def Exploratory_Data_Analysis_():\n",
    "        #3\n",
    "        #Example\n",
    "        # Iris Data -> Print & Visualise\n",
    "        # Exploratory data analysis (EDA)\n",
    "        from sklearn import datasets                       #Import Datasets  #Check different types of data sets with in it !\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.style.use('ggplot')\n",
    "        iris = datasets.load_iris()\n",
    "\n",
    "        #print\n",
    "        print(\"type(iris) :\",type(iris))  # -Bunch -> Similar to Dictonary\n",
    "        print(\"\\niris.keys() :\",iris.keys())\n",
    "        print(\"\\ntype(iris.data) :\",type(iris.data),\"type(iris.target) :\",type(iris.target))\n",
    "        print(\"\\niris.data.shape :\",iris.data.shape)                                # petal Length and width  sepal length and width\n",
    "        print(\"\\niris.target_names :\",iris.target_names)                            # 0 1 2  ->  setosa versicolor virginica\n",
    "\n",
    "        x=iris.data\n",
    "        y=iris.target\n",
    "        print(\"x  :\",type(x))\n",
    "        df=pd.DataFrame(x, columns = iris.feature_names)    # Create DataFrame\n",
    "\n",
    "        #print\n",
    "        print('\\ndf.head() :\\n',df.head())\n",
    "        print('\\ndf.info() :\\n',df.info())\n",
    "        print('\\ndf.describe() :\\n',df.describe())\n",
    "\n",
    "        #visual Data\n",
    "        #c=y -> Colour by species\n",
    "        _=pd.scatter_matrix(df,c=y,figsize = [10,10],s=150,marker ='D')   # Visual Plot                     !Important!\n",
    "        plt.show()                                                        # Histogram and Scatter Plot\n",
    "        \n",
    "        \n",
    "    def Exploratory_Data_Analysis(df_X,df_Y):\n",
    "        #3\n",
    "        #Example\n",
    "        # Iris Data -> Print & Visualise\n",
    "        # Exploratory data analysis (EDA)\n",
    "        from sklearn import datasets                       #Import Datasets  #Check different types of data sets with in it !\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.style.use('ggplot')\n",
    "        \n",
    "        #\n",
    "        ##\n",
    "        # print\n",
    "        print('\\ndf.head() :\\n',df.head())\n",
    "        print('\\ndf.info() :\\n',df.info())\n",
    "        print('\\ndf.describe() :\\n',df.describe())\n",
    "\n",
    "        #visual Data\n",
    "        #c=y -> Colour by species\n",
    "        _=pd.scatter_matrix(df_X,c=df_Y,figsize = [10,10],s=150,marker ='D')   # Visual Plot                     !Important!\n",
    "        plt.show()             \n",
    "        \n",
    "        ## Ex\n",
    "        # import numpy as np\n",
    "        # import pandas as pd\n",
    "        # df =pd.read_csv('Data_files/US_Voting_Filtered_Data_.csv')\n",
    "        # Exploratory_Data_Analysis(df.drop(['Unnamed: 0','diabetes'],axis =1),df['diabetes'])\n",
    "    def SeaBorn_Plot_():\n",
    "    #  4.2\n",
    "    #  seaborn plot library\n",
    "    #  Example\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('Data_files/US_Voting_Data.csv')\n",
    "\n",
    "        plt.figure(1)\n",
    "        sns.countplot(x='education', hue='party', data=df, palette='RdBu')\n",
    "        plt.xticks([0,1], ['No', 'Yes'])\n",
    "        print(\" Democrats voted resoundingly against this bill, compared to Republicans\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(2)\n",
    "        sns.countplot(x='satellite', hue='party', data=df, palette='RdBu')\n",
    "        plt.xticks([0,1], ['No', 'Yes'])\n",
    "        print(\" Democrats voted in favor of  'satellite\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        sns.countplot(x='missile', hue='party', data=df, palette='RdBu')\n",
    "        plt.xticks([0,1], ['No', 'Yes'])\n",
    "        print(\" Democrats voted in favor of 'missile'\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        sns.countplot(x='infants', hue='party', data=df, palette='RdBu')\n",
    "        plt.xticks([0,1], ['No', 'Yes'])\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        sns.countplot(x='water', hue='party', data=df, palette='RdBu')\n",
    "        plt.xticks([0,1], ['No', 'Yes'])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "        sns.countplot(x='religious', hue='party', data=df, palette='RdBu')\n",
    "        plt.xticks([0,1], ['No', 'Yes'])\n",
    "        plt.show()     \n",
    "        \n",
    "    def SeaBorn_Plot(df,columnName_1,columnName_2,No_label='No',Yes_label='Yes'):\n",
    "        #\n",
    "        #  4.2\n",
    "        #  seaborn plot library\n",
    "        #  Example\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        import pandas as pd\n",
    "        df=df\n",
    "\n",
    "        plt.figure(1)\n",
    "        sns.countplot(x=columnName_1, hue=columnName_2, data=df, palette='RdBu')\n",
    "        plt.xticks([0,1], [No_label, Yes_label])\n",
    "        plt.show()\n",
    "\n",
    "        ## EX:\n",
    "        # df=pd.read_csv('Data_files/US_Voting_Data.csv')\n",
    "        # Supervised.SeaBorn_Plot(df,'education','party')\n",
    "    def KNearestNeighbors_():\n",
    "        # 5\n",
    "        # Classification\n",
    "        # K-Nearest Neighbors\n",
    "                # k 5 -> max of 5 points\n",
    "                # Make a Sets of Decision Bountaries\n",
    "                # Higer n_neighbors Low complex   Less Smoot Boundary\n",
    "                # Lower n_neighbors High Complex  High Smoot Boundary\n",
    "        # On iris data\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        from sklearn import datasets                       #Import Datasets  \n",
    "\n",
    "        iris = datasets.load_iris()\n",
    "\n",
    "        knn= KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "        knn.fit(iris['data'],iris['target'])   # inputs as Numpy/pandasDataFrame\n",
    "\n",
    "        prediction = knn.predict(iris['data'])\n",
    "\n",
    "        print('Prediction :',prediction)\n",
    "        print('target',iris.target)\n",
    "\n",
    "    def KNearestNeighbors(x_train,y_target):\n",
    "        # 6\n",
    "        # Classification\n",
    "        # K-Nearest Neighbors\n",
    "                # k 5 -> max of 5 points\n",
    "                # Make a Sets of Decision Bountaries\n",
    "                # Higer n_neighbors Low complex   Less Smoot Boundary\n",
    "                # Lower n_neighbors High Complex  High Smoot Boundary\n",
    "        # On iris data\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        from sklearn import datasets                       #Import Datasets  \n",
    "\n",
    "        iris = datasets.load_iris()\n",
    "\n",
    "        knn= KNeighborsClassifier(n_neighbors=n)\n",
    "\n",
    "        knn.fit(x_train,y_target)   # inputs as Numpy/pandasDataFrame\n",
    "\n",
    "        prediction = knn.predict(x_train)\n",
    "\n",
    "        print('Prediction :',prediction)\n",
    "        print('target',y_target)\n",
    "\n",
    "\n",
    "    def MNIST():\n",
    "        #9\n",
    "        #MNIST\n",
    "        #Redused version of data is obtain from Sklearn\n",
    "\n",
    "\n",
    "\n",
    "        from sklearn import datasets\n",
    "        digits = datasets.load_digits()\n",
    "\n",
    "        print(\"digits.key  :\",digits.keys())\n",
    "        print(\"\\ndigits.DESCR  :\",digits.DESCR)\n",
    "        print(\"\\ndigits.images.shape  :\",digits.images.shape)\n",
    "        print(\"\\ndigits.data.shape   :\",digits.data.shape)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.show()\n",
    "        #9.1\n",
    "        # Create feature and target arrays\n",
    "        X = digits.data\n",
    "        y = digits.target\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "        # Fit\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # accuracy\n",
    "        print(\"Score : \",knn.score(X_test,y_test))\n",
    "        \n",
    "    def Overfitting_Underfitting_():\n",
    "        # 10\n",
    "        # Overfitting and underfitting\n",
    "        # Setup arrays to store train and test accuracies\n",
    "        import numpy as np\n",
    "        neighbors = np.arange(1, 10)\n",
    "        train_accuracy = np.empty(len(neighbors))\n",
    "        test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "        # Loop over different values of k\n",
    "        for i, k in enumerate(neighbors):\n",
    "\n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            knn.fit(X_train,y_train)\n",
    "\n",
    "            #Accuracy on the training set\n",
    "            train_accuracy[i] = knn.score(X_train, y_train)\n",
    "            #Accuracy on the testing set\n",
    "            test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "        #Plot\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.title('k-NN: Varying Number of Neighbors')\n",
    "        plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "        plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Number of Neighbors')\n",
    "        plt.ylabel('Accuracy')\n",
    "        print(\"        Left Overfitting --- Right underfitting\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def LinearRegression_():\n",
    "        #12\n",
    "        #Linear Model\n",
    "        #Regression\n",
    "        # House Data Train Base up on Room data only !\n",
    "        import matplotlib.pyplot as plt\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        boston = pd.read_csv('data_files/Boston_House_Data.csv')\n",
    "        X= boston.drop('MEDV', axis =1).values\n",
    "        y= boston.MEDV.values\n",
    "        # RM room features only for training  # Not all features for trianing  \n",
    "        X_rooms = X[:,5]  #i.e. RM\n",
    "        y = y.reshape(-1,1)\n",
    "        X_rooms= X_rooms.reshape(-1,1)\n",
    "\n",
    "\n",
    "        from sklearn import linear_model\n",
    "        reg = linear_model.LinearRegression()\n",
    "\n",
    "        reg.fit(X_rooms, y)\n",
    "        prediction_space = np.linspace(min(X_rooms),max(X_rooms)).reshape(-1, 1) # Min to Max\n",
    "\n",
    "        plt.scatter(X_rooms,y,color='blue')\n",
    "        plt.plot(prediction_space,reg.predict(prediction_space),color='black',linewidth=3) # [line]to show min to max room to price\n",
    "        plt.show()\n",
    "        print(\"Score :\",reg.score(X_rooms,y))\n",
    "\n",
    "    def LinearREgression_CVSplit_():\n",
    "         \n",
    "        #18\n",
    "        # Cross Validation  (CV)    #the more computationally expensive cross-validation becomes\n",
    "        # Fold  : Split into folds   1 2 3 4 5     \n",
    "        # 1 Test    2 3 4 5 Train\n",
    "        # 2 Test    1 3 4 5 Train\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "        y = df.life.values.reshape(-1,1)\n",
    "        X= df.drop([\"life\",\"Region\"],axis=1).values\n",
    "\n",
    "\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        reg = LinearRegression()\n",
    "\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        cv_results = cross_val_score(reg, X,  y, cv=5)     # Return Cross Validation Score\n",
    "        print(\"Score :\",cv_results)\n",
    "        import numpy as np\n",
    "        print(\"Mean :\",np.mean(cv_results))\n",
    "    def RidgeRegressoin_Split_():\n",
    "\n",
    "        #21\n",
    "        # Ridge Regression \n",
    "        # lOSS FUNCTION    [OLS+(ALPHA)*ai^2]         alpha = 0 overfitting  alpha =inf   underfitting\n",
    "        # Alpha controls model complexity\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "        y = df.life.values.reshape(-1,1)\n",
    "        X= df.drop([\"life\",\"Region\"],axis=1).values\n",
    "\n",
    "\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "        from sklearn.linear_model import Ridge\n",
    "        ridge = Ridge(alpha=0.1,normalize=True)   # (alpha and normalize ?)\n",
    "        ridge.fit(X_train, y_train)\n",
    "        ridge_pred=ridge.predict(X_test)\n",
    "        ridge.score(X_test, y_test)\n",
    "    \n",
    "    \n",
    "    def Reshaping_():\n",
    "        # 13\n",
    "        # reshaping !!\n",
    "        # Gapminder_Region_data  \n",
    "        # to predict life expectensy for different region \n",
    "\n",
    "        #-> Traindata [ []  []  []  [] ]   -> Target  [  []  []  [] ]     || ID to ID  ||\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "\n",
    "\n",
    "        y = df.life.values\n",
    "        X = df.fertility.values\n",
    "        tem=y\n",
    "        print(\"Dimensions of y before reshaping: {}\".format(y.shape))\n",
    "        print(\"Dimensions of X before reshaping: {}\".format(X.shape))\n",
    "\n",
    "        # Reshape X and y\n",
    "        y = y.reshape(-1,1)\n",
    "        X = X.reshape(-1,1)\n",
    "\n",
    "        print(\"Dimensions of y after reshaping: {}\".format(y.shape))\n",
    "        print(\"Dimensions of X after reshaping: {}\".format(X.shape))\n",
    "        print(tem)\n",
    "        print(y)\n",
    "\n",
    "    def HeatMap_():\n",
    "        # 14\n",
    "        # Gapminder Data\n",
    "        # Heat Map:\n",
    "        # Green show positive correlation, Red show negative correlation\n",
    "        import seaborn as sns\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv('Gapminder_Region_data.csv')\n",
    "\n",
    "        # Heat Map Plot\n",
    "        plt.figure(1)\n",
    "        sns.heatmap(df.corr(), square=True, cmap='RdYlGn')\n",
    "        print(\"Green show positive correlation, Red show negative correlation\")\n",
    "        plt.show()\n",
    "        print(df.head(),df.info(),df.describe())\n",
    "    \n",
    "    def LassoRegression_():\n",
    "        #22\n",
    "        # Lasso Regression \n",
    "        # lOSS FUNCTION    [OLS+(ALPHA)*|ai|]         alpha = 0 overfitting  alpha =inf   underfitting\n",
    "        # Alpha controls model complexity\n",
    "        # Can select important features of dataset : shrinks less important data to almost 0\n",
    "\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "        y = df.life.values.reshape(-1,1)\n",
    "        X= df.drop([\"life\",\"Region\"],axis=1).values\n",
    "\n",
    "\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "        from sklearn.linear_model import Lasso\n",
    "        lasso = Lasso(alpha=0.1,normalize=True)   # (alpha and normalize ?)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        lasso_pred=ridge.predict(X_test)\n",
    "        lasso.score(X_test, y_test)     \n",
    "    def LassoRegression_ImpFeatures_():\n",
    "        #22\n",
    "        # Lasso Regression \n",
    "        # lOSS FUNCTION    [OLS+(ALPHA)*|ai|]         alpha = 0 overfitting  alpha =inf   underfitting\n",
    "        # Alpha controls model complexity\n",
    "        # Can select important features of dataset : shrinks less important data to almost 0\n",
    "\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "        y = df.life.values.reshape(-1,1)\n",
    "        X= df.drop([\"life\",\"Region\"],axis=1).values\n",
    "\n",
    "\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "        from sklearn.linear_model import Lasso\n",
    "        lasso = Lasso(alpha=0.1,normalize=True)   # (alpha and normalize ?)\n",
    "        \n",
    "        # 23\n",
    "        # Using Lasso .coef_  \n",
    "        # Can Determinte which is importan features\n",
    "        lasso_coef = lasso.fit(X_train,y_train).coef_\n",
    "        names= df.drop([\"life\",\"Region\"],axis=1).columns\n",
    "        import matplotlib.pyplot as plt\n",
    "        _=plt.plot(range(len(names)), lasso_coef)\n",
    "        _=plt.xticks(range(len(names)),names, rotation=60)\n",
    "        _=plt.ylabel('Coefficients')\n",
    "        print(':: Most Important Features ::')\n",
    "        plt.margins(0.1)  # check this ?\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def RidgeRegression_Alpha_():\n",
    "        # 25 Ridge\n",
    "        #    L1 regularization  + Alpha = LossCalculation\n",
    "        # practice fitting ridge regression models over a range of different alphas, and plot cross-validated R2 scores for each\n",
    "        # cross-validation scores change with different alphas\n",
    "        # Loss function to show\n",
    "        def display_plot(cv_scores, cv_scores_std):  # FillBetween plot\n",
    "            import matplotlib.pyplot as plt\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(1,1,1)\n",
    "            ax.plot(alpha_space, cv_scores)\n",
    "            std_error = cv_scores_std / np.sqrt(10)\n",
    "            ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2) # Loss function to show\n",
    "            ax.set_ylabel('CV Score +/- Std Error')\n",
    "            ax.set_xlabel('Alpha')\n",
    "            ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "            ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "            ax.set_xscale('log')\n",
    "            print(\"Mean - StandardDeviation: How Spread the value.\")\n",
    "            plt.show()\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "        y = df.life.values.reshape(-1,1)\n",
    "        X= df.drop([\"life\",\"Region\"],axis=1).values\n",
    "\n",
    "\n",
    "        from sklearn.linear_model import Ridge\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        import numpy as np\n",
    "\n",
    "\n",
    "        alpha_space = np.logspace(-4, 0, 50)                     # Setup the array of alphas and lists to store scores\n",
    "        ridge_scores = []\n",
    "        ridge_scores_std = []\n",
    "\n",
    "        ridge = Ridge(normalize=True)                            # Create a ridge regressor: ridge\n",
    "\n",
    "\n",
    "        for alpha in alpha_space:                                # Compute scores over range of alphas\n",
    "            ridge.alpha = alpha                                  # Specify the alpha value to use: ridge.alpha\n",
    "            ridge_cv_scores = cross_val_score(ridge, X,y,cv=10 ) # Perform 10-fold CV: ridge_cv_scores\n",
    "            ridge_scores.append(np.mean(ridge_cv_scores))        # Append the mean of ridge_cv_scores to ridge_scores\n",
    "            ridge_scores_std.append(np.std(ridge_cv_scores))     # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "                                                                 # StandardDeviation : \n",
    "        display_plot(ridge_scores, ridge_scores_std)             # Display the plot\n",
    "\n",
    "        \n",
    "        \n",
    "    def Confusion_Matrix():\n",
    "        # 26\n",
    "        # Confusion Matrix\n",
    "        # Classification_report\n",
    "        # Knn Classifier :: Split Data into Training set and Test set\n",
    "        # unit 3\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/US_Voting_Filtered_Data_.csv')  \n",
    "        X = df.drop('party', axis=1).values\n",
    "        y = df['party'].values\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.3,random_state=21,stratify=y)\n",
    "\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=8) \n",
    "        knn.fit(X_train,y_train)\n",
    "        predict=knn.predict(X_test)\n",
    "\n",
    "        #Confusion Matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        print(\"Confusion Matrix :\\n[TP, FN]\\n[FP, TN]\\n\",confusion_matrix(y_test,predict))  \n",
    "        ## [TP, FN]  [0=0, 0=1]\n",
    "        ## [FP, TN]  [1=0, 1=1]\n",
    "\n",
    "        #Classification Report\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(\"\\nprecision    :For True: Do not say as False\")\n",
    "        print(\"recall       :For False: Say as False\\n\")\n",
    "        print(classification_report(y_test, predict))\n",
    "\n",
    "        # Accuracy Score :: not always an informative metric\n",
    "        print(\"Score: \",knn.score(X_test,y_test))\n",
    "\n",
    "    def KNearestNeighbor_ConFusion_():\n",
    "        # 27\n",
    "        # KNN \n",
    "        # Diabatic Data # PIMA Indians # Pima Indians Diabetes\n",
    "        # # Confusion Matrix\n",
    "        # # Classification_report\n",
    "\n",
    "\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/Diabetes_Data.csv')  \n",
    "        df.insulin.replace((0), (155.54), inplace=True)\n",
    "        X = df.drop('diabetes', axis=1).values\n",
    "        y = df['diabetes'].values\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=8) \n",
    "        knn.fit(X_train,y_train)\n",
    "        predict=knn.predict(X_test)\n",
    "\n",
    "        #Confusion Matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        print(\"Confusion Matrix :\\n[TP, FN]\\n[FP, TN]\\n\",confusion_matrix(y_test,predict))  \n",
    "        ## [TP, FN]  [0=0, 0=1]\n",
    "        ## [FP, TN]  [1=0, 1=1]\n",
    "\n",
    "        #Classification Report\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(\"\\nprecision    :For True: Do not say as False\")\n",
    "        print(\"recall       :For False: Say as False\\n\")\n",
    "        print(classification_report(y_test, predict))\n",
    "\n",
    "        # Accuracy Score :: not always an informative metric\n",
    "        print(\"Score: \",knn.score(X_test,y_test))\n",
    "\n",
    "    def LogisticRegression_Classif_():\n",
    "        # 28\n",
    "        #  Logistic regression   # Use in Classsification Problem\n",
    "        #  Logistic regression  Output as probabilities : [P<0.5 =0, p>0.5 =1]\n",
    "        #  US_Voting_Filtered_Data\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/US_Voting_Filtered_Data_.csv')  \n",
    "        X = df.drop('party', axis=1).values\n",
    "        y = df['party'].values\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.4,random_state=42,stratify=y)\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        logreg=LogisticRegression()\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        print(\"score :\",logreg.score(X_test,y_test))\n",
    "\n",
    "    def ROC_():\n",
    "            # 29    ->  # 28 Continuation \n",
    "        # ROC :Reciver Operating Charasterstic curve \n",
    "        #  Logistic regression Probabilities \n",
    "        # Voting Data\n",
    "        import pandas as pd\n",
    "\n",
    "\n",
    "        df=pd.read_csv('data_files/US_Voting_Filtered_Data_.csv')  \n",
    "        X = df.drop('party', axis=1).values\n",
    "        y = df['party'].values\n",
    "\n",
    "        y_test=pd.DataFrame(y_test)\n",
    "        y_test.replace(('republican', 'democrat'), (1, 0), inplace=True)   \n",
    "\n",
    "        y_pred_prob= logreg.predict_proba(X_test)[:,1]     #  [P<0.5 =0, p>0.5 =1] -> Out put two Columns \n",
    "\n",
    "        from sklearn.metrics import roc_curve\n",
    "        fpr, tpr, thresholds= roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot([0,1],[0,1],'k--')\n",
    "        plt.plot(fpr, tpr,label='Logistic Regression')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Logistic Regression ROC Curve')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def LogisticRegression_Classif_ConfMat_():\n",
    "            # 30\n",
    "        # Logistic REgression -> for classification Probelms\n",
    "        # Diabatic Data  \n",
    "        # # Confusion Matrix\n",
    "        # # Classification_report\n",
    "\n",
    "\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/Diabetes_Data.csv')  \n",
    "        df.insulin.replace((0), (155.54), inplace=True)\n",
    "        X = df.drop('diabetes', axis=1).values\n",
    "        y = df['diabetes'].values\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        logreg = LogisticRegression()\n",
    "        logreg.fit(X_train,y_train)\n",
    "        predict=logreg.predict(X_test)\n",
    "\n",
    "        #Confusion Matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        print(\"Confusion Matrix :\\n[TP, FN]\\n[FP, TN]\\n\",confusion_matrix(y_test,predict))  \n",
    "        ## [TP, FN]  [0=0, 0=1]\n",
    "        ## [FP, TN]  [1=0, 1=1]\n",
    "\n",
    "        #Classification Report\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(\"\\nprecision    :For True: Do not say as False\")\n",
    "        print(\"recall       :For False: Say as False\\n\")\n",
    "        print(classification_report(y_test, predict))\n",
    "\n",
    "        # Accuracy Score :: not always an informative metric\n",
    "        print(\"Score: \",logreg.score(X_test,y_test))\n",
    "    def AUC_():\n",
    "        # 32\n",
    "        # roc_auc_score\n",
    "        # AUC:Area Under Curve.,  ROC:Reciver operating Characterstic Curve.\n",
    "        # Diabetes data : Logistic REgression\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/Diabetes_Data.csv')  \n",
    "        df.insulin.replace((0), (155.54), inplace=True)\n",
    "        X = df.drop('diabetes', axis=1).values\n",
    "        y = df['diabetes'].values\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        logreg = LogisticRegression()\n",
    "        logreg.fit(X_train,y_train)\n",
    "        predict=logreg.predict(X_test)\n",
    "        y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        RocAucScore=roc_auc_score(y_test, y_pred_prob)\n",
    "        print(\"roc_auc_score:\",RocAucScore)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        def AUC_CV_():\n",
    "    # 33  Contineoution of 32 \n",
    "    # AUC  Cross Validation\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv('data_files/Diabetes_Data.csv')  \n",
    "    df.insulin.replace((0), (155.54), inplace=True)\n",
    "    X = df.drop('diabetes', axis=1).values\n",
    "    y = df['diabetes'].values\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    cv_scores = cross_val_score(logreg, X,y,cv=5,scoring='roc_auc')\n",
    "    print(\"AUC scores computed using 5-fold cross-validation: \",cv_scores)\n",
    "\n",
    "    def HyperParameterTuning_GridSearch_Knn():\n",
    "        # 34\n",
    "        # HyperParameter Tuning.  \n",
    "        # HyperParameter like Alpha in regression, N in Knn.\n",
    "        # HyperParameter Must Define Before Fitting the Model.\n",
    "\n",
    "        # Grid Search                  # GridSearchCV can be computationally expensive.\n",
    "        # KNeighborsClassifier\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/Diabetes_Data.csv')  \n",
    "        df.insulin.replace((0), (155.54), inplace=True)\n",
    "        X = df.drop('diabetes', axis=1).values\n",
    "        y = df['diabetes'].values\n",
    "\n",
    "\n",
    "        import numpy as np\n",
    "        param_grid ={'n_neighbors':np.arange(1,50)}   # Specificing Grid Hyperparameters   #n_neighbors/Alpha\n",
    "\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier() \n",
    "\n",
    "        from sklearn.model_selection import GridSearchCV \n",
    "        knn_cv= GridSearchCV(knn, param_grid , cv=5) # (algo, grid, nooffolds)\n",
    "\n",
    "        knn_cv.fit(X,y)\n",
    "        best_parameters=knn_cv.best_params_\n",
    "        print(\"best_parameters: \",best_parameters)\n",
    "        best_score=knn_cv.best_score_\n",
    "        print(\"best_score: \",best_score)\n",
    "\n",
    "    def HyperParameterTuning_GridSearch_LogisticRegr():\n",
    "        # 35\n",
    "        # HyperParameter Tuning.    \"C\"\n",
    "        # logistic regression       \"C\"\n",
    "        # Diabetes Data\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/Diabetes_Data.csv')  \n",
    "        df.insulin.replace((0), (155.54), inplace=True)\n",
    "        X = df.drop('diabetes', axis=1).values\n",
    "        y = df['diabetes'].values\n",
    "\n",
    "\n",
    "\n",
    "        c_space = np.logspace(-5, 8, 15)\n",
    "        param_grid = {'C': c_space}       # HyperParameter \"C\" for logistic regression classifier\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        logreg = LogisticRegression()\n",
    "\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "        logreg_cv.fit(X, y)\n",
    "\n",
    "        print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))  # best_params_\n",
    "        print(\"Best score is {}\".format(logreg_cv.best_score_))  # best_score_\n",
    "\n",
    "\n",
    "    def HyperParameterTuning_RandSearch_DecisionTreeClassifier():\n",
    "        # 36\n",
    "        # HyperParameter Tuning. \n",
    "        # RandomizedSearchCV\n",
    "        # RandomizedSearchCV can be computationally expensive than GridSearchCV   ????\n",
    "        # DecisionTreeClassifier\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/Diabetes_Data.csv')  \n",
    "        df.insulin.replace((0), (155.54), inplace=True)\n",
    "        X = df.drop('diabetes', axis=1).values\n",
    "        y = df['diabetes'].values\n",
    "\n",
    "\n",
    "        from scipy.stats import randint                   # randam forest\n",
    "        param_dist = {\"max_depth\": [3, None],             # HyperParameters !\n",
    "                      \"max_features\": randint(1, 9),\n",
    "                      \"min_samples_leaf\": randint(1, 9),\n",
    "                      \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        tree = DecisionTreeClassifier()\n",
    "\n",
    "        from sklearn.model_selection import RandomizedSearchCV\n",
    "        tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "        tree_cv.fit(X,y)\n",
    "        # I guess labels error check how to get best index and best score.\n",
    "        print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_index_))\n",
    "        print(\"Best score is {}\".format(tree_cv.best_score_))\n",
    "\n",
    "\n",
    "\n",
    "    def DecisionTreeClassifier_(): \n",
    "        # 36+\n",
    "        # HyperParameter Tuning. \n",
    "        # RandomizedSearchCV\n",
    "        # RandomizedSearchCV can be computationally expensive than GridSearchCV   ????\n",
    "        # DecisionTreeClassifier\n",
    "\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv('data_files/Diabetes_Data.csv')  \n",
    "        df.insulin.replace((0), (155.54), inplace=True)\n",
    "        X = df.drop('diabetes', axis=1).values\n",
    "        y = df['diabetes'].values\n",
    "\n",
    "\n",
    "\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(X,y)\n",
    "\n",
    "        print(\"Best score is {}\".format(tree.score(X,y)))\n",
    "\n",
    "    def HyperParameterTuning_GridSearch_ElasticNetRegularization_():  \n",
    "        # 38\n",
    "        # In elastic net regularization  = linear combination of the L1 [Lasso] and L2[Ridge]  penalties = a竏有1+b竏有2\n",
    "        # Lasso used the L1 penalty to regularize\n",
    "        # ridge used the L2 penalty\n",
    "\n",
    "        # scikit-learn, this term is represented by the 'l1_ratio' parameter: \n",
    "        # An 'l1_ratio' of 1 corresponds to an L1 penalty,\n",
    "        # and anything lower is a combination of L1 and L2.\n",
    "\n",
    "        # GridSearchCV \n",
    "        # to tune l1_ratio\n",
    "        # Gapminder data.\n",
    "        # ElasticNet regressor \n",
    "\n",
    "\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "        y = df.life.values.reshape(-1,1)\n",
    "        X= df.drop([\"life\",\"Region\"],axis=1).values\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "        l1_space = np.linspace(0, 1, 30) \n",
    "        param_grid = {'l1_ratio': l1_space}  # Hyperparametes\n",
    "\n",
    "        from sklearn.linear_model import ElasticNet     \n",
    "        elastic_net = ElasticNet()       # ElasticNet regressor = Lasso + Ridge\n",
    "\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "\n",
    "        gm_cv.fit(X_train, y_train)\n",
    "        y_pred = gm_cv.predict(X_test)\n",
    "\n",
    "        r2 = gm_cv.score(X_test, y_test)          # Score\n",
    "        mse = mean_squared_error(y_test, y_pred)  # Mean Square Error\n",
    "\n",
    "        print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "        print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "        print(\"Tuned ElasticNet MSE: {}\".format(mse))\n",
    "\n",
    "    def ElasticNetRegularization_():\n",
    "        # 38\n",
    "        # In elastic net regularization  = linear combination of the L1 [Lasso] and L2[Ridge]  penalties = a竏有1+b竏有2\n",
    "        # Lasso used the L1 penalty to regularize\n",
    "        # ridge used the L2 penalty\n",
    "\n",
    "        # scikit-learn, this term is represented by the 'l1_ratio' parameter: \n",
    "        # An 'l1_ratio' of 1 corresponds to an L1 penalty,\n",
    "        # and anything lower is a combination of L1 and L2.\n",
    "\n",
    "\n",
    "\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "        y = df.life.values.reshape(-1,1)\n",
    "        X= df.drop([\"life\",\"Region\"],axis=1).values\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "\n",
    "        from sklearn.linear_model import ElasticNet     \n",
    "        elastic_net = ElasticNet()       # ElasticNet regressor = Lasso + Ridge\n",
    "\n",
    "\n",
    "\n",
    "        elastic_net.fit(X_train, y_train)\n",
    "        y_pred = elastic_net.predict(X_test)\n",
    "\n",
    "        r2 = elastic_net.score(X_test, y_test)          # Score\n",
    "        print(r2)\n",
    "\n",
    "    def Preprocessing_Char2Num_():\n",
    "            # 39\n",
    "        # Unit 4 \n",
    "        # PreProcessing Data\n",
    "        # get_dummies\n",
    "        # Ex red/Blue -> 1/2\n",
    "        #  -> Scikit Learn: OneHotEncoder()\n",
    "        #  -> pandas:get_dummies()\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Automobile_MilesPerGallon_dataset.csv')\n",
    "\n",
    "\n",
    "        df_origin = pd.get_dummies(df)   # # get_dummies\n",
    "        print(df.head())\n",
    "        print(df_origin.head())\n",
    "\n",
    "        df_origin=df_origin.drop('origin_Europe',axis=1)  # Dropped origin_Europe\n",
    "        # df_region = pd.get_dummies(df, drop_first=True) # Direct drop 1th dummy\n",
    "        print(df_origin.head())\n",
    "\n",
    "\n",
    "    def Box_plot_():\n",
    "        # 40\n",
    "        # Box Plot\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "\n",
    "        df.boxplot('life', 'Region', rot=60)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"life vs Region PLOT\")\n",
    "        plt.show()\n",
    "\n",
    "    def Preprocessing_DummyDrop_():\n",
    "        # 41\n",
    "        # Create and drop dummy variable\n",
    "\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "\n",
    "        df_region = pd.get_dummies(df)\n",
    "        # print(df_region.head())\n",
    "\n",
    "        print(df_region.columns)\n",
    "\n",
    "        df_region = pd.get_dummies(df, drop_first=True)\n",
    "        print(\"\\nRegion_America Dropped\\n\")\n",
    "        print(df_region.columns)\n",
    "\n",
    "    def RidgeRegression_DropDummy_():\n",
    "        # 42\n",
    "        # RidgeRegression  + Create and drop dummy variable + cv on Gapminder DAta\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data_files/Gapminder_Region_data.csv')\n",
    "        y = df.life.values.reshape(-1,1)\n",
    "\n",
    "        df_region = pd.get_dummies(df, drop_first=True)   # Dummies Drop implement\n",
    "\n",
    "        X= df_region.drop([\"life\"],axis=1).values\n",
    "\n",
    "\n",
    "        from sklearn.linear_model import Ridge\n",
    "        ridge = Ridge(alpha=0.5, normalize=True)\n",
    "\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        ridge_cv = cross_val_score(ridge, X, y, cv=5)\n",
    "\n",
    "        print(ridge_cv)\n",
    "\n",
    "    def Preprocessing_dropNan_():\n",
    "            # 44 contineoution of 43\n",
    "        # Method 1 Drop\n",
    "        # dropping Nan\n",
    "        # drop  cause huge loss of data\n",
    "        print(\"Before drop:\" ,df.shape)\n",
    "        df= df.dropna()  #Drop Nan\n",
    "        print(\"After drop : \",df.shape)\n",
    "    def Preprocessing_Replace():\n",
    "        # 43\n",
    "        # Handling Missing Data\n",
    "        # PIMA indian Diabetes data set\n",
    "        # Missing data as  insulin 0 : triceps 0 : bmi 0\n",
    "        # Missing as : 0 ? nan '9999'\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        df= pd.read_csv('data_files/Diabetes_Data.csv')\n",
    "        # print(df.info())\n",
    "        # print(df.head()) \n",
    "\n",
    "        # 0 -> nan\n",
    "        # df[df == '?'] = np.nan   # One Step Replacement\n",
    "        # df[df == 0] = np.nan\n",
    "        df.insulin.replace(0,np.nan,inplace=True)\n",
    "        df.triceps.replace(0,np.nan,inplace=True) \n",
    "        df.bmi.replace(0,np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    def Preprocessing_Imputer():\n",
    "        # 45  \n",
    "        # Method 2 Mean/Median/.. replacement -> to Nan\n",
    "        # Using Imputer\n",
    "        # Imputers are known as transformer\n",
    "        # No loss of data\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        df= pd.read_csv('data_files/Diabetes_Data.csv')\n",
    "        # print(df.info())\n",
    "        # print(df.head()) \n",
    "\n",
    "        # 0 -> nan\n",
    "        df.insulin.replace(0,np.nan,inplace=True)\n",
    "        df.triceps.replace(0,np.nan,inplace=True) \n",
    "        df.bmi.replace(0,np.nan, inplace=True)\n",
    "\n",
    "\n",
    "        print(df.head(1))\n",
    "        from sklearn.preprocessing import Imputer\n",
    "        imp = Imputer(missing_values='NaN', strategy='mean',axis=0)   # Initialize Instance   # [axis =0 Column axis =1 Rows]\n",
    "        imp.fit(df)\n",
    "        df=imp.transform(df)\n",
    "        print(\"Mean/Median/Mode :\",df[0])\n",
    "\n",
    "    def SVM_Class_ImpPip_():\n",
    "        # 48 \n",
    "        # SVM: Support Vector Machine\n",
    "        # Imputer \n",
    "        # Pipeline\n",
    "        # classification_report\n",
    "        # US_Voting_Data\n",
    "        import pandas as pd\n",
    "        df=pd.read_csv(\"data_files/US_Voting_Data.csv\")\n",
    "        df.replace(('y', 'n'), (1, 0), inplace=True) # df_region = pd.get_dummies(df)\n",
    "        df[df == '?'] = np.nan\n",
    "\n",
    "        X = df.drop('party', axis=1) \n",
    "        y = df['party'] \n",
    "\n",
    "        from sklearn.preprocessing import Imputer\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.pipeline import Pipeline\n",
    "\n",
    "        imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0) # NOT MEAN WE USE MOST FREQUENT ONE TO REPLACE\n",
    "\n",
    "        clf = SVC()                      # SVC classifier: clf\n",
    "\n",
    "        steps = [('imputation', imp), ('SVM', clf)]\n",
    "\n",
    "        pipeline = Pipeline(steps)       # Initialize Stepss         Imputer-> SVC classifier\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        from sklearn.metrics import classification_report   # // yes No Accuracy \n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    def Preprocessing_Cent_Scal_():\n",
    "        # 49\n",
    "        # Centering and Scaline !\n",
    "        # Data Relatively high difference influence the model \n",
    "        # Ex data_1 :0.001 , data_2:1000000        Need to normalize both the columns\n",
    "        # we want Features to be on similar scale by # Centering and Scaline !\n",
    "        df=pd.read_csv(\"White_wine_data.csv\")\n",
    "        print(\"Check Mean for relative value difference\\n\")\n",
    "        print(df.describe())\n",
    "\n",
    "    def Preprocessing_Scal_():\n",
    "        # 50 \n",
    "        # Scale\n",
    "        from sklearn.preprocessing import scale\n",
    "        import numpy as np\n",
    "        df=pd.read_csv(\"White_wine_data.csv\")\n",
    "        X_scaled=scale(df.iloc[0])\n",
    "        print(\" \\nUnscaled: \\n\",np.mean(df.iloc[0] ),np.std(df.iloc[0] ))\n",
    "        print(\" \\nscaled:\\n \",np.mean(X_scaled ),np.std(X_scaled ))\n",
    "\n",
    "            # 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  displ   hp  weight  accel  origin  size\n",
      "0  18.0  250.0   88    3139   14.5      US  15.0\n",
      "1   9.0  304.0  193    4732   18.5      US  20.0\n",
      "2  36.1   91.0   60    1800   16.4    Asia  10.0\n",
      "3  18.5  250.0   98    3525   19.0      US  15.0\n",
      "4  34.3   97.0   78    2188   15.8  Europe  10.0\n",
      "    mpg  displ   hp  weight  accel  size  origin_Asia  origin_Europe  \\\n",
      "0  18.0  250.0   88    3139   14.5  15.0            0              0   \n",
      "1   9.0  304.0  193    4732   18.5  20.0            0              0   \n",
      "2  36.1   91.0   60    1800   16.4  10.0            1              0   \n",
      "3  18.5  250.0   98    3525   19.0  15.0            0              0   \n",
      "4  34.3   97.0   78    2188   15.8  10.0            0              1   \n",
      "\n",
      "   origin_US  \n",
      "0          1  \n",
      "1          1  \n",
      "2          0  \n",
      "3          1  \n",
      "4          0  \n",
      "    mpg  displ   hp  weight  accel  size  origin_Asia  origin_US\n",
      "0  18.0  250.0   88    3139   14.5  15.0            0          1\n",
      "1   9.0  304.0  193    4732   18.5  20.0            0          1\n",
      "2  36.1   91.0   60    1800   16.4  10.0            1          0\n",
      "3  18.5  250.0   98    3525   19.0  15.0            0          1\n",
      "4  34.3   97.0   78    2188   15.8  10.0            0          0\n"
     ]
    }
   ],
   "source": [
    "Preprocessing_Char2Num_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('Data_files/US_Voting_Data.csv')\n",
    "# Supervised.SeaBorn_Plot(df,'education','party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "      <th>infants</th>\n",
       "      <th>water</th>\n",
       "      <th>budget</th>\n",
       "      <th>physician</th>\n",
       "      <th>salvador</th>\n",
       "      <th>religious</th>\n",
       "      <th>satellite</th>\n",
       "      <th>aid</th>\n",
       "      <th>missile</th>\n",
       "      <th>immigration</th>\n",
       "      <th>synfuels</th>\n",
       "      <th>education</th>\n",
       "      <th>superfund</th>\n",
       "      <th>crime</th>\n",
       "      <th>duty_free_exports</th>\n",
       "      <th>eaa_rsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>republican</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>republican</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democrat</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>democrat</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>democrat</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        party infants water budget physician salvador religious satellite aid  \\\n",
       "0  republican       n     y      n         y        y         y         n   n   \n",
       "1  republican       n     y      n         y        y         y         n   n   \n",
       "2    democrat       ?     y      y         ?        y         y         n   n   \n",
       "3    democrat       n     y      y         n        ?         y         n   n   \n",
       "4    democrat       y     y      y         n        y         y         n   n   \n",
       "\n",
       "  missile immigration synfuels education superfund crime duty_free_exports  \\\n",
       "0       n           y        ?         y         y     y                 n   \n",
       "1       n           n        n         y         y     y                 n   \n",
       "2       n           n        y         n         y     y                 n   \n",
       "3       n           n        y         n         y     n                 n   \n",
       "4       n           n        y         ?         y     y                 y   \n",
       "\n",
       "  eaa_rsa  \n",
       "0       y  \n",
       "1       ?  \n",
       "2       n  \n",
       "3       y  \n",
       "4       y  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits.key  : dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      "\n",
      "digits.DESCR  : Optical Recognition of Handwritten Digits Data Set\n",
      "===================================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      "References\n",
      "----------\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n",
      "\n",
      "digits.images.shape  : (1797, 8, 8)\n",
      "\n",
      "digits.data.shape   : (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAFJCAYAAADngYQlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19d301d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "Supervised.MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
