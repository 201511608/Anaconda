{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONTENT\n",
    "# 1 ::  PoleBalance\n",
    "# 2 ::\n",
    "# 3 ::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Reinforcement_PoleBalance\n",
    "\n",
    "class Reinforcement:\n",
    "    class Reinforcement_PoleBalance:\n",
    "        # Defautl Goal_steps , LR, score_requirement and initia_games Defined  \n",
    "        def __init__( self,goal_steps = 500,LR = 1e-3,score_requirement = 100,initial_games = 10000):\n",
    "            print(\"Reinforcement_PoleBalance Class Instance Begin :: Takes default inputs\") \n",
    "            self.LR = LR\n",
    "            self.goal_steps = goal_steps\n",
    "            self.score_requirement = score_requirement\n",
    "            self.initial_games = initial_games \n",
    "\n",
    "\n",
    "        def __del__(self):\n",
    "            print(\"Reinforcement_PoleBalance Class Instance Destroyed\")\n",
    "\n",
    "        def check(self):\n",
    "            print(\"In Reinforcement_PoleBalance class\")\n",
    "\n",
    "         # 1.1\n",
    "        # Pole Balance\n",
    "        # Reinforcement \n",
    "        # Make it Better\n",
    "        def Pole_Balance(self):\n",
    "            import gym\n",
    "            import random\n",
    "            import numpy as np\n",
    "            import tflearn\n",
    "            from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "            from tflearn.layers.estimator import regression\n",
    "            from statistics import median, mean\n",
    "            from collections import Counter\n",
    "\n",
    "            LR = 1e-3\n",
    "            env = gym.make(\"CartPole-v0\")\n",
    "            env.reset()\n",
    "            goal_steps = 500\n",
    "            score_requirement = 100\n",
    "            initial_games = 10000\n",
    "            return score_requirement\n",
    "\n",
    "        def some_random_games_first(self):\n",
    "            # Each of these is its own game.\n",
    "            for episode in range(5):\n",
    "                env.reset()\n",
    "                # this is each frame, up to 200...but we wont make it that far.\n",
    "                for t in range(200):\n",
    "                    # This will display the environment\n",
    "                    # Only display if you really want to see it.\n",
    "                    # Takes much longer to display it.\n",
    "                    env.render()\n",
    "\n",
    "                    # This will just create a sample action in any environment.\n",
    "\n",
    "    # In this environment, the action can be 0 or 1, which is left or right\n",
    "                    action = env.action_space.sample()\n",
    "\n",
    "                    # this executes the environment with an action, \n",
    "                    # and returns the observation of the environment, \n",
    "                    # the reward, if the env is over, and other info.\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "        #some_random_games_first()\n",
    "        def initial_population(self):\n",
    "            import numpy as np\n",
    "            import random\n",
    "            import gym\n",
    "            from statistics import median, mean\n",
    "            from collections import Counter\n",
    "\n",
    "            env = gym.make(\"CartPole-v0\")\n",
    "            env.reset()\n",
    "\n",
    "            # [OBS, MOVES]\n",
    "            training_data = []\n",
    "            # all scores:\n",
    "            scores = []\n",
    "            # just the scores that met our threshold:\n",
    "            accepted_scores = []\n",
    "            # iterate through however many games we want:\n",
    "            for _ in range(self.initial_games):\n",
    "                score = 0\n",
    "                # moves specifically from this environment:\n",
    "                game_memory = []\n",
    "                # previous observation that we saw\n",
    "                prev_observation = []\n",
    "                # for each frame in 200\n",
    "                for _ in range(self.goal_steps):\n",
    "                    # choose random action (0 or 1)\n",
    "                    action = random.randrange(0,2)\n",
    "                    # do it!\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "\n",
    "                    # notice that the observation is returned FROM the action\n",
    "                    # so we'll store the previous observation here, pairing\n",
    "                    # the prev observation to the action we'll take.\n",
    "                    if len(prev_observation) > 0 :\n",
    "                        game_memory.append([prev_observation, action])\n",
    "                    prev_observation = observation\n",
    "                    score+=reward\n",
    "                    if done: break\n",
    "\n",
    "                # IF our score is higher than our threshold, we'd like to save\n",
    "                # every move we made\n",
    "                # NOTE the reinforcement methodology here. \n",
    "                # all we're doing is reinforcing the score, we're not trying \n",
    "                # to influence the machine in any way as to HOW that score is \n",
    "                # reached.\n",
    "                if score >= self.score_requirement:\n",
    "                    accepted_scores.append(score)\n",
    "                    for data in game_memory:\n",
    "                        # convert to one-hot (this is the output layer for our neural network)\n",
    "                        if data[1] == 1:\n",
    "                            output = [0,1]\n",
    "                        elif data[1] == 0:\n",
    "                            output = [1,0]\n",
    "\n",
    "                        # saving our training data2\n",
    "                        training_data.append([data[0], output])\n",
    "\n",
    "                # reset env to play again\n",
    "                env.reset()\n",
    "                # save overall scores\n",
    "                scores.append(score)\n",
    "\n",
    "            # just in case you wanted to reference later\n",
    "            training_data_save = np.array(training_data)\n",
    "            np.save('saved2.npy',training_data_save)\n",
    "\n",
    "            # some stats here, to further illustrate the neural network magic!\n",
    "            print('Average accepted score:',mean(accepted_scores))\n",
    "            print('Median score for accepted scores:',median(accepted_scores))\n",
    "            print(Counter(accepted_scores))\n",
    "\n",
    "            return training_data\n",
    "   \n",
    "\n",
    "        def Trained_population_Data(self,model):\n",
    "            # 'Nural_pole_data1.npy Output file\n",
    "            # [OBS, MOVES]\n",
    "            import numpy as np\n",
    "            import random\n",
    "            import gym\n",
    "            from statistics import median, mean\n",
    "            from collections import Counter\n",
    "            env = gym.make(\"CartPole-v0\")\n",
    "            env.reset()\n",
    "            training_data = []\n",
    "            # all scores:\n",
    "            scores = []\n",
    "            # just the scores that met our threshold:\n",
    "            accepted_scores = []\n",
    "            # iterate through however many games we want:\n",
    "            initial_games=10   ############################################\n",
    "            for _ in range(initial_games):# 10000           \n",
    "                score = 0\n",
    "                # moves specifically from this environment:\n",
    "                game_memory = []\n",
    "                # previous observation that we saw\n",
    "                prev_observation = []\n",
    "                # for each frame in 200\n",
    "                prev_obs=[]\n",
    "                env.reset()\n",
    "                goal_steps=2000 ################################################\n",
    "                for _ in range(goal_steps):#500prev_obs\n",
    "\n",
    "                    ##\n",
    "                    if len(prev_obs)==0:\n",
    "                        action = random.randrange(0,2)###\n",
    "                    else:\n",
    "                        action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])####\n",
    "\n",
    "\n",
    "                    observation, reward, done, info = env.step(action)                \n",
    "                    prev_obs = observation\n",
    "\n",
    "\n",
    "                    ##\n",
    "\n",
    "                    # do it!\n",
    "\n",
    "\n",
    "                    # notice that the observation is returned FROM the action\n",
    "                    # so we'll store the previous observation here, pairing\n",
    "                    # the prev observation to the action we'lltraining_data take.\n",
    "                    if len(prev_observation) > 0 :\n",
    "                        game_memory.append([prev_observation, action])\n",
    "                    prev_observation = observation\n",
    "                    score+=reward\n",
    "                    if done: break\n",
    "\n",
    "                # IF our score is higher than our threshold, we'd like to save\n",
    "                # every move we made\n",
    "                # NOTE the reinforcement methodology here. \n",
    "                # all we're doing is reinforcing the score, we're not trying \n",
    "                # to influence the machine in any way as to HOW that score is \n",
    "                # reached.\n",
    "                score_requirement= 2000   ###################################################\n",
    "                if score >= score_requirement:\n",
    "                    accepted_scores.append(score)\n",
    "                    for data in game_memory:\n",
    "                        # convert to one-hot (this is the output layer for our neural network)\n",
    "                        if data[1] == 1:\n",
    "                            output = [0,1]\n",
    "                        elif data[1] == 0:\n",
    "                            output = [1,0]\n",
    "\n",
    "                        # saving our training data\n",
    "                        training_data.append([data[0], output])\n",
    "\n",
    "                # reset env to play again\n",
    "                env.reset()\n",
    "                # save overall scores\n",
    "                scores.append(score)\n",
    "\n",
    "            # just in case you wanted to reference later\n",
    "            training_data_save = np.array(training_data)\n",
    "            np.save('Nural_pole_data1.npy',training_data_save)\n",
    "\n",
    "            # some stats here, to further illustrate the neural network magic!\n",
    "            print('Done ***********************************')\n",
    "\n",
    "\n",
    "            return training_data\n",
    "        def neural_network_model(self, input_size):\n",
    "            import tflearn\n",
    "            from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "            from tflearn.layers.estimator import regression\n",
    "\n",
    "            network = input_data(shape=[None, input_size, 1], name='input')\n",
    "\n",
    "            network = fully_connected(network, 128, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 256, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 512, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 512, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 512, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 2, activation='softmax')\n",
    "            network = regression(network, optimizer='adam', learning_rate=self.LR, loss='categorical_crossentropy', name='targets')\n",
    "            model = tflearn.DNN(network, tensorboard_dir='log')\n",
    "\n",
    "            return model\n",
    "        \n",
    "        def neural_network_model_initialize (self,training_data):\n",
    "            import numpy as np\n",
    "            X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "            y = [i[1] for i in training_data]\n",
    "            model = Reinforcement.Reinforcement_PoleBalance.neural_network_model(self,input_size = len(X[0]))\n",
    "            return model\n",
    "\n",
    "        def train_model(self,training_data, model):\n",
    "            import numpy as np\n",
    "        \n",
    "            X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "            y = [i[1] for i in training_data]\n",
    "            model.fit({'input': X}, {'targets': y}, n_epoch=3, snapshot_step=500, show_metric=True, run_id='openai_learning')\n",
    "            return model\n",
    "\n",
    "        # model = train_model(initial_population())\n",
    "\n",
    "        def Pole_test(self,model):\n",
    "            # 1.2\n",
    "            import gym\n",
    "            import random\n",
    "            import numpy as np\n",
    "            import tflearn\n",
    "            from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "            from tflearn.layers.estimator import regression\n",
    "            from statistics import median, mean\n",
    "            from collections import Counter\n",
    "\n",
    "            LR = 1e-3\n",
    "            env = gym.make(\"CartPole-v0\")\n",
    "            env.reset()\n",
    "            goal_steps = 2000\n",
    "            score_requirement = 50\n",
    "            initial_games = 10000\n",
    "        #    def     training_data =initial_population()\n",
    "        #         model = train_model(training_data)\n",
    "            scores = []\n",
    "            choices = []\n",
    "\n",
    "            for each_game in range(10):\n",
    "                score = 0\n",
    "                game_memory = []\n",
    "                prev_obs = []\n",
    "                env.reset()\n",
    "                for _ in range(goal_steps):\n",
    "                    env.render()\n",
    "                    if len(prev_obs)==0:\n",
    "                        action = random.randrange(0,2)\n",
    "                    else:\n",
    "                        action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])\n",
    "                    choices.append(action)               \n",
    "                    new_observation, reward, done, info = env.step(action)\n",
    "                    prev_obs = new_observation\n",
    "                    game_memory.append([new_observation, action])\n",
    "                    score+=reward\n",
    "#                     if done: break\n",
    "                scores.append(score)\n",
    "            env.close()\n",
    "            print('Average Score:',sum(scores)/len(scores))\n",
    "            print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
    "            print(score_requirement)\n",
    "        \n",
    "        #Function Call\n",
    "#         r=Reinforcement.Reinforcement_PoleBalance()\n",
    "#         training_data=r.initial_population()\n",
    "#         model = r.neural_network_model_initialize (training_data)\n",
    "#         model=r.train_model(training_data,model)\n",
    "#         r.Pole_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9329  | total loss: \u001b[1m\u001b[32m0.07801\u001b[0m\u001b[0m | time: 90.864s\n",
      "| Adam | epoch: 003 | loss: 0.07801 - acc: 0.9787 -- iter: 198976/199000\n",
      "Training Step: 9330  | total loss: \u001b[1m\u001b[32m0.08548\u001b[0m\u001b[0m | time: 90.894s\n",
      "| Adam | epoch: 003 | loss: 0.08548 - acc: 0.9762 -- iter: 199000/199000\n",
      "--\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Average Score: 200.0\n",
      "choice 1:0.4945  choice 0:0.5055\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 1.2\n",
    "# Run Reinforcement Class+\n",
    "import numpy as np\n",
    "r=Reinforcement.Reinforcement_PoleBalance()\n",
    "# training_data=r.initial_population()\n",
    "training_data=np.load('Nural_pole_data.npy')  # 'Nural_pole_data.npy' perfect data\n",
    "model = r.neural_network_model_initialize (training_data)\n",
    "model=r.train_model(training_data,model)\n",
    "r.Pole_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement_PoleBalance Class Instance Begin :: Takes default inputs\n",
      "Reinforcement_PoleBalance Class Instance Destroyed\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Average Score: 2000.0\n",
      "choice 1:0.50005  choice 0:0.49995\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 1.3\n",
    "# Test \n",
    "r.Pole_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:C:\\Users\\midas\\Z_anaconda_git\\New_Anaconda2\\Anaconda\\Nural_pole_model.h5 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# 1.4\n",
    "# Save\n",
    "model.save(\"Nural_pole_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "# Pendulum\n",
    "# Sample test\n",
    "import gym\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env.reset()\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "#     print(action, observation, reward, done,info)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#2.1\n",
    "# Pendulum  # Make as class\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "first_observation=env.reset()\n",
    "score=0\n",
    "data=[]\n",
    "temp_data=[]\n",
    "for _ in range(10000):\n",
    "    for i in range(100+1):\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        # env.render()\n",
    "        # print(action, observation, reward, done,info)\n",
    "    \n",
    "        # Temp data for every loop\n",
    "        \n",
    "        temp_data.append([first_observation[0],first_observation[1],first_observation[2],action]) \n",
    "\n",
    "        first_observation = observation \n",
    "            \n",
    "        score= reward+score   \n",
    "        if score >=-500 and i>=100:\n",
    "            print(i,score)\n",
    "            data.append(temp_data)\n",
    "            \n",
    "            first_observation=env.reset()\n",
    "            score=0\n",
    "            temp_data=[]\n",
    "            \n",
    "        if done:\n",
    "            first_observation=env.reset()\n",
    "            score=0\n",
    "    env.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2\n",
    "file = open('Pendulum_Data2.csv','w+') \n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        file.write(str(data[i][j][0])+\",\"+str(data[i][j][1])+\",\"+str(data[i][j][2])+\",\"+str(data[i][j][3][0])+\"\\n\")      \n",
    "file.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#2.3\n",
    "import pandas as pd\n",
    "df=pd.read_csv('Pendulum_Data1.csv', sep=',',header=None)\n",
    "y=df[3].values\n",
    "X=df.drop(3,axis=1).values\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "ycat=to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4\n",
    "#Hourly_wages\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Import Data\n",
    "predictors=X\n",
    "target=y\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation= 'tanh',input_shape=(n_cols,))) \n",
    "model.add(Dense(100, activation='tanh'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(25, activation='tanh'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#compile\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#Fit\n",
    "model.fit(predictors, target)\n",
    "\n",
    "model.fit({'input': X}, {'targets': y}, n_epoch=3, snapshot_step=500, show_metric=True, run_id='openai_learning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[-1.13729775] [ 0.03540393  0.99937308  0.39964168]\n",
      "[ 0.00178109] [-0.02205881  0.99975667  1.14943866]\n",
      "[ 1.15673685] [-0.12536818  0.99211029  2.07276669]\n",
      "[-1.96102691] [-0.24918024  0.96845713  2.52269537]\n",
      "[ 0.44970915] [-0.40562128  0.91404123  3.31649459]\n",
      "[-1.53647292] [-0.56977839  0.82179839  3.77155458]\n",
      "[ 0.41619769] [-0.7370891   0.67579557  4.45033302]\n",
      "[ 1.34723055] [-0.88510335  0.46539453  5.15926428]\n",
      "[-1.08606815] [-0.97658857  0.2151157   5.34539995]\n",
      "[-0.01794099] [-0.99829567 -0.0583588   5.50404558]\n",
      "[ 1.09621167] [-0.94287886 -0.33313579  5.62470823]\n",
      "[-1.29769003] [-0.82610416 -0.56351746  5.18020289]\n",
      "[-1.38975549] [-0.67775399 -0.73528874  4.54910147]\n",
      "[ 1.62018347] [-0.50783575 -0.86145392  4.24066244]\n",
      "[ 0.77858663] [-0.34017463 -0.94036228  3.71135999]\n",
      "[ 0.94629347] [-0.18856525 -0.98206066  3.1480323 ]\n",
      "[ 1.37587452] [-0.05877369 -0.99827133  2.61786798]\n",
      "[ 0.15770312] [ 0.03582574 -0.99935805  1.89281995]\n",
      "[-0.046378] [ 0.09251815 -0.995711    1.13634471]\n",
      "[ 0.22502941] [ 0.11357087 -0.9935299   0.42331587]\n",
      "[ 0.86807185] [ 0.10404675 -0.99457241 -0.19162077]\n",
      "[-0.43625078] [ 0.05405965 -0.99853771 -1.00298769]\n",
      "[-1.61932707] [-0.04563763 -0.99895806 -1.99479003]\n",
      "[-1.45761251] [-0.19257541 -0.98128218 -2.96265046]\n",
      "[ 1.89561188] [-0.35648174 -0.93430229 -3.41427031]\n",
      "[-1.79312932] [-0.55111329 -0.83443043 -4.38396642]\n",
      "[ 0.84647733] [-0.73647106 -0.67646906 -4.88281765]\n",
      "[ 1.77472615] [-0.88385325 -0.46776429 -5.12396051]\n",
      "[-1.48296177] [-0.9796877  -0.20052931 -5.697228  ]\n",
      "[ 0.88905752] [-0.99648957  0.08371704 -5.71426635]\n",
      "[-0.79017359] [-0.93148757  0.36377315 -5.77000461]\n",
      "[-1.05213988] [-0.79300823  0.60921092 -5.65499573]\n",
      "[ 1.41049039] [-0.61816451  0.78604875 -4.98651399]\n",
      "[ 1.63741481] [-0.44290589  0.89656811 -4.1513652 ]\n",
      "[-0.61179799] [-0.27664565  0.960972   -3.57070882]\n",
      "[ 0.53059018] [-0.14130766  0.98996573 -2.77039129]\n",
      "[-1.29624951] [-0.03065968  0.99952988 -2.22235442]\n",
      "[ 1.15903771] [ 0.03427137  0.99941256 -1.29885135]\n",
      "[ 1.52495956] [ 0.05028427  0.99873495 -0.320548  ]\n",
      "[-0.72531056] [ 0.03431342  0.99941112  0.31970663]\n",
      "[ 0.61186796] [-0.02372988  0.99971841  1.16104516]\n",
      "[ 1.50567079] [-0.13019568  0.99148832  2.13668459]\n",
      "[-1.9385004] [-0.25712174  0.96637902  2.58952577]\n",
      "[ 1.42294109] [-0.42270696  0.90626642  3.52775119]\n",
      "[ 0.28845736] [-0.60436306  0.79670904  4.25071961]\n",
      "[-0.34393847] [-0.77631539  0.63034469  4.79666062]\n",
      "[ 0.09046338] [-0.91396445  0.40579427  5.28298864]\n",
      "[ 1.29624808] [-0.99172199  0.12840364  5.78177156]\n",
      "[ 0.88851821] [-0.98527719 -0.17096448  6.01135202]\n",
      "[ 0.0050594] [-0.89337141 -0.44931895  5.88388757]\n",
      "[-0.0493907] [-0.7364577  -0.67648359  5.53948975]\n",
      "[ 1.59703088] [-0.53476946 -0.844998    5.27168169]\n",
      "[-1.07063115] [-0.33383398 -0.94263189  4.47733852]\n",
      "[-1.61719322] [-0.16324481 -0.98658559  3.52778562]\n",
      "[ 0.71455246] [-0.01922604 -0.99981516  2.89502929]\n",
      "[ 0.80884683] [ 0.09395872 -0.99557609  2.26649494]\n",
      "[ 0.2735441] [ 0.17129086 -0.9852205   1.56084449]\n",
      "[-0.94985384] [ 0.20465604 -0.97883395  0.67945103]\n",
      "[-0.81618774] [ 0.19598043 -0.98060781 -0.17710259]\n",
      "[ 0.46817291] [ 0.15451895 -0.98798982 -0.84233251]\n",
      "[-0.90816665] [ 0.06910792 -0.99760919 -1.71954987]\n",
      "[-0.00151515] [-0.05420957 -0.99852958 -2.46798404]\n",
      "[-0.67170262] [-0.21834476 -0.97587169 -3.31763662]\n",
      "[ 0.0417527] [-0.40984293 -0.91215611 -4.04327748]\n",
      "[-1.08254445] [-0.61845154 -0.78582294 -4.88977623]\n",
      "[-1.20775807] [-0.81329102 -0.58185712 -5.66030715]\n",
      "[ 0.93599337] [-0.94822546 -0.31759799 -5.95630099]\n",
      "[-0.45708984] [-0.99994958 -0.01004193 -6.26306296]\n",
      "[-1.06973529] [-0.95187165  0.30649692 -6.4310547 ]\n",
      "[ 1.43368161] [-0.81917964  0.57353702 -5.98612977]\n",
      "[-0.3931981] [-0.62819517  0.7780558  -5.61495672]\n",
      "[-1.53651929] [-0.40422991  0.91465741 -5.26189276]\n",
      "[ 0.83219188] [-0.19237638  0.98132122 -4.45107092]\n",
      "[-0.88011205] [ -1.22834162e-03   9.99999246e-01  -3.84709682e+00]\n",
      "[ 0.26096898] [ 0.15108844  0.98852025 -3.05795204]\n",
      "[ 0.49477217] [ 0.26073799  0.9654096  -2.24234602]\n",
      "[ 1.85029745] [ 0.32008932  0.94738737 -1.24074421]\n",
      "[-1.23559523] [ 0.35377208  0.93533166 -0.71554296]\n",
      "[-0.86331177] [ 0.36047583  0.93276855 -0.14354098]\n",
      "[ 1.72887933] [ 0.32215939  0.94668544  0.81536733]\n",
      "[ 0.87317163] [ 0.24274234  0.9700908   1.65635716]\n",
      "[-0.20267601] [ 0.12717026  0.9918809   2.35352386]\n",
      "[ 0.93909359] [-0.03439259  0.9994084   3.23829857]\n",
      "[-0.34645951] [-0.2291396   0.97339357  3.93588595]\n",
      "[-0.3597939] [-0.44555292  0.8952556   4.61196204]\n",
      "[-1.37433302] [-0.65611146  0.754664    5.07725379]\n",
      "[ 0.08819298] [-0.84064794  0.54158198  5.65648074]\n",
      "[-0.94934237] [-0.96207014  0.27280222  5.92026586]\n",
      "[ 0.53437376] [-0.99942387 -0.03394013  6.20502359]\n",
      "[-0.40557528] [-0.94279359 -0.33337703  6.1187322 ]\n",
      "[ 1.41488409] [-0.79974268 -0.60034295  6.08093204]\n",
      "[ 1.08545041] [-0.59494068 -0.80376961  5.79349239]\n",
      "[-1.40826702] [-0.37854202 -0.92558411  4.97942513]\n",
      "[ 0.36436784] [-0.17039053 -0.98537661  4.33989223]\n",
      "[ 0.85496247] [ 0.01522864 -0.99988404  3.72910414]\n",
      "[ 0.05776365] [ 0.16387946 -0.98648037  2.98785566]\n",
      "[-1.54421091] [ 0.26233408 -0.96497712  2.01636375]\n",
      "[ 0.01298958] [ 0.32420306 -0.94598751  1.29457935]\n",
      "[-0.26840404] [ 0.34984962 -0.93680587  0.54482811]\n",
      "[-0.69698226] [ 0.33753256 -0.94131385 -0.26232364]\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "# Pendulum\n",
    "import gym\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env.reset()\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(action,observation)\n",
    "#     print(action, observation, reward, done,info)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
