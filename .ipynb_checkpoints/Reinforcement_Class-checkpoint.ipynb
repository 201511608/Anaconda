{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONTENT\n",
    "# 1 ::  PoleBalance\n",
    "# 2 ::\n",
    "# 3 ::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Reinforcement_PoleBalance\n",
    "\n",
    "class Reinforcement:\n",
    "    class Reinforcement_PoleBalance:\n",
    "        # Defautl Goal_steps , LR, score_requirement and initia_games Defined  \n",
    "        def __init__( self,goal_steps = 500,LR = 1e-3,score_requirement = 100,initial_games = 10000):\n",
    "            print(\"Reinforcement_PoleBalance Class Instance Begin :: Takes default inputs\") \n",
    "            self.LR = LR\n",
    "            self.goal_steps = goal_steps\n",
    "            self.score_requirement = score_requirement\n",
    "            self.initial_games = initial_games \n",
    "\n",
    "\n",
    "        def __del__(self):\n",
    "            print(\"Reinforcement_PoleBalance Class Instance Destroyed\")\n",
    "\n",
    "        def check(self):\n",
    "            print(\"In Reinforcement_PoleBalance class\")\n",
    "\n",
    "         # 1.1\n",
    "        # Pole Balance\n",
    "        # Reinforcement \n",
    "        # Make it Better\n",
    "        def Pole_Balance(self):\n",
    "            import gym\n",
    "            import random\n",
    "            import numpy as np\n",
    "            import tflearn\n",
    "            from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "            from tflearn.layers.estimator import regression\n",
    "            from statistics import median, mean\n",
    "            from collections import Counter\n",
    "\n",
    "            LR = 1e-3\n",
    "            env = gym.make(\"CartPole-v0\")\n",
    "            env.reset()\n",
    "            goal_steps = 500\n",
    "            score_requirement = 100\n",
    "            initial_games = 10000\n",
    "            return score_requirement\n",
    "\n",
    "        def some_random_games_first(self):\n",
    "            # Each of these is its own game.\n",
    "            for episode in range(5):\n",
    "                env.reset()\n",
    "                # this is each frame, up to 200...but we wont make it that far.\n",
    "                for t in range(200):\n",
    "                    # This will display the environment\n",
    "                    # Only display if you really want to see it.\n",
    "                    # Takes much longer to display it.\n",
    "                    env.render()\n",
    "\n",
    "                    # This will just create a sample action in any environment.\n",
    "\n",
    "    # In this environment, the action can be 0 or 1, which is left or right\n",
    "                    action = env.action_space.sample()\n",
    "\n",
    "                    # this executes the environment with an action, \n",
    "                    # and returns the observation of the environment, \n",
    "                    # the reward, if the env is over, and other info.\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "        #some_random_games_first()\n",
    "        def initial_population(self):\n",
    "            import numpy as np\n",
    "            import random\n",
    "            import gym\n",
    "            from statistics import median, mean\n",
    "            from collections import Counter\n",
    "\n",
    "            env = gym.make(\"CartPole-v0\")\n",
    "            env.reset()\n",
    "\n",
    "            # [OBS, MOVES]\n",
    "            training_data = []\n",
    "            # all scores:\n",
    "            scores = []\n",
    "            # just the scores that met our threshold:\n",
    "            accepted_scores = []\n",
    "            # iterate through however many games we want:\n",
    "            for _ in range(self.initial_games):\n",
    "                score = 0\n",
    "                # moves specifically from this environment:\n",
    "                game_memory = []\n",
    "                # previous observation that we saw\n",
    "                prev_observation = []\n",
    "                # for each frame in 200\n",
    "                for _ in range(self.goal_steps):\n",
    "                    # choose random action (0 or 1)\n",
    "                    action = random.randrange(0,2)\n",
    "                    # do it!\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "\n",
    "                    # notice that the observation is returned FROM the action\n",
    "                    # so we'll store the previous observation here, pairing\n",
    "                    # the prev observation to the action we'll take.\n",
    "                    if len(prev_observation) > 0 :\n",
    "                        game_memory.append([prev_observation, action])\n",
    "                    prev_observation = observation\n",
    "                    score+=reward\n",
    "                    if done: break\n",
    "\n",
    "                # IF our score is higher than our threshold, we'd like to save\n",
    "                # every move we made\n",
    "                # NOTE the reinforcement methodology here. \n",
    "                # all we're doing is reinforcing the score, we're not trying \n",
    "                # to influence the machine in any way as to HOW that score is \n",
    "                # reached.\n",
    "                if score >= self.score_requirement:\n",
    "                    accepted_scores.append(score)\n",
    "                    for data in game_memory:\n",
    "                        # convert to one-hot (this is the output layer for our neural network)\n",
    "                        if data[1] == 1:\n",
    "                            output = [0,1]\n",
    "                        elif data[1] == 0:\n",
    "                            output = [1,0]\n",
    "\n",
    "                        # saving our training data\n",
    "                        training_data.append([data[0], output])\n",
    "\n",
    "                # reset env to play again\n",
    "                env.reset()\n",
    "                # save overall scores\n",
    "                scores.append(score)\n",
    "\n",
    "            # just in case you wanted to reference later\n",
    "            training_data_save = np.array(training_data)\n",
    "            np.save('saved1.npy',training_data_save)\n",
    "\n",
    "            # some stats here, to further illustrate the neural network magic!\n",
    "            print('Average accepted score:',mean(accepted_scores))\n",
    "            print('Median score for accepted scores:',median(accepted_scores))\n",
    "            print(Counter(accepted_scores))\n",
    "\n",
    "            return training_data\n",
    "\n",
    "\n",
    "\n",
    "        def neural_network_model(self, input_size):\n",
    "            import tflearn\n",
    "            from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "            from tflearn.layers.estimator import regression\n",
    "\n",
    "            network = input_data(shape=[None, input_size, 1], name='input')\n",
    "\n",
    "            network = fully_connected(network, 128, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 256, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 512, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 512, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 512, activation='relu')\n",
    "            network = dropout(network, 0.8)\n",
    "\n",
    "            network = fully_connected(network, 2, activation='softmax')\n",
    "            network = regression(network, optimizer='adam', learning_rate=self.LR, loss='categorical_crossentropy', name='targets')\n",
    "            model = tflearn.DNN(network, tensorboard_dir='log')\n",
    "\n",
    "            return model\n",
    "        \n",
    "        def neural_network_model_initialize (self,training_data):\n",
    "            import numpy as np\n",
    "            X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "            y = [i[1] for i in training_data]\n",
    "            model = Reinforcement.Reinforcement_PoleBalance.neural_network_model(self,input_size = len(X[0]))\n",
    "            return model\n",
    "\n",
    "        def train_model(self,training_data, model):\n",
    "            import numpy as np\n",
    "        \n",
    "            X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "            y = [i[1] for i in training_data]\n",
    "            model.fit({'input': X}, {'targets': y}, n_epoch=3, snapshot_step=500, show_metric=True, run_id='openai_learning')\n",
    "            return model\n",
    "\n",
    "        # model = train_model(initial_population())\n",
    "\n",
    "        def Pole_test(self,model):\n",
    "            # 1.2\n",
    "            import gym\n",
    "            import random\n",
    "            import numpy as np\n",
    "            import tflearn\n",
    "            from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "            from tflearn.layers.estimator import regression\n",
    "            from statistics import median, mean\n",
    "            from collections import Counter\n",
    "\n",
    "            LR = 1e-3\n",
    "            env = gym.make(\"CartPole-v0\")\n",
    "            env.reset()\n",
    "            goal_steps = 500\n",
    "            score_requirement = 50\n",
    "            initial_games = 10000\n",
    "        #    def     training_data =initial_population()\n",
    "        #         model = train_model(training_data)\n",
    "            scores = []\n",
    "            choices = []\n",
    "\n",
    "            for each_game in range(10):\n",
    "                score = 0\n",
    "                game_memory = []\n",
    "                prev_obs = []\n",
    "                env.reset()\n",
    "                for _ in range(goal_steps):\n",
    "                    env.render()\n",
    "                    if len(prev_obs)==0:\n",
    "                        action = random.randrange(0,2)\n",
    "                    else:\n",
    "                        action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])\n",
    "                    choices.append(action)               \n",
    "                    new_observation, reward, done, info = env.step(action)\n",
    "                    prev_obs = new_observation\n",
    "                    game_memory.append([new_observation, action])\n",
    "                    score+=reward\n",
    "                    if done: break\n",
    "                scores.append(score)\n",
    "            env.close()\n",
    "            print('Average Score:',sum(scores)/len(scores))\n",
    "            print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
    "            print(score_requirement)\n",
    "        \n",
    "        #Function Call\n",
    "#         r=Reinforcement.Reinforcement_PoleBalance()\n",
    "#         training_data=r.initial_population()\n",
    "#         model = r.neural_network_model_initialize (training_data)\n",
    "#         model=r.train_model(training_data,model)\n",
    "#         r.Pole_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement_PoleBalance Class Instance Begin :: Takes default inputs\n",
      "Reinforcement_PoleBalance Class Instance Destroyed\n"
     ]
    }
   ],
   "source": [
    "# 1.2\n",
    "# Run Reinforcement Class\n",
    "r=Reinforcement.Reinforcement_PoleBalance()\n",
    "training_data=r.initial_population()\n",
    "model = r.neural_network_model_initialize (training_data)\n",
    "model=r.train_model(training_data,model)\n",
    "r.Pole_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input/X:0' shape=(?, 4, 1) dtype=float32>, <tf.Tensor 'input_1/X:0' shape=(?, 4, 1) dtype=float32>, <tf.Tensor 'input_2/X:0' shape=(?, 4, 1) dtype=float32>] <bound method Tensor.get_shape of <tf.Tensor 'FullyConnected_17/Softmax:0' shape=(?, 2) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "print(model.inputs,model.net.get_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
