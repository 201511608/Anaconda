{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Content\n",
    "#1   ::\n",
    "#2   ::  \n",
    "#3   ::\n",
    "#4   ::\n",
    "#5   ::\n",
    "#6   ::\n",
    "#7   ::\n",
    "#8   ::\n",
    "#9   ::\n",
    "#10  ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common Functions\n",
    "\n",
    "#Numpy array to DataFrame\n",
    "def Array2DataFrame(array):\n",
    "    import numpy as np\n",
    "    df = pd.DataFrame(data=array)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Numpy array to DataFrame to SaveFile\n",
    "def Array2SaveCSV(NpArray,filename='Delete'):\n",
    "    import numpy as np\n",
    "    df = pd.DataFrame(data=NpArray)\n",
    "    df.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Deep_Learning_Keras \n",
    "#1 Arch : how many layers Activation\n",
    "#2 Compile :: loss and how it works\n",
    "#3 Fit :: Froword and Back \n",
    "#4 Predict\n",
    "\n",
    "#MIT Licensed\n",
    "\n",
    "#Scale Data :: Getting data in to similar average value\n",
    " \n",
    "#Regression       Y=f(x)\n",
    "#Classification  Predictiong from set of descrete Options\n",
    "\n",
    "#Deep Learning on Large Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.1\n",
    "#import\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.2\n",
    "#Import Data\n",
    "df = pandas.read_csv('hourly_wages.csv')\n",
    "predictors=np.delete(df.values,0,1) # Detete a Column  [1->column 0->row]\n",
    "target=df.values[:,0]\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.3\n",
    "#1 Arch\n",
    "\n",
    "# Model 1)Sequential 2)  Check other models very complex connections  Very important   ??????????\n",
    "model = Sequential()\n",
    "\n",
    "# Dense :: All the Nodes in previous layers connect all the nodes in the current layer\n",
    "model.add(Dense(100, activation= 'relu',input_shape=(n_cols,))) # (n_cols,) = any no of rows\\data\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.4\n",
    "#Compile\n",
    "#2 Compile\n",
    "#Optimizer Learing rate\n",
    "#Adam is Best for Optimizer\n",
    "#loss function Mean SquareError\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "534/534 [==============================] - 98s 184ms/step - loss: 27.1264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f005898>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.5\n",
    "#Fit\n",
    "#Do Forword and Backword Propogation\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.48613453],\n",
       "       [ 11.24972343],\n",
       "       [  7.92303801],\n",
       "       [  8.43539429],\n",
       "       [ 10.4601059 ],\n",
       "       [ 10.11528206],\n",
       "       [ 10.028265  ],\n",
       "       [  9.39352798],\n",
       "       [ 12.23272514],\n",
       "       [  9.39352798]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.6\n",
    "#prediction\n",
    "model.predict(predictors[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.8 \n",
    "#Saving \n",
    "model.save('model_hourly_wages.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.9\n",
    "#Loading Model\n",
    "my_model=load_model('model_hourly_wages.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               1000      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 11,201\n",
      "Trainable params: 11,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#1.10\n",
    "#Check the Architure of Model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "#1.11\n",
    "# Print\n",
    "print(\"Loss function: \" + model.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 50.6288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd039860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "#Hourly_wages\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Import Data\n",
    "df = pandas.read_csv('hourly_wages.csv')\n",
    "predictors=np.delete(df.values,0,1) # Detete a Column  [1->column 0->row]\n",
    "target=df.values[:,0] # or # df.wage_per_hour\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation= 'relu',input_shape=(n_cols,))) \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#compile\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#Fit\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values\n",
    "df.describe()\n",
    "df.index\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      " [0 1 2 3]\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      " [ 0  1  2 13]\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "# Categorical\n",
    "# Single -> Multi Column Convertion\n",
    "from keras.utils import to_categorical\n",
    "a=np.array([1,2,3])\n",
    "cat1=to_categorical(a)\n",
    "print(a)\n",
    "print(cat1)\n",
    "\n",
    "b=np.array([0,1,2,3])\n",
    "cat2=to_categorical(b)\n",
    "print(\"\\n\",b)\n",
    "print(cat2)\n",
    "\n",
    "c=np.array([0,1,2,13])\n",
    "cat3=to_categorical(c)\n",
    "print(\"\\n\",c)\n",
    "print(cat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5\n",
    "# Classification \n",
    "# To Select from Bunch of Options\n",
    "# Similar to Regression Only Few things are Different\n",
    "# Loss function :: Categorical_crossentropy\n",
    "# Log Loss\n",
    "# Metric = ['accuracy'] Print accuracy at end of each epoch\n",
    "# optimizer = 'sgd' 'adam'\n",
    "# Output  Have Softmax activation  = Pridiction Sum to One  :: To Check Proabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "891/891 [==============================] - 1s 2ms/step - loss: 2.5718 - acc: 0.6128\n",
      "data\n",
      "    survived  pclass   age  sibsp  parch     fare  male  age_was_missing  \\\n",
      "0         0       3  22.0      1      0   7.2500     1            False   \n",
      "1         1       1  38.0      1      0  71.2833     0            False   \n",
      "\n",
      "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
      "0                        0                         0   \n",
      "1                        1                         0   \n",
      "\n",
      "   embarked_from_southampton  \n",
      "0                          1  \n",
      "1                          0  \n",
      "\n",
      "Predictors\n",
      " [3 22.0 1 0 7.25 1 False 0 0 1]\n",
      "\n",
      "Target\n",
      " [ 1.  0.]\n",
      "\n",
      "Predictors Shape\n",
      " (91, 2)\n",
      "\n",
      "Probabilities of Survival\n",
      "Yes  No\n",
      " [[  1.84627413e-06   9.99998212e-01]\n",
      " [  6.81351253e-10   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "# Titanic\n",
    "# sgd :: Stochastic Gradient descent\n",
    "#Output  : Dead or Alive    Yes or No\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Import Data\n",
    "df = pandas.read_csv('Titanic_data.csv')\n",
    "predictors=np.delete(df.values,0,1) # Detete a Column  [1->column 0->row]\n",
    "target=to_categorical(df.survived)\n",
    "pred_data=np.delete(pandas.read_csv('Titanic_pred_data.csv').values,0,1)\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "#\n",
    "model= Sequential()\n",
    "model.add(Dense(32,activation='relu',input_shape=(n_cols,)))\n",
    "model.add(Dense(2,activation='softmax')) # Output Survived or Died\n",
    "\n",
    "#\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#\n",
    "model.fit(predictors, target)\n",
    "\n",
    "#\n",
    "predictions = model.predict(pred_data)\n",
    "probability_true =predictions\n",
    "\n",
    "#Print\n",
    "print(\"data\\n\",df.head(2))\n",
    "print(\"\\nPredictors\\n\",predictors[0])\n",
    "print(\"\\nTarget\\n\",target[0])\n",
    "print(\"\\nPredictors Shape\\n\",predictions.shape)\n",
    "print(\"\\nProbabilities of Survival\\nYes  No\\n\",probability_true[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised Instance Begin\n"
     ]
    }
   ],
   "source": [
    "#6 Optimisation\n",
    "# Smart Optimizer  'Adams'   'Stochastic graient descent SGD'\n",
    "# Learning Rate\n",
    "# Dying Nuron Problem i.e   NEURON -> 0  | No effect to out put |\n",
    "# Vanishing gradiend i.e too low slopes updates almost to O | Ex tanhx function\n",
    "# Lower value of loss function is Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/1\n",
      "891/891 [==============================] - 1s 690us/step - loss: 2.4169 - acc: 0.3838\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/1\n",
      "891/891 [==============================] - 1s 681us/step - loss: 2.5932 - acc: 0.5612\n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/1\n",
      "891/891 [==============================] - 1s 673us/step - loss: 6.0605 - acc: 0.6105\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.001000\n",
      "\n",
      "Epoch 1/1\n",
      "891/891 [==============================] - 1s 723us/step - loss: 0.8222 - acc: 0.6128\n"
     ]
    }
   ],
   "source": [
    "#7 Learning Rates Different\n",
    "# Titanic Data\n",
    "from keras.optimizers import SGD # Import the SGD optimizer\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Import Data\n",
    "df = pandas.read_csv('Titanic_data.csv')\n",
    "predictors=np.delete(df.values,0,1) # Detete a Column  [1->column 0->row]\n",
    "target=to_categorical(df.survived)\n",
    "pred_data=np.delete(pandas.read_csv('Titanic_pred_data.csv').values,0,1)\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "lr_to_test = [.000001, 0.01, 1,0.001]\n",
    "\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    model= Sequential()\n",
    "    model.add(Dense(32,activation='relu',input_shape=(n_cols,)))\n",
    "    model.add(Dense(2,activation='softmax'))  \n",
    "    my_optimizer = SGD(lr=lr) # Optimizear\n",
    "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/1\n",
      "623/623 [==============================] - 0s 441us/step - loss: 0.8026 - acc: 0.5859 - val_loss: 1.2996 - val_acc: 0.6493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246a4707ba8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validation split Data\n",
    "#Split data as per percentage\n",
    "#8\n",
    "model.fit(predictors, target,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 801 samples, validate on 90 samples\n",
      "Epoch 1/100\n",
      "801/801 [==============================] - 0s 233us/step - loss: 0.8942 - acc: 0.6380 - val_loss: 1.3715 - val_acc: 0.3778\n",
      "Epoch 2/100\n",
      "801/801 [==============================] - 0s 106us/step - loss: 0.7440 - acc: 0.6180 - val_loss: 0.8801 - val_acc: 0.6333\n",
      "Epoch 3/100\n",
      "801/801 [==============================] - 0s 121us/step - loss: 0.7005 - acc: 0.6479 - val_loss: 0.8977 - val_acc: 0.6222\n",
      "Epoch 4/100\n",
      "801/801 [==============================] - 0s 111us/step - loss: 0.8095 - acc: 0.6242 - val_loss: 0.7381 - val_acc: 0.7444\n",
      "Epoch 5/100\n",
      "801/801 [==============================] - 0s 124us/step - loss: 0.7842 - acc: 0.6517 - val_loss: 0.6533 - val_acc: 0.6556\n",
      "Epoch 6/100\n",
      "801/801 [==============================] - 0s 108us/step - loss: 0.7776 - acc: 0.6529 - val_loss: 1.2642 - val_acc: 0.6333\n",
      "Epoch 7/100\n",
      "801/801 [==============================] - 0s 129us/step - loss: 0.8237 - acc: 0.6242 - val_loss: 0.8714 - val_acc: 0.6444\n",
      "Epoch 8/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 0.7838 - acc: 0.6292 - val_loss: 1.2301 - val_acc: 0.6333\n",
      "Epoch 9/100\n",
      "801/801 [==============================] - 0s 105us/step - loss: 0.8422 - acc: 0.6380 - val_loss: 0.6435 - val_acc: 0.6222\n",
      "Epoch 10/100\n",
      "801/801 [==============================] - 0s 114us/step - loss: 0.7253 - acc: 0.6454 - val_loss: 1.0070 - val_acc: 0.6444\n",
      "Epoch 11/100\n",
      "801/801 [==============================] - 0s 97us/step - loss: 0.7958 - acc: 0.6442 - val_loss: 1.3552 - val_acc: 0.3778\n",
      "Epoch 12/100\n",
      "801/801 [==============================] - 0s 139us/step - loss: 0.8005 - acc: 0.6292 - val_loss: 2.3628 - val_acc: 0.6222\n",
      "Epoch 13/100\n",
      "801/801 [==============================] - 0s 120us/step - loss: 0.9624 - acc: 0.6242 - val_loss: 0.7766 - val_acc: 0.6444\n",
      "Epoch 14/100\n",
      "801/801 [==============================] - 0s 128us/step - loss: 0.7119 - acc: 0.6554 - val_loss: 0.9438 - val_acc: 0.6333\n",
      "Epoch 15/100\n",
      "801/801 [==============================] - 0s 105us/step - loss: 0.7092 - acc: 0.6642 - val_loss: 1.1646 - val_acc: 0.4556\n",
      "Epoch 16/100\n",
      "801/801 [==============================] - 0s 105us/step - loss: 0.7827 - acc: 0.6429 - val_loss: 1.0833 - val_acc: 0.6333\n",
      "Epoch 17/100\n",
      "801/801 [==============================] - 0s 104us/step - loss: 0.8036 - acc: 0.6404 - val_loss: 0.6003 - val_acc: 0.6556\n",
      "Epoch 18/100\n",
      "801/801 [==============================] - 0s 114us/step - loss: 0.6650 - acc: 0.6704 - val_loss: 0.8168 - val_acc: 0.4667\n",
      "Epoch 19/100\n",
      "801/801 [==============================] - 0s 115us/step - loss: 0.6650 - acc: 0.6542 - val_loss: 0.6070 - val_acc: 0.6778\n",
      "Epoch 20/100\n",
      "801/801 [==============================] - 0s 104us/step - loss: 0.7292 - acc: 0.6355 - val_loss: 1.2265 - val_acc: 0.3778\n",
      "Epoch 21/100\n",
      "801/801 [==============================] - 0s 106us/step - loss: 0.7489 - acc: 0.6629 - val_loss: 1.7604 - val_acc: 0.6222\n",
      "Epoch 22/100\n",
      "801/801 [==============================] - 0s 99us/step - loss: 0.7085 - acc: 0.6729 - val_loss: 1.2394 - val_acc: 0.3778\n",
      "Epoch 23/100\n",
      "801/801 [==============================] - 0s 119us/step - loss: 0.7566 - acc: 0.6529 - val_loss: 0.5427 - val_acc: 0.6667\n",
      "Epoch 24/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 0.7106 - acc: 0.6479 - val_loss: 0.5977 - val_acc: 0.7444\n",
      "Epoch 25/100\n",
      "801/801 [==============================] - 0s 96us/step - loss: 0.7461 - acc: 0.6592 - val_loss: 0.6122 - val_acc: 0.6556\n",
      "Epoch 26/100\n",
      "801/801 [==============================] - 0s 92us/step - loss: 0.6789 - acc: 0.6617 - val_loss: 1.3632 - val_acc: 0.3778\n",
      "Epoch 27/100\n",
      "801/801 [==============================] - 0s 105us/step - loss: 0.7617 - acc: 0.6380 - val_loss: 1.3299 - val_acc: 0.3778\n",
      "Epoch 28/100\n",
      "801/801 [==============================] - 0s 117us/step - loss: 0.7447 - acc: 0.6454 - val_loss: 0.5287 - val_acc: 0.7111\n",
      "Epoch 29/100\n",
      "801/801 [==============================] - 0s 89us/step - loss: 0.6855 - acc: 0.6355 - val_loss: 3.2825 - val_acc: 0.6222\n",
      "Epoch 30/100\n",
      "801/801 [==============================] - 0s 106us/step - loss: 1.1826 - acc: 0.6380 - val_loss: 1.1478 - val_acc: 0.4111\n",
      "Epoch 31/100\n",
      "801/801 [==============================] - 0s 99us/step - loss: 0.6849 - acc: 0.6629 - val_loss: 1.7030 - val_acc: 0.3778\n",
      "Epoch 32/100\n",
      "801/801 [==============================] - 0s 105us/step - loss: 0.7862 - acc: 0.6205 - val_loss: 0.8034 - val_acc: 0.3778\n",
      "Epoch 33/100\n",
      "801/801 [==============================] - 0s 100us/step - loss: 0.7186 - acc: 0.6504 - val_loss: 1.0810 - val_acc: 0.6222\n",
      "Epoch 34/100\n",
      "801/801 [==============================] - 0s 106us/step - loss: 0.8022 - acc: 0.6567 - val_loss: 0.5574 - val_acc: 0.7222\n",
      "Epoch 35/100\n",
      "801/801 [==============================] - 0s 116us/step - loss: 0.7230 - acc: 0.6417 - val_loss: 0.9153 - val_acc: 0.3889\n",
      "Epoch 36/100\n",
      "801/801 [==============================] - 0s 95us/step - loss: 0.6800 - acc: 0.6429 - val_loss: 1.2653 - val_acc: 0.3667\n",
      "Epoch 37/100\n",
      "801/801 [==============================] - ETA: 0s - loss: 0.6467 - acc: 0.667 - 0s 92us/step - loss: 0.6469 - acc: 0.6667 - val_loss: 0.8271 - val_acc: 0.6222\n",
      "Epoch 38/100\n",
      "801/801 [==============================] - 0s 104us/step - loss: 0.6388 - acc: 0.6816 - val_loss: 1.2322 - val_acc: 0.4111\n",
      "Epoch 39/100\n",
      "801/801 [==============================] - 0s 93us/step - loss: 0.7304 - acc: 0.6792 - val_loss: 0.5664 - val_acc: 0.7333\n",
      "Epoch 40/100\n",
      "801/801 [==============================] - 0s 119us/step - loss: 0.6899 - acc: 0.6742 - val_loss: 0.5467 - val_acc: 0.7000\n",
      "Epoch 41/100\n",
      "801/801 [==============================] - 0s 109us/step - loss: 0.6309 - acc: 0.6767 - val_loss: 0.9277 - val_acc: 0.6222\n",
      "Epoch 42/100\n",
      "801/801 [==============================] - 0s 122us/step - loss: 0.7235 - acc: 0.6692 - val_loss: 0.6716 - val_acc: 0.6556\n",
      "Epoch 43/100\n",
      "801/801 [==============================] - 0s 112us/step - loss: 0.6944 - acc: 0.6692 - val_loss: 0.5815 - val_acc: 0.6778\n",
      "Epoch 44/100\n",
      "801/801 [==============================] - 0s 110us/step - loss: 0.7331 - acc: 0.6567 - val_loss: 0.5163 - val_acc: 0.7111\n",
      "Epoch 45/100\n",
      "801/801 [==============================] - 0s 81us/step - loss: 0.6616 - acc: 0.6767 - val_loss: 0.6978 - val_acc: 0.6556\n",
      "Epoch 46/100\n",
      "801/801 [==============================] - 0s 71us/step - loss: 0.6940 - acc: 0.6729 - val_loss: 0.7979 - val_acc: 0.6333\n",
      "Epoch 47/100\n",
      "801/801 [==============================] - 0s 71us/step - loss: 0.6408 - acc: 0.6941 - val_loss: 0.6564 - val_acc: 0.6333\n",
      "Epoch 48/100\n",
      "801/801 [==============================] - 0s 79us/step - loss: 0.7142 - acc: 0.6529 - val_loss: 1.1665 - val_acc: 0.3667\n",
      "Epoch 49/100\n",
      "801/801 [==============================] - 0s 72us/step - loss: 0.7437 - acc: 0.6604 - val_loss: 1.5231 - val_acc: 0.6222\n",
      "Epoch 50/100\n",
      "801/801 [==============================] - 0s 79us/step - loss: 0.7163 - acc: 0.6829 - val_loss: 0.8955 - val_acc: 0.6333\n",
      "Epoch 51/100\n",
      "801/801 [==============================] - 0s 81us/step - loss: 0.7186 - acc: 0.6742 - val_loss: 9.8787 - val_acc: 0.3778\n",
      "Epoch 52/100\n",
      "801/801 [==============================] - 0s 112us/step - loss: 9.2721 - acc: 0.3858 - val_loss: 4.5971 - val_acc: 0.4444\n",
      "Epoch 53/100\n",
      "801/801 [==============================] - 0s 124us/step - loss: 4.0547 - acc: 0.5818 - val_loss: 2.8011 - val_acc: 0.7444\n",
      "Epoch 54/100\n",
      "801/801 [==============================] - 0s 159us/step - loss: 3.4194 - acc: 0.6442 - val_loss: 2.5657 - val_acc: 0.7222\n",
      "Epoch 55/100\n",
      "801/801 [==============================] - 0s 132us/step - loss: 3.0633 - acc: 0.6579 - val_loss: 2.4351 - val_acc: 0.7444\n",
      "Epoch 56/100\n",
      "801/801 [==============================] - 0s 168us/step - loss: 2.7321 - acc: 0.6517 - val_loss: 2.2885 - val_acc: 0.7444\n",
      "Epoch 57/100\n",
      "801/801 [==============================] - 0s 161us/step - loss: 2.4929 - acc: 0.6579 - val_loss: 2.2354 - val_acc: 0.7000\n",
      "Epoch 58/100\n",
      "801/801 [==============================] - 0s 149us/step - loss: 2.2794 - acc: 0.6604 - val_loss: 2.0305 - val_acc: 0.7556\n",
      "Epoch 59/100\n",
      "801/801 [==============================] - 0s 110us/step - loss: 2.1077 - acc: 0.6679 - val_loss: 1.9496 - val_acc: 0.7444\n",
      "Epoch 60/100\n",
      "801/801 [==============================] - 0s 99us/step - loss: 1.9755 - acc: 0.6717 - val_loss: 1.9302 - val_acc: 0.6778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "801/801 [==============================] - 0s 99us/step - loss: 1.8503 - acc: 0.6879 - val_loss: 1.7949 - val_acc: 0.6889\n",
      "Epoch 62/100\n",
      "801/801 [==============================] - 0s 99us/step - loss: 1.7220 - acc: 0.6792 - val_loss: 1.6449 - val_acc: 0.6778\n",
      "Epoch 63/100\n",
      "801/801 [==============================] - 0s 113us/step - loss: 1.5514 - acc: 0.6891 - val_loss: 1.4254 - val_acc: 0.6556\n",
      "Epoch 64/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 1.2907 - acc: 0.6629 - val_loss: 1.2400 - val_acc: 0.6333\n",
      "Epoch 65/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 1.0529 - acc: 0.6542 - val_loss: 2.5041 - val_acc: 0.3778\n",
      "Epoch 66/100\n",
      "801/801 [==============================] - 0s 92us/step - loss: 1.0315 - acc: 0.6355 - val_loss: 1.2879 - val_acc: 0.3889\n",
      "Epoch 67/100\n",
      "801/801 [==============================] - 0s 111us/step - loss: 0.9119 - acc: 0.6317 - val_loss: 1.3481 - val_acc: 0.3778\n",
      "Epoch 68/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 0.8927 - acc: 0.6217 - val_loss: 0.8030 - val_acc: 0.5444\n",
      "Epoch 69/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 0.8300 - acc: 0.6355 - val_loss: 1.8047 - val_acc: 0.3778\n",
      "Epoch 70/100\n",
      "801/801 [==============================] - 0s 93us/step - loss: 0.8581 - acc: 0.6267 - val_loss: 1.1359 - val_acc: 0.6556\n",
      "Epoch 71/100\n",
      "801/801 [==============================] - 0s 102us/step - loss: 0.7689 - acc: 0.6529 - val_loss: 1.4985 - val_acc: 0.4111\n",
      "Epoch 72/100\n",
      "801/801 [==============================] - 0s 90us/step - loss: 0.8461 - acc: 0.6117 - val_loss: 1.0909 - val_acc: 0.4556\n",
      "Epoch 73/100\n",
      "801/801 [==============================] - 0s 97us/step - loss: 0.7677 - acc: 0.6317 - val_loss: 1.1445 - val_acc: 0.3889\n",
      "Epoch 74/100\n",
      "801/801 [==============================] - 0s 96us/step - loss: 0.8052 - acc: 0.6305 - val_loss: 1.2067 - val_acc: 0.6333\n",
      "Epoch 75/100\n",
      "801/801 [==============================] - 0s 87us/step - loss: 0.7818 - acc: 0.6404 - val_loss: 0.6443 - val_acc: 0.6444\n",
      "Epoch 76/100\n",
      "801/801 [==============================] - 0s 96us/step - loss: 0.7539 - acc: 0.6492 - val_loss: 1.4071 - val_acc: 0.6333\n",
      "Epoch 77/100\n",
      "801/801 [==============================] - 0s 98us/step - loss: 0.7310 - acc: 0.6654 - val_loss: 0.9414 - val_acc: 0.4111\n",
      "Epoch 78/100\n",
      "801/801 [==============================] - 0s 99us/step - loss: 0.8037 - acc: 0.6380 - val_loss: 2.8857 - val_acc: 0.6222\n",
      "Epoch 79/100\n",
      "801/801 [==============================] - 0s 109us/step - loss: 0.9805 - acc: 0.6330 - val_loss: 0.6857 - val_acc: 0.6333\n",
      "Epoch 80/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 0.8208 - acc: 0.6305 - val_loss: 0.6996 - val_acc: 0.6556\n",
      "Epoch 81/100\n",
      "801/801 [==============================] - 0s 95us/step - loss: 0.7986 - acc: 0.6492 - val_loss: 0.7081 - val_acc: 0.5000\n",
      "Epoch 82/100\n",
      "801/801 [==============================] - 0s 93us/step - loss: 0.6962 - acc: 0.6330 - val_loss: 1.7865 - val_acc: 0.6222\n",
      "Epoch 83/100\n",
      "801/801 [==============================] - 0s 99us/step - loss: 0.7562 - acc: 0.6517 - val_loss: 0.6001 - val_acc: 0.6444\n",
      "Epoch 84/100\n",
      "801/801 [==============================] - 0s 94us/step - loss: 0.6792 - acc: 0.6529 - val_loss: 1.1369 - val_acc: 0.4111\n",
      "Epoch 85/100\n",
      "801/801 [==============================] - 0s 91us/step - loss: 0.7602 - acc: 0.6155 - val_loss: 0.6351 - val_acc: 0.7556\n",
      "Epoch 86/100\n",
      "801/801 [==============================] - 0s 95us/step - loss: 0.8182 - acc: 0.6417 - val_loss: 0.7365 - val_acc: 0.6667\n",
      "Epoch 87/100\n",
      "801/801 [==============================] - 0s 95us/step - loss: 0.7180 - acc: 0.6404 - val_loss: 2.4003 - val_acc: 0.3778\n",
      "Epoch 88/100\n",
      "801/801 [==============================] - 0s 107us/step - loss: 0.8526 - acc: 0.6142 - val_loss: 0.5580 - val_acc: 0.6889\n",
      "Epoch 89/100\n",
      "801/801 [==============================] - 0s 92us/step - loss: 0.6949 - acc: 0.6417 - val_loss: 0.5850 - val_acc: 0.6556\n",
      "Epoch 90/100\n",
      "801/801 [==============================] - 0s 96us/step - loss: 0.6933 - acc: 0.6542 - val_loss: 0.9023 - val_acc: 0.6333\n",
      "Epoch 91/100\n",
      "801/801 [==============================] - 0s 94us/step - loss: 0.7153 - acc: 0.6367 - val_loss: 1.4384 - val_acc: 0.4111\n",
      "Epoch 92/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 0.7785 - acc: 0.6392 - val_loss: 0.5147 - val_acc: 0.7444\n",
      "Epoch 93/100\n",
      "801/801 [==============================] - 0s 90us/step - loss: 0.6764 - acc: 0.6642 - val_loss: 1.2207 - val_acc: 0.6333\n",
      "Epoch 94/100\n",
      "801/801 [==============================] - 0s 96us/step - loss: 0.6831 - acc: 0.6629 - val_loss: 0.5213 - val_acc: 0.7222\n",
      "Epoch 95/100\n",
      "801/801 [==============================] - 0s 97us/step - loss: 0.6562 - acc: 0.6579 - val_loss: 1.3011 - val_acc: 0.3889\n",
      "Epoch 96/100\n",
      "801/801 [==============================] - 0s 102us/step - loss: 0.7158 - acc: 0.6330 - val_loss: 0.8840 - val_acc: 0.6222\n",
      "Epoch 97/100\n",
      "801/801 [==============================] - 0s 89us/step - loss: 0.6742 - acc: 0.6604 - val_loss: 0.9459 - val_acc: 0.4111\n",
      "Epoch 98/100\n",
      "801/801 [==============================] - 0s 89us/step - loss: 0.6527 - acc: 0.6542 - val_loss: 0.5955 - val_acc: 0.6444\n",
      "Epoch 99/100\n",
      "801/801 [==============================] - 0s 90us/step - loss: 0.6465 - acc: 0.6654 - val_loss: 0.5226 - val_acc: 0.7222\n",
      "Epoch 100/100\n",
      "801/801 [==============================] - 0s 101us/step - loss: 0.6866 - acc: 0.6579 - val_loss: 0.5240 - val_acc: 0.7444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246a4707550>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epochs\n",
    "#Do Forword and backword Propogatin for Learning\n",
    "#An epoch is one forward pass and one backward pass of all training examples\n",
    "#One epoch consists of one full training cycle on the training set\n",
    "#5 epochs as pybrain advice, the dataset will be divided into 5 subsets\n",
    "#and the wights will update 5 times as maximum\n",
    "#9\n",
    "model.fit(predictors, target,validation_split=0.1,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/20\n",
      "623/623 [==============================] - 0s 116us/step - loss: 0.6854 - acc: 0.6629 - val_loss: 1.0468 - val_acc: 0.6418\n",
      "Epoch 2/20\n",
      "623/623 [==============================] - 0s 92us/step - loss: 0.7025 - acc: 0.6356 - val_loss: 0.5677 - val_acc: 0.7276\n",
      "Epoch 3/20\n",
      "623/623 [==============================] - 0s 90us/step - loss: 0.7164 - acc: 0.6340 - val_loss: 0.7875 - val_acc: 0.6418\n",
      "Epoch 4/20\n",
      "623/623 [==============================] - 0s 133us/step - loss: 0.6584 - acc: 0.6372 - val_loss: 0.5642 - val_acc: 0.7239\n",
      "Epoch 5/20\n",
      "623/623 [==============================] - 0s 122us/step - loss: 0.7065 - acc: 0.6469 - val_loss: 1.1013 - val_acc: 0.6418\n",
      "Epoch 6/20\n",
      "623/623 [==============================] - 0s 133us/step - loss: 0.7494 - acc: 0.6372 - val_loss: 0.5910 - val_acc: 0.7201\n",
      "Epoch 7/20\n",
      "623/623 [==============================] - 0s 138us/step - loss: 0.7202 - acc: 0.6501 - val_loss: 0.5774 - val_acc: 0.7276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246a48c0278>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Early Stopping\n",
    "#How many epoch before stopping training\n",
    "#Call Early Stopping Through Callbacks  :: This take List as input\n",
    "# optimization will automatically stop when it is no longer helpful\n",
    "#9\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "model.fit(predictors, target,validation_split=0.3, epochs=20,callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Nural Network_Regression \n",
    "# Nuron Layers   (Nodes_Nuron and Activation Function) |Nurons_Layerof hidden layers Only\n",
    "# predictors and target in nparray format\n",
    "# Activation= 'relu' 'sigmodial' 'tanh'\n",
    "# Return Model with Trained -> just do prediction\n",
    "#10\n",
    "def DeepNetwork_Reg(predictors,target,layers=5,Nurons_Layer=[(100,'relu'),(100,'relu'),(100,'relu'),(100,'relu'),(100,'relu')]\n",
    "                   ,optimizer='adam', loss='mean_squared_error'):\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    \n",
    "    n_cols = predictors.shape[1]\n",
    "    model= Sequential()\n",
    "    model.add(Dense(Nurons_Layer[0][0], activation=Nurons_Layer[0][1],input_shape=(n_cols,))) \n",
    "    for i in range(layers-1):\n",
    "        model.add(Dense(Nurons_Layer[i][0], activation=Nurons_Layer[i][1]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Deep Learning Classification\n",
    "# optimizer = 'sgd' 'adam'    #optimizer( optimizer, LearningRate)\n",
    "# SGD    lr=0.01 Default if not defined  \n",
    "# Loss function :: Categorical_crossentropy\n",
    "# 10.1\n",
    "def DeepNetwork_Clas(predictors,target,layers=5,Nurons_Layer=[(100,'relu'),(100,'relu'),(100,'relu'),(100,'relu'),(100,'relu')]\n",
    "                   ,activation_output='softmax',optimizer=('sgd',0.01), loss='categorical_crossentropy'):\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.optimizers import SGD\n",
    "    \n",
    "    target=to_categorical(target) # Convertion \n",
    "    \n",
    "    n_cols = predictors.shape[1]\n",
    "    model= Sequential()\n",
    "    model.add(Dense(Nurons_Layer[0][0], activation=Nurons_Layer[0][1],input_shape=(n_cols,))) \n",
    "    for i in range(layers-1):\n",
    "        model.add(Dense(Nurons_Layer[i][0], activation=Nurons_Layer[i][1]))\n",
    "    model.add(Dense(target.shape[1],activation=activation_output))\n",
    "    if (optimizer[0]=='sgd'):\n",
    "        model.compile(optimizer=SGD(lr=optimizer[1]), loss=loss)\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        #Learning rate Dosent works #Update Later\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[(100,'relu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_80 (Dense)             (None, 100)               1000      \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,501\n",
      "Trainable params: 41,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -9.24663603e-01],\n",
       "       [ -1.42051113e+00],\n",
       "       [  9.54150259e-02],\n",
       "       [  7.86441714e-02],\n",
       "       [ -6.08947098e-01],\n",
       "       [ -1.20674372e-02],\n",
       "       [ -1.23713923e+00],\n",
       "       [ -4.19161320e-02],\n",
       "       [  3.70910764e-03],\n",
       "       [ -4.19161320e-02],\n",
       "       [ -6.23576462e-01],\n",
       "       [ -7.69156814e-01],\n",
       "       [ -1.08366466e+00],\n",
       "       [ -1.31182981e+00],\n",
       "       [ -1.17579162e+00],\n",
       "       [ -1.52612722e+00],\n",
       "       [ -1.32978058e+00],\n",
       "       [ -1.23463237e+00],\n",
       "       [ -6.08316243e-01],\n",
       "       [ -1.41528451e+00],\n",
       "       [ -4.72852796e-01],\n",
       "       [ -1.34747124e+00],\n",
       "       [ -4.19161320e-02],\n",
       "       [ -4.59509909e-01],\n",
       "       [ -1.07139599e+00],\n",
       "       [ -1.22200596e+00],\n",
       "       [  1.81257129e-02],\n",
       "       [ -1.23694491e+00],\n",
       "       [  4.74543869e-02],\n",
       "       [  1.08866394e-02],\n",
       "       [ -2.15100095e-01],\n",
       "       [ -1.79599071e+00],\n",
       "       [ -9.10483658e-01],\n",
       "       [  1.09482378e-01],\n",
       "       [ -7.34625518e-01],\n",
       "       [ -1.51706457e+00],\n",
       "       [ -9.01911139e-01],\n",
       "       [ -1.54748154e+00],\n",
       "       [  8.13895613e-02],\n",
       "       [ -1.32336915e-02],\n",
       "       [  1.09592974e-01],\n",
       "       [ -2.48564914e-01],\n",
       "       [ -3.18430603e-01],\n",
       "       [ -6.16659403e-01],\n",
       "       [  4.71800566e-03],\n",
       "       [ -6.35931551e-01],\n",
       "       [ -3.83695960e-02],\n",
       "       [ -4.48875129e-01],\n",
       "       [ -1.26135838e+00],\n",
       "       [ -2.34550238e-02],\n",
       "       [  4.08233702e-02],\n",
       "       [ -9.24344957e-02],\n",
       "       [  6.07560575e-02],\n",
       "       [ -4.49198306e-01],\n",
       "       [  1.12037659e-01],\n",
       "       [ -9.43449736e-02],\n",
       "       [ -5.36487937e-01],\n",
       "       [ -3.41334522e-01],\n",
       "       [ -1.01753855e+00],\n",
       "       [ -1.27199793e+00],\n",
       "       [ -6.81775093e-01],\n",
       "       [ -1.20973027e+00],\n",
       "       [ -1.14589739e+00],\n",
       "       [  4.85200584e-02],\n",
       "       [  5.44678867e-02],\n",
       "       [ -1.21534085e+00],\n",
       "       [ -7.68191338e-01],\n",
       "       [ -8.26471329e-01],\n",
       "       [ -1.48375833e+00],\n",
       "       [ -1.49423695e+00],\n",
       "       [  6.37136102e-02],\n",
       "       [ -7.64425695e-01],\n",
       "       [ -5.52513242e-01],\n",
       "       [ -9.88469571e-02],\n",
       "       [ -1.28194380e+00],\n",
       "       [ -1.26251280e+00],\n",
       "       [ -1.18030214e+00],\n",
       "       [ -1.37258112e-01],\n",
       "       [ -2.01321810e-01],\n",
       "       [ -1.48305798e+00],\n",
       "       [ -2.88397908e-01],\n",
       "       [ -5.43806672e-01],\n",
       "       [ -1.64832342e+00],\n",
       "       [ -1.10858232e-01],\n",
       "       [  1.41970217e-02],\n",
       "       [ -2.64809430e-01],\n",
       "       [ -7.66768694e-01],\n",
       "       [ -6.73877418e-01],\n",
       "       [  5.31542897e-02],\n",
       "       [ -1.47095716e+00],\n",
       "       [  6.10445738e-02],\n",
       "       [ -1.01224518e+00],\n",
       "       [  8.70559663e-02],\n",
       "       [ -3.93101573e-02],\n",
       "       [ -1.01224518e+00],\n",
       "       [  2.74654329e-02],\n",
       "       [  7.18292594e-02],\n",
       "       [ -1.51451147e+00],\n",
       "       [ -7.02030659e-01],\n",
       "       [ -3.18625003e-01],\n",
       "       [ -3.47305208e-01],\n",
       "       [ -3.50846052e-01],\n",
       "       [  1.30802393e-03],\n",
       "       [ -1.01224518e+00],\n",
       "       [ -1.49897397e-01],\n",
       "       [ -4.29768562e-02],\n",
       "       [ -8.45066845e-01],\n",
       "       [ -1.23604512e+00],\n",
       "       [  6.92967176e-02],\n",
       "       [ -1.34837079e+00],\n",
       "       [ -4.00397182e-02],\n",
       "       [ -2.00879991e-01],\n",
       "       [ -1.68054342e+00],\n",
       "       [ -2.89160907e-02],\n",
       "       [ -1.31476152e+00],\n",
       "       [  1.07674897e-01],\n",
       "       [ -2.46156275e-01],\n",
       "       [ -2.12248310e-01],\n",
       "       [ -1.00971580e+00],\n",
       "       [ -2.01321810e-01],\n",
       "       [  6.47646487e-02],\n",
       "       [ -4.00640249e-01],\n",
       "       [ -1.31701791e+00],\n",
       "       [ -8.50042164e-01],\n",
       "       [  3.09413671e-02],\n",
       "       [  5.14767766e-02],\n",
       "       [ -1.15272963e+00],\n",
       "       [ -6.37447596e-01],\n",
       "       [  5.38129508e-02],\n",
       "       [ -5.87829709e-01],\n",
       "       [  1.05938137e-01],\n",
       "       [  8.39887410e-02],\n",
       "       [  1.08866394e-02],\n",
       "       [  9.25008953e-02],\n",
       "       [ -6.52153194e-01],\n",
       "       [ -1.36676878e-01],\n",
       "       [  8.01106989e-02],\n",
       "       [ -1.28334808e+00],\n",
       "       [ -1.32030177e+00],\n",
       "       [ -2.92836726e-02],\n",
       "       [ -2.32473955e-01],\n",
       "       [ -6.33155763e-01],\n",
       "       [ -4.97303545e-01],\n",
       "       [ -1.35447931e+00],\n",
       "       [ -1.27904570e+00],\n",
       "       [ -1.52326667e+00],\n",
       "       [  7.30996430e-02],\n",
       "       [ -1.66087723e+00],\n",
       "       [ -5.78625381e-01],\n",
       "       [  5.68760037e-02],\n",
       "       [ -1.10939312e+00],\n",
       "       [ -1.07148850e+00],\n",
       "       [  4.52841818e-02],\n",
       "       [ -5.64808130e-01],\n",
       "       [  8.25743377e-02],\n",
       "       [ -2.42939159e-01],\n",
       "       [ -4.85998452e-01],\n",
       "       [ -8.51642072e-01],\n",
       "       [  8.93867016e-02],\n",
       "       [  5.59439659e-02],\n",
       "       [ -1.38831818e+00],\n",
       "       [ -1.49306536e+00],\n",
       "       [ -4.52477932e-02],\n",
       "       [ -6.37824118e-01],\n",
       "       [ -1.19784403e+00],\n",
       "       [  5.00590801e-02],\n",
       "       [  6.67473674e-02],\n",
       "       [  1.15884751e-01],\n",
       "       [ -1.48267770e+00],\n",
       "       [ -1.13310254e+00],\n",
       "       [  9.47162509e-02],\n",
       "       [ -1.58188045e+00],\n",
       "       [  1.04295284e-01],\n",
       "       [ -1.41422629e+00],\n",
       "       [ -9.26859915e-01],\n",
       "       [ -4.85998452e-01],\n",
       "       [ -4.38753575e-01],\n",
       "       [ -8.85993838e-02],\n",
       "       [ -1.64318514e+00],\n",
       "       [ -1.43164301e+00],\n",
       "       [ -1.23190355e+00],\n",
       "       [  6.22051954e-02],\n",
       "       [ -3.76572311e-02],\n",
       "       [  1.18308365e-02],\n",
       "       [ -7.99300790e-01],\n",
       "       [  2.33083367e-02],\n",
       "       [ -1.64877594e-01],\n",
       "       [ -1.23667336e+00],\n",
       "       [ -4.52645868e-01],\n",
       "       [ -6.56635463e-02],\n",
       "       [ -1.51725769e+00],\n",
       "       [ -4.02739286e-01],\n",
       "       [  7.79592991e-02],\n",
       "       [ -4.01185155e-02],\n",
       "       [ -9.43613708e-01],\n",
       "       [  8.34286511e-02],\n",
       "       [ -1.10528898e+00],\n",
       "       [ -1.73075378e-01],\n",
       "       [ -1.56789410e+00],\n",
       "       [ -1.13562250e+00],\n",
       "       [ -1.68304420e+00],\n",
       "       [  8.62920284e-02],\n",
       "       [ -1.69844270e-01],\n",
       "       [  9.25197005e-02],\n",
       "       [  7.55556524e-02],\n",
       "       [  6.12608790e-02],\n",
       "       [ -1.45900905e+00],\n",
       "       [ -2.45197088e-01],\n",
       "       [ -1.57066047e-01],\n",
       "       [ -1.09358454e+00],\n",
       "       [ -9.71969664e-02],\n",
       "       [ -1.49039900e+00],\n",
       "       [ -5.31576931e-01],\n",
       "       [  6.69839978e-02],\n",
       "       [ -9.26859915e-01],\n",
       "       [  8.93867016e-02],\n",
       "       [ -1.36124575e+00],\n",
       "       [ -1.60007894e+00],\n",
       "       [  5.92305660e-02],\n",
       "       [ -1.58765763e-01],\n",
       "       [ -1.32823551e+00],\n",
       "       [ -1.19951046e+00],\n",
       "       [ -1.71917570e+00],\n",
       "       [  6.18405044e-02],\n",
       "       [ -8.19975972e-01],\n",
       "       [  7.76227713e-02],\n",
       "       [ -3.14486980e-01],\n",
       "       [ -1.35647261e+00],\n",
       "       [  6.62406981e-02],\n",
       "       [ -7.04528689e-02],\n",
       "       [ -1.76156509e+00],\n",
       "       [  5.62903583e-02],\n",
       "       [ -4.50423688e-01],\n",
       "       [  6.12487495e-02],\n",
       "       [  6.12897277e-02],\n",
       "       [ -2.59428263e-01],\n",
       "       [ -1.49646866e+00],\n",
       "       [ -3.41639370e-01],\n",
       "       [ -8.16554844e-01],\n",
       "       [ -1.70494759e+00],\n",
       "       [ -1.28982627e+00],\n",
       "       [  6.05781972e-02],\n",
       "       [  1.13463759e-01],\n",
       "       [ -1.40709090e+00],\n",
       "       [ -8.47066343e-01],\n",
       "       [  7.93436766e-02],\n",
       "       [ -1.43100202e+00],\n",
       "       [  7.47230202e-02],\n",
       "       [ -5.41556478e-02],\n",
       "       [ -4.80535865e-01],\n",
       "       [ -2.23717988e-02],\n",
       "       [  8.23746324e-02],\n",
       "       [ -1.20992661e+00],\n",
       "       [ -6.73877418e-01],\n",
       "       [ -1.12702799e+00],\n",
       "       [  9.63103175e-02],\n",
       "       [ -2.30841726e-01],\n",
       "       [ -1.08851850e+00],\n",
       "       [ -2.59589255e-02],\n",
       "       [ -3.45838726e-01],\n",
       "       [ -2.31199339e-01],\n",
       "       [ -1.13310254e+00],\n",
       "       [ -1.72691488e+00],\n",
       "       [ -2.13260263e-01],\n",
       "       [ -5.38114965e-01],\n",
       "       [ -1.14741826e+00],\n",
       "       [  6.27697110e-02],\n",
       "       [ -1.13310254e+00],\n",
       "       [ -1.71917570e+00],\n",
       "       [ -8.78334641e-01],\n",
       "       [ -1.51725769e+00],\n",
       "       [  5.91697395e-02],\n",
       "       [ -5.62277079e-01],\n",
       "       [ -1.42304981e+00],\n",
       "       [  9.75720286e-02],\n",
       "       [ -1.34344912e+00],\n",
       "       [  9.79432166e-02],\n",
       "       [  4.23439443e-02],\n",
       "       [ -1.64025784e+00],\n",
       "       [ -2.32473955e-01],\n",
       "       [  5.21725416e-03],\n",
       "       [  5.96148670e-02],\n",
       "       [ -1.33339703e+00],\n",
       "       [  1.07088685e-01],\n",
       "       [  9.27454233e-02],\n",
       "       [ -7.18956292e-02],\n",
       "       [ -1.54662275e+00],\n",
       "       [ -1.13310254e+00],\n",
       "       [ -1.47771382e+00],\n",
       "       [  9.81763303e-02],\n",
       "       [ -1.20992661e+00],\n",
       "       [  1.30802393e-03],\n",
       "       [ -4.50883389e-01],\n",
       "       [  4.55340147e-02],\n",
       "       [ -1.46587813e+00],\n",
       "       [ -1.20992661e+00],\n",
       "       [ -4.21705604e-01],\n",
       "       [ -3.44840348e-01],\n",
       "       [  1.00304753e-01],\n",
       "       [ -7.65668690e-01],\n",
       "       [ -9.50995922e-01],\n",
       "       [  5.65845072e-02],\n",
       "       [ -3.39949518e-01],\n",
       "       [ -1.40804064e+00],\n",
       "       [  3.13800871e-02],\n",
       "       [ -1.38370574e+00],\n",
       "       [ -5.06021559e-01],\n",
       "       [ -8.67734194e-01],\n",
       "       [  5.75782955e-02],\n",
       "       [ -3.34248543e-02],\n",
       "       [  9.84774232e-02],\n",
       "       [  7.61847943e-02],\n",
       "       [  1.07088685e-01],\n",
       "       [ -4.02652293e-01],\n",
       "       [  3.13800871e-02],\n",
       "       [  6.07053488e-02],\n",
       "       [ -4.50423688e-01],\n",
       "       [ -1.35073280e+00],\n",
       "       [ -1.07198560e+00],\n",
       "       [ -1.53873897e+00],\n",
       "       [ -3.74132931e-01],\n",
       "       [  6.11901879e-02],\n",
       "       [ -1.31937474e-01],\n",
       "       [ -1.25289071e+00],\n",
       "       [ -1.49646866e+00],\n",
       "       [  7.35670924e-02],\n",
       "       [ -1.57699537e+00],\n",
       "       [ -6.21239066e-01],\n",
       "       [ -2.38499194e-01],\n",
       "       [ -1.48839688e+00],\n",
       "       [ -7.55114615e-01],\n",
       "       [ -1.81169724e+00],\n",
       "       [  7.79940784e-02],\n",
       "       [ -1.42833996e+00],\n",
       "       [ -3.74132931e-01],\n",
       "       [ -1.03846574e+00],\n",
       "       [ -8.40489626e-01],\n",
       "       [ -4.21705604e-01],\n",
       "       [ -1.44716680e+00],\n",
       "       [  7.79592991e-02],\n",
       "       [ -1.72836828e+00],\n",
       "       [ -1.05875432e-02],\n",
       "       [  5.84754944e-02],\n",
       "       [  8.57990682e-02],\n",
       "       [ -6.59902334e-01],\n",
       "       [ -3.74132931e-01],\n",
       "       [ -1.23361647e+00],\n",
       "       [ -5.09472907e-01],\n",
       "       [ -1.13601923e+00],\n",
       "       [ -1.18384361e+00],\n",
       "       [ -5.67095876e-01],\n",
       "       [ -1.19841170e+00],\n",
       "       [ -8.98940206e-01],\n",
       "       [ -1.44371200e+00],\n",
       "       [ -1.28222990e+00],\n",
       "       [ -1.66285121e+00],\n",
       "       [ -5.33762455e-01],\n",
       "       [ -1.31541669e+00],\n",
       "       [  1.13463759e-01],\n",
       "       [ -4.03153062e-01],\n",
       "       [  7.09951520e-02],\n",
       "       [ -7.22660959e-01],\n",
       "       [ -3.73193175e-01],\n",
       "       [ -6.51832879e-01],\n",
       "       [ -8.64666700e-02],\n",
       "       [  3.33307683e-02],\n",
       "       [  5.55950850e-02],\n",
       "       [ -1.49466097e-02],\n",
       "       [ -1.66353977e+00],\n",
       "       [ -9.01757479e-01],\n",
       "       [ -1.58445823e+00],\n",
       "       [ -1.51725769e+00],\n",
       "       [ -2.50682235e-01],\n",
       "       [ -3.41639370e-01],\n",
       "       [ -8.81552041e-01],\n",
       "       [  5.12944162e-02],\n",
       "       [  2.81407535e-02],\n",
       "       [  5.65845072e-02],\n",
       "       [  5.16187847e-02],\n",
       "       [  9.84774232e-02],\n",
       "       [ -9.26882386e-01],\n",
       "       [ -1.13310254e+00],\n",
       "       [ -4.54514503e-01],\n",
       "       [  1.74656808e-02],\n",
       "       [ -6.71066761e-01],\n",
       "       [  7.61847943e-02],\n",
       "       [ -8.74650657e-01],\n",
       "       [ -1.28163338e+00],\n",
       "       [ -1.41848564e+00],\n",
       "       [ -1.66356158e+00],\n",
       "       [ -1.28023779e+00],\n",
       "       [  7.16010332e-02],\n",
       "       [ -4.29696888e-01],\n",
       "       [ -6.40220344e-01],\n",
       "       [ -7.39291310e-02],\n",
       "       [  1.16032362e-01],\n",
       "       [ -1.71844149e+00],\n",
       "       [  8.46279413e-02],\n",
       "       [  1.00304753e-01],\n",
       "       [ -1.24393010e+00],\n",
       "       [ -1.21255827e+00],\n",
       "       [ -4.21705604e-01],\n",
       "       [ -7.02324510e-03],\n",
       "       [ -1.78412664e+00],\n",
       "       [  7.81511068e-02],\n",
       "       [ -1.55784512e+00],\n",
       "       [  7.76227713e-02],\n",
       "       [ -1.38195419e+00],\n",
       "       [  1.00341976e-01],\n",
       "       [  9.77478772e-02],\n",
       "       [ -1.61281264e+00],\n",
       "       [ -1.49991477e+00],\n",
       "       [ -6.15532160e-01],\n",
       "       [ -1.62459588e+00],\n",
       "       [  7.63682127e-02],\n",
       "       [  9.99227166e-02],\n",
       "       [ -1.08851850e+00],\n",
       "       [ -1.23653316e+00],\n",
       "       [  5.74384332e-02],\n",
       "       [ -5.97805381e-02],\n",
       "       [ -1.42833996e+00],\n",
       "       [ -2.26354152e-01],\n",
       "       [ -1.22466683e-03],\n",
       "       [ -7.94308484e-01],\n",
       "       [ -7.39291310e-02],\n",
       "       [ -1.50346518e+00],\n",
       "       [ -1.60676658e+00],\n",
       "       [ -6.45954669e-01],\n",
       "       [ -1.47806609e+00],\n",
       "       [ -1.59688473e+00],\n",
       "       [ -1.60370231e-01],\n",
       "       [ -1.36117756e+00],\n",
       "       [  7.72280395e-02],\n",
       "       [ -3.74909848e-01],\n",
       "       [ -1.36864281e+00],\n",
       "       [  9.68922973e-02],\n",
       "       [  1.06445104e-01],\n",
       "       [  1.03882760e-01],\n",
       "       [ -5.96959472e-01],\n",
       "       [  1.02740109e-01],\n",
       "       [  1.20268166e-02],\n",
       "       [ -1.57476354e+00],\n",
       "       [  9.93159711e-02],\n",
       "       [  4.15501297e-02],\n",
       "       [ -1.23532742e-01],\n",
       "       [ -2.21550465e-04],\n",
       "       [  7.76227713e-02],\n",
       "       [  9.04493034e-02],\n",
       "       [ -1.87505245e-01],\n",
       "       [  9.22907591e-02],\n",
       "       [  3.21670175e-02],\n",
       "       [  6.50613606e-02],\n",
       "       [ -8.80647063e-01],\n",
       "       [  4.72046137e-02],\n",
       "       [ -1.23531520e-02],\n",
       "       [ -4.58914459e-01],\n",
       "       [ -2.33523741e-01],\n",
       "       [ -3.53525817e-01],\n",
       "       [  7.29543865e-02],\n",
       "       [  9.49471295e-02],\n",
       "       [ -7.73796439e-01],\n",
       "       [ -1.31961334e+00],\n",
       "       [ -3.18370461e-01],\n",
       "       [ -3.69984925e-01],\n",
       "       [  8.23746324e-02],\n",
       "       [ -1.26146019e-01],\n",
       "       [ -1.01812482e-01],\n",
       "       [ -5.33762455e-01],\n",
       "       [ -1.57766771e+00],\n",
       "       [ -9.37960982e-01],\n",
       "       [  9.13873911e-02],\n",
       "       [ -1.45764798e-01],\n",
       "       [ -9.84675288e-02],\n",
       "       [  5.92423379e-02],\n",
       "       [ -1.10216349e-01],\n",
       "       [ -1.32602322e+00],\n",
       "       [  7.86441714e-02],\n",
       "       [ -1.07898498e+00],\n",
       "       [  6.92703873e-02],\n",
       "       [ -1.87505245e-01],\n",
       "       [  1.07726932e-01],\n",
       "       [ -1.51536751e+00],\n",
       "       [ -1.73869264e+00],\n",
       "       [ -3.50514174e-01],\n",
       "       [  1.05144083e-01],\n",
       "       [ -1.87247038e+00],\n",
       "       [ -6.43991232e-01],\n",
       "       [  8.21005106e-02],\n",
       "       [  2.07929015e-02],\n",
       "       [ -2.90478021e-01],\n",
       "       [ -8.97035122e-01],\n",
       "       [ -6.26356006e-02],\n",
       "       [ -1.30311525e+00],\n",
       "       [  5.92423379e-02],\n",
       "       [  6.59373999e-02],\n",
       "       [  9.15201753e-02],\n",
       "       [ -1.69603288e+00],\n",
       "       [ -1.07285655e+00],\n",
       "       [  6.27697110e-02],\n",
       "       [ -1.45721364e+00],\n",
       "       [ -3.78695428e-02],\n",
       "       [ -1.64318514e+00],\n",
       "       [ -4.30783212e-01],\n",
       "       [  6.12608790e-02],\n",
       "       [  6.28829598e-02],\n",
       "       [  4.52012718e-02],\n",
       "       [ -3.81646037e-01],\n",
       "       [  9.49471295e-02],\n",
       "       [ -1.39071137e-01],\n",
       "       [ -8.00881147e-01],\n",
       "       [ -1.01656914e-01],\n",
       "       [ -8.36268067e-03],\n",
       "       [ -8.06014359e-01],\n",
       "       [ -1.61112893e+00],\n",
       "       [ -2.38273114e-01],\n",
       "       [ -3.41765434e-01],\n",
       "       [  1.18308365e-02],\n",
       "       [ -1.20453227e+00],\n",
       "       [ -8.64666700e-02],\n",
       "       [  6.41424060e-02],\n",
       "       [  8.55728090e-02],\n",
       "       [ -3.54275286e-01],\n",
       "       [  3.97285521e-02],\n",
       "       [  8.59262049e-02],\n",
       "       [ -1.33274305e+00],\n",
       "       [ -6.23038411e-03],\n",
       "       [ -1.01084709e-02],\n",
       "       [ -2.32473955e-01],\n",
       "       [  7.35605061e-02],\n",
       "       [  1.02924168e-01],\n",
       "       [ -1.42833996e+00],\n",
       "       [ -9.91011262e-01],\n",
       "       [ -3.17403913e-01],\n",
       "       [ -1.49861503e+00]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regression Function Calling\n",
    "#11\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "df = pandas.read_csv('hourly_wages.csv')\n",
    "predictors=np.delete(df.values,0,1) # Detete a Column  [1->column 0->row]\n",
    "target=df.values[:,0] # or # df.wage_per_hour\n",
    "\n",
    "A=DeepNetwork_Reg(predictors,target) #np arrays\n",
    "A.summary()\n",
    "\n",
    "#\n",
    "A.predict(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_68 (Dense)             (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 41,702\n",
      "Trainable params: 41,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.16365631,  0.83634377],\n",
       "       [ 0.226697  ,  0.77330297],\n",
       "       [ 0.2181461 ,  0.78185397],\n",
       "       [ 0.41411492,  0.58588511],\n",
       "       [ 0.19812499,  0.80187505],\n",
       "       [ 0.1588922 ,  0.84110785],\n",
       "       [ 0.03326764,  0.96673232],\n",
       "       [ 0.33062503,  0.66937494],\n",
       "       [ 0.11663951,  0.88336051],\n",
       "       [ 0.16361001,  0.83638996],\n",
       "       [ 0.22205923,  0.77794075],\n",
       "       [ 0.21500425,  0.78499579],\n",
       "       [ 0.13044009,  0.86955994],\n",
       "       [ 0.41949582,  0.58050424],\n",
       "       [ 0.16779181,  0.83220822],\n",
       "       [ 0.06773765,  0.9322623 ],\n",
       "       [ 0.26770872,  0.73229134],\n",
       "       [ 0.17482685,  0.82517314],\n",
       "       [ 0.06051283,  0.93948722],\n",
       "       [ 0.34818095,  0.65181911],\n",
       "       [ 0.08022562,  0.91977435],\n",
       "       [ 0.21753317,  0.78246683],\n",
       "       [ 0.03610273,  0.96389729],\n",
       "       [ 0.26221314,  0.73778689],\n",
       "       [ 0.40886709,  0.59113288],\n",
       "       [ 0.17580637,  0.8241936 ],\n",
       "       [ 0.17658006,  0.82341993],\n",
       "       [ 0.39613789,  0.60386205],\n",
       "       [ 0.18549928,  0.81450069],\n",
       "       [ 0.04858477,  0.95141524],\n",
       "       [ 0.33672145,  0.66327864],\n",
       "       [ 0.44329688,  0.55670315],\n",
       "       [ 0.18331254,  0.81668741],\n",
       "       [ 0.26303926,  0.73696077],\n",
       "       [ 0.33708629,  0.66291368],\n",
       "       [ 0.1594059 ,  0.84059411],\n",
       "       [ 0.30379987,  0.69620007],\n",
       "       [ 0.17145579,  0.82854426],\n",
       "       [ 0.15801783,  0.84198219],\n",
       "       [ 0.22362904,  0.77637094],\n",
       "       [ 0.30967563,  0.69032437],\n",
       "       [ 0.3656773 ,  0.6343227 ],\n",
       "       [ 0.21500419,  0.78499579],\n",
       "       [ 0.12535383,  0.87464619],\n",
       "       [ 0.35014105,  0.64985895],\n",
       "       [ 0.06979636,  0.93020362],\n",
       "       [ 0.19401722,  0.80598277],\n",
       "       [ 0.13499938,  0.86500061],\n",
       "       [ 0.2043566 ,  0.79564345],\n",
       "       [ 0.2268925 ,  0.77310747],\n",
       "       [ 0.43381828,  0.56618172],\n",
       "       [ 0.00620354,  0.99379647],\n",
       "       [ 0.36534446,  0.63465548],\n",
       "       [ 0.31657979,  0.6834203 ],\n",
       "       [ 0.1784202 ,  0.82157987],\n",
       "       [ 0.33975232,  0.66024768],\n",
       "       [ 0.12713228,  0.8728677 ],\n",
       "       [ 0.13102554,  0.86897451],\n",
       "       [ 0.29543582,  0.70456415],\n",
       "       [ 0.18331254,  0.81668741],\n",
       "       [ 0.11484185,  0.88515812],\n",
       "       [ 0.31624177,  0.6837582 ],\n",
       "       [ 0.14806522,  0.85193479],\n",
       "       [ 0.19956289,  0.80043709],\n",
       "       [ 0.29730159,  0.70269841],\n",
       "       [ 0.09815017,  0.90184987],\n",
       "       [ 0.29644305,  0.70355695],\n",
       "       [ 0.17270558,  0.82729441],\n",
       "       [ 0.18615535,  0.81384468],\n",
       "       [ 0.39377216,  0.60622782],\n",
       "       [ 0.22216302,  0.77783698],\n",
       "       [ 0.10890288,  0.89109707],\n",
       "       [ 0.08894776,  0.91105223],\n",
       "       [ 0.05394391,  0.94605607],\n",
       "       [ 0.25034851,  0.74965143],\n",
       "       [ 0.39186805,  0.60813189],\n",
       "       [ 0.3233932 ,  0.67660683],\n",
       "       [ 0.32295051,  0.67704952],\n",
       "       [ 0.1696194 ,  0.83038062],\n",
       "       [ 0.05872477,  0.94127524],\n",
       "       [ 0.24413817,  0.75586188],\n",
       "       [ 0.13912208,  0.86087793],\n",
       "       [ 0.30308241,  0.69691765],\n",
       "       [ 0.20571133,  0.79428864],\n",
       "       [ 0.22521552,  0.77478451],\n",
       "       [ 0.20436566,  0.79563433],\n",
       "       [ 0.25970277,  0.7402972 ],\n",
       "       [ 0.2794919 ,  0.72050804],\n",
       "       [ 0.23917989,  0.76082015],\n",
       "       [ 0.22720583,  0.77279419],\n",
       "       [ 0.16519485,  0.83480513]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification Function Calling\n",
    "#12\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "#Import Data\n",
    "df = pandas.read_csv('Titanic_data.csv')\n",
    "predictors=np.delete(df.values,0,1) # Detete a Column  [1->column 0->row]\n",
    "target=np.array(df.survived)\n",
    "\n",
    "pred_data=np.delete(pandas.read_csv('Titanic_pred_data.csv').values,0,1)\n",
    "n_cols = predictors.shape[1]\n",
    "A=DeepNetwork_Clas(predictors,target)\n",
    "A.summary()\n",
    "\n",
    "\n",
    "#\n",
    "pred_data=np.delete(pandas.read_csv('Titanic_pred_data.csv').values,0,1)\n",
    "A.predict(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x246e94bef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Deeper the network the good the results\n",
      "Lower Network Lower Newrons = Low accuracy -> Higher Nuron  Higher network = High accuracy\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import Data\n",
    "df = pandas.read_csv('Titanic_data.csv')\n",
    "predictors=np.delete(df.values,0,1) # Detete a Column  [1->column 0->row]\n",
    "target=to_categorical(df.survived)\n",
    "pred_data=np.delete(pandas.read_csv('Titanic_pred_data.csv').values,0,1)\n",
    "input_shape = (predictors.shape[1],)\n",
    "\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_1.add(Dense(10, activation='relu', input_shape=input_shape))\n",
    "model_1.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create the new model: model_3\n",
    "model_3 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_3.add(Dense(1000, activation='relu', input_shape=input_shape))\n",
    "model_3.add(Dense(1000, activation='relu'))\n",
    "model_3.add(Dense(1000, activation='relu'))\n",
    "model_3.add(Dense(1000, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_3.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_2\n",
    "model_3_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create the new model: model_4\n",
    "model_4 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_4.add(Dense(1000, activation='relu', input_shape=input_shape))\n",
    "model_4.add(Dense(100, activation='relu'))\n",
    "model_4.add(Dense(10, activation='relu'))\n",
    "model_4.add(Dense(5, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_4.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_2\n",
    "model_4_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the new model: model_4\n",
    "model_5 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_5.add(Dense(10000, activation='relu', input_shape=input_shape))\n",
    "model_5.add(Dense(1000, activation='relu'))\n",
    "model_5.add(Dense(100, activation='relu'))\n",
    "model_5.add(Dense(50, activation='relu'))\n",
    "model_5.add(Dense(50, activation='relu'))\n",
    "# Add the output layer\n",
    "model_5.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_5.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_2\n",
    "model_5_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r',\n",
    "         model_2_training.history['val_loss'], 'b',\n",
    "         model_3_training.history['val_loss'], 'g',\n",
    "         model_4_training.history['val_loss'], 'm',\n",
    "        model_5_training.history['val_loss'], 'y')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n",
    "print(\"The Deeper the network the good the results\")\n",
    "print(\"Lower Network Lower Newrons = Low accuracy -> Higher Nuron  Higher network = High accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71540627765921905, 0.65235679276162684, 0.62005860212794894, 0.60594261391868809, 0.59707941489512695, 0.58152006158615621, 0.57295705186588142, 0.56013032444362532, 0.55599553458517492, 0.5420447827717445, 0.53369017419868336, 0.53235662932502492, 0.52697970331048172, 0.51699945680255999, 0.5166859799923178] \n",
      " [0.71540627765921905, 0.65235679276162684, 0.62005860212794894, 0.60594261391868809, 0.59707941489512695, 0.58152006158615621, 0.57295705186588142, 0.56013032444362532, 0.55599553458517492, 0.5420447827717445, 0.53369017419868336, 0.53235662932502492, 0.52697970331048172, 0.51699945680255999, 0.5166859799923178] [0.60162672250630467, 0.53010120188723731, 0.5190721665347755, 0.6983629764791307, 0.52293257090632472] \n",
      " [0.56619439527975113, 0.60155735901614138, 0.56589885230836923, 0.61120860736463323, 0.4367636383911751, 0.43831636652600164, 0.45236721768059546]\n"
     ]
    }
   ],
   "source": [
    "print(model_1_training.history['val_loss'],\"\\n\",model_1_training.history['val_loss'],model_2_training.history['val_loss'],\"\\n\",model_3_training.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Validation\n",
    "# Over Fitting    :: Only predicts to train data\n",
    "# Under fitting   :: Dosen't predict to any data  i.e Train_Data or Predict_Data\n",
    "# Model Capacity  :: Ability to predict    More capacity More Predicitbility\n",
    "# Model Capacity  :: More Capacity more Patterans Captures\n",
    "# Model Capacity :: Increase Layers and Increase More Nurons = Increases the Model capacity  || up to sertain Limits || Move lower to higher model\n",
    "# Model Capacity :: Nothing But Decresing the Error ! \n",
    "# 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 601 samples\n",
      "Epoch 1/100\n",
      "1400/1400 [==============================] - 9s 6ms/step - loss: 12.0866 - acc: 0.2400 - val_loss: 11.7679 - val_acc: 0.2679\n",
      "Epoch 2/100\n",
      "1400/1400 [==============================] - 1s 393us/step - loss: 11.3120 - acc: 0.2964 - val_loss: 11.7032 - val_acc: 0.2662\n",
      "Epoch 3/100\n",
      "1400/1400 [==============================] - 1s 412us/step - loss: 11.2844 - acc: 0.2964 - val_loss: 11.5175 - val_acc: 0.2829\n",
      "Epoch 4/100\n",
      "1400/1400 [==============================] - 1s 682us/step - loss: 11.2418 - acc: 0.3014 - val_loss: 11.4013 - val_acc: 0.2912\n",
      "Epoch 5/100\n",
      "1400/1400 [==============================] - 1s 414us/step - loss: 11.2342 - acc: 0.3021 - val_loss: 11.3460 - val_acc: 0.2945\n",
      "Epoch 6/100\n",
      "1400/1400 [==============================] - 1s 430us/step - loss: 11.2672 - acc: 0.3000 - val_loss: 11.5172 - val_acc: 0.2845\n",
      "Epoch 7/100\n",
      "1400/1400 [==============================] - 1s 436us/step - loss: 11.3694 - acc: 0.2943 - val_loss: 11.3878 - val_acc: 0.2928\n",
      "Epoch 8/100\n",
      "1400/1400 [==============================] - 1s 417us/step - loss: 11.2674 - acc: 0.2993 - val_loss: 11.3378 - val_acc: 0.2945\n",
      "Epoch 9/100\n",
      "1400/1400 [==============================] - 1s 406us/step - loss: 11.2623 - acc: 0.2993 - val_loss: 11.2666 - val_acc: 0.2995\n",
      "Epoch 10/100\n",
      "1400/1400 [==============================] - 1s 429us/step - loss: 10.5960 - acc: 0.3400 - val_loss: 10.3410 - val_acc: 0.3544\n",
      "Epoch 11/100\n",
      "1400/1400 [==============================] - 1s 407us/step - loss: 9.8930 - acc: 0.3843 - val_loss: 9.7805 - val_acc: 0.3927\n",
      "Epoch 12/100\n",
      "1400/1400 [==============================] - 1s 451us/step - loss: 9.6042 - acc: 0.4029 - val_loss: 9.6605 - val_acc: 0.3977\n",
      "Epoch 13/100\n",
      "1400/1400 [==============================] - 1s 401us/step - loss: 9.6191 - acc: 0.4014 - val_loss: 9.8057 - val_acc: 0.3894\n",
      "Epoch 14/100\n",
      "1400/1400 [==============================] - 1s 414us/step - loss: 10.1443 - acc: 0.3686 - val_loss: 10.3016 - val_acc: 0.3594\n",
      "Epoch 15/100\n",
      "1400/1400 [==============================] - 1s 430us/step - loss: 9.9289 - acc: 0.3821 - val_loss: 9.9341 - val_acc: 0.3810\n",
      "Epoch 16/100\n",
      "1400/1400 [==============================] - 1s 443us/step - loss: 9.6717 - acc: 0.3993 - val_loss: 9.7048 - val_acc: 0.3960\n",
      "Epoch 17/100\n",
      "1400/1400 [==============================] - 1s 416us/step - loss: 9.7599 - acc: 0.3929 - val_loss: 9.8723 - val_acc: 0.3844\n",
      "Epoch 18/100\n",
      "1400/1400 [==============================] - 1s 442us/step - loss: 9.6786 - acc: 0.3993 - val_loss: 9.7352 - val_acc: 0.3960\n",
      "Epoch 19/100\n",
      "1400/1400 [==============================] - 1s 421us/step - loss: 9.6624 - acc: 0.4000 - val_loss: 9.6574 - val_acc: 0.3993\n",
      "Epoch 20/100\n",
      "1400/1400 [==============================] - 1s 412us/step - loss: 9.5505 - acc: 0.4050 - val_loss: 9.5817 - val_acc: 0.4027\n",
      "Epoch 21/100\n",
      "1400/1400 [==============================] - 1s 454us/step - loss: 9.4119 - acc: 0.4129 - val_loss: 9.1023 - val_acc: 0.4293\n",
      "Epoch 22/100\n",
      "1400/1400 [==============================] - 1s 462us/step - loss: 9.7880 - acc: 0.3893 - val_loss: 10.0571 - val_acc: 0.3727\n",
      "Epoch 23/100\n",
      "1400/1400 [==============================] - 1s 455us/step - loss: 9.7819 - acc: 0.3907 - val_loss: 8.6886 - val_acc: 0.4592\n",
      "Epoch 24/100\n",
      "1400/1400 [==============================] - 1s 435us/step - loss: 8.9832 - acc: 0.4421 - val_loss: 8.8935 - val_acc: 0.4459\n",
      "Epoch 25/100\n",
      "1400/1400 [==============================] - 1s 445us/step - loss: 8.9606 - acc: 0.4400 - val_loss: 8.6768 - val_acc: 0.4592\n",
      "Epoch 26/100\n",
      "1400/1400 [==============================] - 1s 418us/step - loss: 8.8142 - acc: 0.4507 - val_loss: 8.7043 - val_acc: 0.4592\n",
      "Epoch 27/100\n",
      "1400/1400 [==============================] - 1s 422us/step - loss: 8.5679 - acc: 0.4629 - val_loss: 8.5810 - val_acc: 0.4676\n",
      "Epoch 28/100\n",
      "1400/1400 [==============================] - 1s 412us/step - loss: 8.1522 - acc: 0.4914 - val_loss: 8.2450 - val_acc: 0.4859\n",
      "Epoch 29/100\n",
      "1400/1400 [==============================] - 1s 430us/step - loss: 8.1443 - acc: 0.4921 - val_loss: 7.5930 - val_acc: 0.5275\n",
      "Epoch 30/100\n",
      "1400/1400 [==============================] - 1s 545us/step - loss: 7.8605 - acc: 0.5086 - val_loss: 8.1862 - val_acc: 0.4875\n",
      "Epoch 31/100\n",
      "1400/1400 [==============================] - 0s 354us/step - loss: 7.7291 - acc: 0.5186 - val_loss: 7.9594 - val_acc: 0.5058\n",
      "Epoch 32/100\n",
      "1400/1400 [==============================] - 1s 459us/step - loss: 7.2935 - acc: 0.5450 - val_loss: 7.5839 - val_acc: 0.5258\n",
      "Epoch 33/100\n",
      "1400/1400 [==============================] - 1s 517us/step - loss: 7.1341 - acc: 0.5550 - val_loss: 7.5587 - val_acc: 0.5291\n",
      "Epoch 34/100\n",
      "1400/1400 [==============================] - 1s 436us/step - loss: 7.0871 - acc: 0.5593 - val_loss: 7.7070 - val_acc: 0.5191\n",
      "Epoch 35/100\n",
      "1400/1400 [==============================] - 1s 470us/step - loss: 7.1368 - acc: 0.5550 - val_loss: 7.4926 - val_acc: 0.5308\n",
      "Epoch 36/100\n",
      "1400/1400 [==============================] - 1s 412us/step - loss: 6.9206 - acc: 0.5671 - val_loss: 7.5018 - val_acc: 0.5341\n",
      "Epoch 37/100\n",
      "1400/1400 [==============================] - 1s 424us/step - loss: 6.9464 - acc: 0.5664 - val_loss: 7.6929 - val_acc: 0.5208\n",
      "Epoch 38/100\n",
      "1400/1400 [==============================] - 1s 432us/step - loss: 7.1396 - acc: 0.5564 - val_loss: 7.7138 - val_acc: 0.5191\n",
      "Epoch 39/100\n",
      "1400/1400 [==============================] - 1s 475us/step - loss: 6.8884 - acc: 0.5714 - val_loss: 7.3301 - val_acc: 0.5424\n",
      "Epoch 40/100\n",
      "1400/1400 [==============================] - 1s 456us/step - loss: 6.9168 - acc: 0.5707 - val_loss: 7.3639 - val_acc: 0.5408\n",
      "Epoch 41/100\n",
      "1400/1400 [==============================] - 1s 424us/step - loss: 6.8944 - acc: 0.5686 - val_loss: 7.5122 - val_acc: 0.5324\n",
      "Epoch 42/100\n",
      "1400/1400 [==============================] - 1s 415us/step - loss: 6.8802 - acc: 0.5700 - val_loss: 7.2075 - val_acc: 0.5507\n",
      "Epoch 43/100\n",
      "1400/1400 [==============================] - 1s 416us/step - loss: 6.8438 - acc: 0.5750 - val_loss: 7.3753 - val_acc: 0.5408\n",
      "Epoch 44/100\n",
      "1400/1400 [==============================] - 1s 458us/step - loss: 6.7503 - acc: 0.5800 - val_loss: 7.3569 - val_acc: 0.5391\n",
      "Epoch 45/100\n",
      "1400/1400 [==============================] - 1s 411us/step - loss: 6.9335 - acc: 0.5686 - val_loss: 7.5277 - val_acc: 0.5291\n",
      "Epoch 46/100\n",
      "1400/1400 [==============================] - 1s 406us/step - loss: 6.6680 - acc: 0.5850 - val_loss: 7.2497 - val_acc: 0.5474\n",
      "Epoch 47/100\n",
      "1400/1400 [==============================] - 1s 431us/step - loss: 6.6724 - acc: 0.5843 - val_loss: 7.3447 - val_acc: 0.5441\n",
      "Epoch 48/100\n",
      "1400/1400 [==============================] - 1s 518us/step - loss: 6.7232 - acc: 0.5807 - val_loss: 7.1884 - val_acc: 0.5524\n",
      "Epoch 49/100\n",
      "1400/1400 [==============================] - 1s 459us/step - loss: 6.7020 - acc: 0.5821 - val_loss: 7.1606 - val_acc: 0.5557\n",
      "Epoch 50/100\n",
      "1400/1400 [==============================] - 1s 463us/step - loss: 6.7113 - acc: 0.5821 - val_loss: 7.2368 - val_acc: 0.5491\n",
      "Epoch 51/100\n",
      "1400/1400 [==============================] - 1s 427us/step - loss: 6.6032 - acc: 0.5893 - val_loss: 7.1978 - val_acc: 0.5524\n",
      "Epoch 52/100\n",
      "1400/1400 [==============================] - 1s 417us/step - loss: 6.6979 - acc: 0.5821 - val_loss: 7.1340 - val_acc: 0.5574\n",
      "Epoch 53/100\n",
      "1400/1400 [==============================] - 1s 423us/step - loss: 6.7034 - acc: 0.5821 - val_loss: 7.1989 - val_acc: 0.5524\n",
      "Epoch 54/100\n",
      "1400/1400 [==============================] - 1s 439us/step - loss: 6.7441 - acc: 0.5807 - val_loss: 7.2706 - val_acc: 0.5474\n",
      "Epoch 55/100\n",
      "1400/1400 [==============================] - 1s 441us/step - loss: 6.7146 - acc: 0.5821 - val_loss: 7.1084 - val_acc: 0.5574\n",
      "Epoch 56/100\n",
      "1400/1400 [==============================] - 1s 449us/step - loss: 6.5709 - acc: 0.5921 - val_loss: 7.1002 - val_acc: 0.5574\n",
      "Epoch 57/100\n",
      "1400/1400 [==============================] - 1s 502us/step - loss: 6.5740 - acc: 0.5921 - val_loss: 7.0754 - val_acc: 0.5607\n",
      "Epoch 58/100\n",
      "1400/1400 [==============================] - 1s 474us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 59/100\n",
      "1400/1400 [==============================] - 1s 463us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "1400/1400 [==============================] - 0s 353us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 61/100\n",
      "1400/1400 [==============================] - 0s 350us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 62/100\n",
      "1400/1400 [==============================] - 0s 345us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 63/100\n",
      "1400/1400 [==============================] - 0s 338us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 64/100\n",
      "1400/1400 [==============================] - 1s 424us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 65/100\n",
      "1400/1400 [==============================] - 1s 409us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 66/100\n",
      "1400/1400 [==============================] - 1s 417us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 67/100\n",
      "1400/1400 [==============================] - 1s 404us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 68/100\n",
      "1400/1400 [==============================] - 1s 401us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 69/100\n",
      "1400/1400 [==============================] - 1s 396us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 70/100\n",
      "1400/1400 [==============================] - 1s 546us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 71/100\n",
      "1400/1400 [==============================] - 1s 428us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 72/100\n",
      "1400/1400 [==============================] - 1s 430us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 73/100\n",
      "1400/1400 [==============================] - 1s 469us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 74/100\n",
      "1400/1400 [==============================] - 1s 511us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 75/100\n",
      "1400/1400 [==============================] - 1s 502us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 76/100\n",
      "1400/1400 [==============================] - 1s 571us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 77/100\n",
      "1400/1400 [==============================] - 1s 536us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 78/100\n",
      "1400/1400 [==============================] - 1s 486us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 79/100\n",
      "1400/1400 [==============================] - 1s 452us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 80/100\n",
      "1400/1400 [==============================] - 1s 414us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 81/100\n",
      "1400/1400 [==============================] - 1s 398us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 82/100\n",
      "1400/1400 [==============================] - 1s 417us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 83/100\n",
      "1400/1400 [==============================] - 1s 404us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 84/100\n",
      "1400/1400 [==============================] - 1s 415us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 85/100\n",
      "1400/1400 [==============================] - 1s 407us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 86/100\n",
      "1400/1400 [==============================] - 1s 444us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 87/100\n",
      "1400/1400 [==============================] - 1s 416us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 88/100\n",
      "1400/1400 [==============================] - 1s 495us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 89/100\n",
      "1400/1400 [==============================] - 1s 430us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 90/100\n",
      "1400/1400 [==============================] - 1s 448us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 91/100\n",
      "1400/1400 [==============================] - 1s 393us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 92/100\n",
      "1400/1400 [==============================] - 0s 350us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 93/100\n",
      "1400/1400 [==============================] - 1s 366us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 94/100\n",
      "1400/1400 [==============================] - 1s 399us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 95/100\n",
      "1400/1400 [==============================] - 1s 425us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 96/100\n",
      "1400/1400 [==============================] - 1s 460us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 97/100\n",
      "1400/1400 [==============================] - 1s 375us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 98/100\n",
      "1400/1400 [==============================] - 1s 372us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 99/100\n",
      "1400/1400 [==============================] - 1s 376us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n",
      "Epoch 100/100\n",
      "1400/1400 [==============================] - 1s 468us/step - loss: 6.5739 - acc: 0.5921 - val_loss: 7.0755 - val_acc: 0.5607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24683e9bcf8>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNIST\n",
    "# HANDWRITTEN_DATASET = 0 1 2 3 4 5 6 7 8 9 \n",
    "# 28 X 28 = 784\n",
    "# 784 pixels -> 10\n",
    "# Download the actual MNIST data consist of 60000 data instead 2000\n",
    "# 14\n",
    "\n",
    "\n",
    "#Import Data\n",
    "df = pandas.read_csv('MNIST_HANDWRITTEN_NUMBER_DATA.csv',header=None)\n",
    "X=np.delete(df.values,0,1) # Detete a Column  [1->column 0->row]\n",
    "y=to_categorical(df[0])\n",
    "pred_data=np.delete(pandas.read_csv('Titanic_pred_data.csv').values,0,1)\n",
    "input_shape = (predictors.shape[1],)\n",
    "\n",
    "\n",
    "# Create the model: model\n",
    "model = Sequential()\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X,y,validation_split=0.3,epochs=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot Checks\n",
    "# 14.1\n",
    "plt.imshow(X[110].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 785)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolute Nural Networks\n",
    "# Kaggle :: Great place to find data sets\n",
    "# Keras and Tensorflow have good examples in Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nivADSRF+ICnCDyRF+IGk/h+477hoj+yiTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24686b77860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x246e94f95f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
